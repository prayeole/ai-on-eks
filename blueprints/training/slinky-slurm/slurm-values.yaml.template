# Inter-pod anti-affinity and node affinity for non-compute components
commonAffinity: &commonAffinity
  nodeAffinity:
    requiredDuringSchedulingIgnoredDuringExecution:
      nodeSelectorTerms:
        - matchExpressions:
          - key: "node.kubernetes.io/instance-type"
            operator: In
            values:
              - "m5.xlarge"
  podAntiAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
    - weight: 100
      podAffinityTerm:
        labelSelector:
          matchExpressions:
          - key: "app.kubernetes.io/name"
            operator: In
            values: ["slurmdbd", "slurmctld", "slurm-exporter", "login", "mariadb", "slurmrestd"]
        topologyKey: "kubernetes.io/hostname"


# Slurm controller (slurmctld) configurations.
controller:
  persistence:
    storageClassName: gp3
  podSpec:
    affinity: *commonAffinity

# Slurm accounting (slurmdbd) configurations.
accounting:
  enabled: true
  podSpec:
    affinity: *commonAffinity

# Slurm REST API (slurmrestd) configurations.
restapi:
  podSpec:
    affinity: *commonAffinity

# `slurm-exporter` subchart configurations.
slurm-exporter:
  exporter:
    affinity: *commonAffinity

# Login node configurations.
loginsets:
  slinky:
    enabled: true
    replicas: 1
    login:
      volumeMounts:
        - name: fsx-lustre
          mountPath: /fsx
    rootSshAuthorizedKeys: |
      ${ssh_key}
    extraSshdConfig: |
      AuthorizedKeysFile /root/.ssh/authorized_keys
      ChallengeResponseAuthentication no
      PasswordAuthentication no
      PubkeyAuthentication yes
      PermitRootLogin yes
      Port 22
    podSpec:
      affinity: *commonAffinity
      volumes:
        - name: fsx-lustre
          persistentVolumeClaim:
            claimName: fsx-static-pvc
    service:
      spec:
        type: LoadBalancer
      port: 22

# Slurm compute (slurmd) configurations.
nodesets:
  slinky:
    enabled: true
    replicas: 4
    slurmd:
      image:
        repository: ${image_repository}
        tag: ${image_tag}
      resources:
        limits:
          nvidia.com/gpu: "1"
          vpc.amazonaws.com/efa: 1
        requests:
          nvidia.com/gpu: "1"
          vpc.amazonaws.com/efa: 1
      volumeMounts:
        - name: fsx-lustre
          mountPath: /fsx
        - name: shmem
          mountPath: /dev/shm
    podSpec:
      nodeSelector:
        instanceType: g5-gpu-karpenter
        kubernetes.io/os: linux
      tolerations:
        - key: "nvidia.com/gpu"
          operator: "Exists"
          effect: "NoSchedule"
      volumes:
        - name: fsx-lustre
          persistentVolumeClaim:
            claimName: fsx-static-pvc
        - name: shmem
          hostPath:
            path: /dev/shm
