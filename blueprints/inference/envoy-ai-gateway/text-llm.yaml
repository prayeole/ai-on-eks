---
apiVersion: v1
kind: Namespace
metadata:
  name: ai-models
---
# Mock vLLM script for Ray-based text-llm service
apiVersion: v1
kind: ConfigMap
metadata:
  name: vllm-serve-script
  namespace: ai-models
data:
  vllm_serve.py: |
    import os
    from fastapi import FastAPI
    from starlette.responses import JSONResponse
    from ray import serve

    app = FastAPI()

    @serve.deployment(name="VLLMDeployment")
    @serve.ingress(app)
    class VLLMDeployment:
        def __init__(self):
            pass

        @app.get("/v1/models")
        async def get_models(self):
            return JSONResponse(content={
                "object": "list",
                "data": [{
                    "id": "text-llm",
                    "object": "model",
                    "created": 1677610602,
                    "owned_by": "self-hosted"
                }]
            })

        @app.post("/v1/chat/completions")
        async def create_chat_completion(self, request: dict):
            return JSONResponse(content={
                "id": "chatcmpl-123",
                "object": "chat.completion",
                "created": 1677652288,
                "model": "text-llm",
                "choices": [{
                    "index": 0,
                    "message": {
                        "role": "assistant",
                        "content": "Hello! This is a mock response from the Ray-based text LLM service."
                    },
                    "finish_reason": "stop"
                }],
                "usage": {
                    "prompt_tokens": 10,
                    "completion_tokens": 8,
                    "total_tokens": 18
                }
            })

    deployment = VLLMDeployment.bind()
---
# Ray-based Text LLM Service with Inferentia2 support
apiVersion: ray.io/v1
kind: RayService
metadata:
  name: text-llm
  namespace: ai-models
spec:
  serviceUnhealthySecondThreshold: 900
  deploymentUnhealthySecondThreshold: 300
  serveConfigV2: |
    applications:
      - name: text-llm
        import_path: vllm_serve:deployment
        route_prefix: "/"
  rayClusterConfig:
    rayVersion: '2.9.0'
    headGroupSpec:
      rayStartParams:
        dashboard-host: '0.0.0.0'
        num-cpus: '0'
      template:
        spec:
          containers:
            - name: ray-head
              image: rayproject/ray:2.9.0-py310
              resources:
                limits:
                  cpu: 2
                  memory: 8Gi
                requests:
                  cpu: 2
                  memory: 8Gi
              volumeMounts:
                - name: vllm-script
                  mountPath: /home/ray/vllm_serve.py
                  subPath: vllm_serve.py
          volumes:
            - name: vllm-script
              configMap:
                name: vllm-serve-script
    workerGroupSpecs:
      - replicas: 1
        minReplicas: 1
        maxReplicas: 1
        groupName: inf2-worker-group
        rayStartParams: {}
        template:
          spec:
            nodeSelector:
              node.kubernetes.io/instance-type: inf2.xlarge
            tolerations:
              - key: aws.amazon.com/neuron
                value: "true"
                effect: NoSchedule
            containers:
              - name: ray-worker
                image: rayproject/ray:2.9.0-py310
                resources:
                  limits:
                    aws.amazon.com/neuron: 1
                  requests:
                    aws.amazon.com/neuron: 1
                volumeMounts:
                  - name: vllm-script
                    mountPath: /home/ray/vllm_serve.py
                    subPath: vllm_serve.py
            volumes:
              - name: vllm-script
                configMap:
                  name: vllm-serve-script
---
# Service for text-llm
apiVersion: v1
kind: Service
metadata:
  name: text-llm-service
  namespace: ai-models
spec:
  selector:
    app.kubernetes.io/created-by: kuberay-operator
    ray.io/node-type: head
    ray.io/serve: "true"
  ports:
    - port: 8000
      targetPort: 8000
---
# Backend resources for AI Gateway
apiVersion: gateway.envoyproxy.io/v1alpha1
kind: Backend
metadata:
  name: text-llm-backend
  namespace: default
spec:
  endpoints:
    - fqdn:
        hostname: text-llm-service.ai-models.svc.cluster.local
        port: 8000
---
# AIServiceBackend resources
apiVersion: aigateway.envoyproxy.io/v1alpha1
kind: AIServiceBackend
metadata:
  name: self-hosted-text-llm
  namespace: default
spec:
  schema:
    name: OpenAI
  backendRef:
    name: text-llm-backend
    kind: Backend
    group: gateway.envoyproxy.io
