"use strict";(self.webpackChunkdoeks_website=self.webpackChunkdoeks_website||[]).push([[5054],{28453:(e,n,r)=>{r.d(n,{R:()=>i,x:()=>o});var t=r(96540);const a={},s=t.createContext(a);function i(e){const n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:i(e.components),t.createElement(s.Provider,{value:n},e.children)}},52244:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>p,frontMatter:()=>i,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"guidance/benchmarking/benchmarking-with-inference-perf/complete-deployment-example-guide","title":"Complete Deployment Example","description":"This example demonstrates a production-ready deployment with S3 storage, realistic load testing, and proper AWS integration. Follow these steps to deploy your benchmark.","source":"@site/docs/guidance/benchmarking/3-benchmarking-with-inference-perf/3-complete-deployment-example-guide.md","sourceDirName":"guidance/benchmarking/3-benchmarking-with-inference-perf","slug":"/guidance/benchmarking/benchmarking-with-inference-perf/complete-deployment-example-guide","permalink":"/ai-on-eks/docs/guidance/benchmarking/benchmarking-with-inference-perf/complete-deployment-example-guide","draft":false,"unlisted":false,"editUrl":"https://github.com/awslabs/ai-on-eks/blob/main/website/docs/guidance/benchmarking/3-benchmarking-with-inference-perf/3-complete-deployment-example-guide.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_label":"Complete Deployment Example"},"sidebar":"guidance","previous":{"title":"Running Inference Perf on EKS","permalink":"/ai-on-eks/docs/guidance/benchmarking/benchmarking-with-inference-perf/running-inference-perf-on-eks"},"next":{"title":"Test Scenarios","permalink":"/ai-on-eks/docs/guidance/benchmarking/test-scenarios/"}}');var a=r(74848),s=r(28453);const i={sidebar_label:"Complete Deployment Example"},o="Complete Deployment Example",c={},l=[{value:"STEP 0: ENVIRONMENT SETUP (OPTIONAL)",id:"step-0-environment-setup-optional",level:2},{value:"Path A (Recommended): Use ai-on-eks blueprint",id:"path-a-recommended-use-ai-on-eks-blueprint",level:3},{value:"Path B:  Existing EKS Cluster",id:"path-b--existing-eks-cluster",level:3},{value:"STEP 1: Deploy Inference Model",id:"step-1-deploy-inference-model",level:2},{value:"STEP 2:  AWS Storage Setup (Using S3 - Recommendation)",id:"step-2--aws-storage-setup-using-s3---recommendation",level:2},{value:"STEP 3: Deploy Benchmark Resources",id:"step-3-deploy-benchmark-resources",level:2},{value:"Option A: Using Helm Chart (Recommended)",id:"option-a-using-helm-chart-recommended",level:3},{value:"Option B: Manual Kubernetes YAML (Educational)",id:"option-b-manual-kubernetes-yaml-educational",level:3},{value:"Handling Model Dependencies",id:"handling-model-dependencies",level:4},{value:"Approach A: Runtime Installation (Recommended - Simple)",id:"approach-a-runtime-installation-recommended---simple",level:4},{value:"Approach B: Custom Container Image (Advanced)",id:"approach-b-custom-container-image-advanced",level:4},{value:"When to use each approach:",id:"when-to-use-each-approach",level:4},{value:"Create Namespace and Service Account",id:"create-namespace-and-service-account",level:3},{value:"Create HuggingFace Token Secret (Optional but Recommended)",id:"create-huggingface-token-secret-optional-but-recommended",level:3},{value:"Create ConfigMap and Job",id:"create-configmap-and-job",level:3},{value:"STEP 4: Deploy and Monitor",id:"step-4-deploy-and-monitor",level:2},{value:"For Helm Deployments:",id:"for-helm-deployments",level:3},{value:"For Manual YAML Deployments:",id:"for-manual-yaml-deployments",level:3},{value:"STEP 5: Retrieve Results",id:"step-5-retrieve-results",level:2},{value:"With S3 Storage (Recommended):",id:"with-s3-storage-recommended",level:3},{value:"With Local Storage (Alternative):",id:"with-local-storage-alternative",level:3},{value:"Storage Comparison:",id:"storage-comparison",level:3}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components},{Details:r}=n;return r||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"complete-deployment-example",children:"Complete Deployment Example"})}),"\n",(0,a.jsx)(n.p,{children:"This example demonstrates a production-ready deployment with S3 storage, realistic load testing, and proper AWS integration. Follow these steps to deploy your benchmark."}),"\n",(0,a.jsx)(n.h2,{id:"step-0-environment-setup-optional",children:"STEP 0: ENVIRONMENT SETUP (OPTIONAL)"}),"\n",(0,a.jsx)(n.p,{children:"Choose your deployment path:"}),"\n",(0,a.jsx)(n.h3,{id:"path-a-recommended-use-ai-on-eks-blueprint",children:"Path A (Recommended): Use ai-on-eks blueprint"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Deploys kube-prometheus-stack automatically"}),"\n",(0,a.jsxs)(n.li,{children:["Fixed Prometheus URL: ",(0,a.jsx)(n.a,{href:"http://kube-prometheus-stack-prometheus.monitoring:9090",children:"http://kube-prometheus-stack-prometheus.monitoring:9090"})]}),"\n",(0,a.jsxs)(n.li,{children:["Follow: ",(0,a.jsx)(n.a,{href:"https://awslabs.github.io/ai-on-eks/docs/infra/inference-ready-cluster",children:"https://awslabs.github.io/ai-on-eks/docs/infra/inference-ready-cluster"})]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"path-b--existing-eks-cluster",children:"Path B:  Existing EKS Cluster"}),"\n",(0,a.jsx)(n.p,{children:"If you have an existing cluster, ensure these prerequisites:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"EKS cluster with Kubernetes 1.28+"}),"\n",(0,a.jsx)(n.li,{children:"GPU nodes (g5.xlarge or larger) with NVIDIA drivers installed"}),"\n",(0,a.jsx)(n.li,{children:"Karpenter (optional but recommended for autoscaling)"}),"\n",(0,a.jsx)(n.li,{children:"Pod Identity or IRSA configured for S3 access"}),"\n",(0,a.jsx)(n.li,{children:"kubectl configured with cluster access"}),"\n",(0,a.jsx)(n.li,{children:"MUST have Prometheus pre-deployed"}),"\n",(0,a.jsx)(n.li,{children:"Metrics collection is OPTIONAL for benchmarking"}),"\n",(0,a.jsx)(n.li,{children:"You must know your Prometheus service name and namespace"}),"\n",(0,a.jsx)(n.li,{children:"Example: http://<your-prometheus-service>.<namespace>:9090"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"step-1-deploy-inference-model",children:"STEP 1: Deploy Inference Model"}),"\n",(0,a.jsx)(n.p,{children:"Before running benchmarks, you need an active LLM inference endpoint."}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Path A Users:"})," If you deployed using the ai-on-eks blueprint with a pre-configured inference deployment, skip to STEP 2."]}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Path B Users:"})," Deploy vLLM with your chosen model using the inference-charts:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"# Add the AI on EKS Helm repository\nhelm repo add ai-on-eks https://awslabs.github.io/ai-on-eks-charts/\nhelm repo update\n\n# Deploy Qwen3-8B with vLLM\nhelm install qwen3-vllm ai-on-eks/inference-charts \\\n  --set model=Qwen/Qwen3-8B \\\n  --set inference.framework=vllm \\\n  --namespace default --create-namespace\n\n# Verify deployment\nkubectl get pods -n default -l app.kubernetes.io/name=inference-charts\nkubectl logs -n default -l app.kubernetes.io/name=inference-charts -f\n"})}),"\n",(0,a.jsx)(n.p,{children:"Wait for the model to be ready before proceeding to benchmarking. This typically takes 3-10 minutes depending on model size and download speed."}),"\n",(0,a.jsx)(n.h2,{id:"step-2--aws-storage-setup-using-s3---recommendation",children:"STEP 2:  AWS Storage Setup (Using S3 - Recommendation)"}),"\n",(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"Note:"})," If you deployed your cluster using the ai-on-eks inference-ready-cluster blueprint, the EKS Pod Identity Agent addon is already installed. You can skip the addon installation command below and proceed directly to creating the S3 bucket and IAM role."]}),"\n",(0,a.jsx)(n.p,{children:"Set up AWS credentials so your benchmark pod can write results to S3 without hardcoded credentials."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:'# Create S3 bucket for benchmark results\nexport BUCKET_NAME="inference-perf-results-$(aws sts get-caller-identity --query Account --output text)"\naws s3 mb s3://${BUCKET_NAME} --region us-west-2\n\n# Install EKS Pod Identity Agent (already deployed on the blueprint reference - https://awslabs.github.io/ai-on-eks/docs/infra/inference-ready-cluster)\n\naws eks create-addon \\\n  --cluster-name my-cluster \\\n  --addon-name eks-pod-identity-agent \\\n  --addon-version v1.3.0-eksbuild.1\n\n# Create IAM role with S3 permissions\n\ncat > trust-policy.json <<EOF\n{\n  "Version": "2012-10-17",\n  "Statement": [{\n    "Effect": "Allow",\n    "Principal": {\n      "Service": "pods.eks.amazonaws.com"\n    },\n    "Action": [\n      "sts:AssumeRole",\n      "sts:TagSession"\n    ]\n  }]\n}\nEOF\n\n\naws iam create-role \\\n  --role-name InferencePerfRole \\\n  --assume-role-policy-document file://trust-policy.json\n\n\n\naws iam attach-role-policy \\\n  --role-name InferencePerfRole \\\n  --policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess\n\n\n\n# Link the role to your Kubernetes service account\n\naws eks create-pod-identity-association \\\n  --cluster-name my-cluster \\\n  --namespace benchmarking \\\n  --service-account inference-perf-sa \\\n  --role-arn arn:aws:iam::ACCOUNT_ID:role/InferencePerfRole\n'})}),"\n",(0,a.jsx)(n.h2,{id:"step-3-deploy-benchmark-resources",children:"STEP 3: Deploy Benchmark Resources"}),"\n",(0,a.jsx)(n.h3,{id:"option-a-using-helm-chart-recommended",children:"Option A: Using Helm Chart (Recommended)"}),"\n",(0,a.jsxs)(n.p,{children:["The ",(0,a.jsx)(n.a,{href:"https://github.com/awslabs/ai-on-eks-charts/tree/main/charts/benchmark-charts",children:"AI on EKS Benchmark Helm Chart"})," provides a production-ready deployment with simplified configuration management."]}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Install the benchmark:"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"# Add the AI on EKS Helm repository\nhelm repo add ai-on-eks https://awslabs.github.io/ai-on-eks-charts/\nhelm repo update\n\n# Deploy a production simulation test\nhelm install production-test ai-on-eks/benchmark-charts \\\n  --set benchmark.scenario=production \\\n  --set benchmark.target.baseUrl=http://qwen3-vllm.default:8000 \\\n  --set benchmark.target.modelName=qwen3-8b \\\n  --set benchmark.target.tokenizerPath=Qwen/Qwen3-8B \\\n  --namespace benchmarking --create-namespace\n"})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Customize with your own values:"})}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-yaml",children:'# custom-benchmark.yaml\nbenchmark:\n  scenario: production\n  target:\n    baseUrl: http://qwen3-vllm.default:8000\n    modelName: qwen3-8b\n    tokenizerPath: Qwen/Qwen3-8B\n\n  # S3 storage configuration\n  storage:\n    s3:\n      enabled: true\n      bucketName: inference-perf-results\n      path: "inference-perf/results"\n\n  # Pod affinity for same-AZ placement\n  affinity:\n    enabled: true\n    targetLabels:\n      app.kubernetes.io/component: qwen3-vllm\n\n  # Resource allocation\n  resources:\n    requests:\n      cpu: "2"\n      memory: "4Gi"\n    limits:\n      cpu: "4"\n      memory: "8Gi"\n'})}),"\n",(0,a.jsx)(n.p,{children:"Deploy with custom values:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"helm install production-test ai-on-eks/benchmark-charts \\\n  -f custom-benchmark.yaml \\\n  --namespace benchmarking --create-namespace\n"})}),"\n",(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Benefits of Helm approach:"})}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Simplified configuration"})," through values.yaml instead of verbose YAML"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Pre-configured scenarios"})," (baseline, saturation, sweep, production)"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Consistent defaults"})," for pod affinity, resources, and dependencies"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Easy upgrades"})," and rollbacks with Helm versioning"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"option-b-manual-kubernetes-yaml-educational",children:"Option B: Manual Kubernetes YAML (Educational)"}),"\n",(0,a.jsx)(n.p,{children:"For learning purposes or highly customized deployments, you can deploy directly with Kubernetes manifests. This approach provides full transparency of all resources."}),"\n",(0,a.jsxs)(r,{children:[(0,a.jsx)("summary",{children:(0,a.jsx)("strong",{children:"Click to expand: Manual YAML deployment instructions"})}),(0,a.jsx)(n.h4,{id:"handling-model-dependencies",children:"Handling Model Dependencies"}),(0,a.jsxs)(n.p,{children:["Some models require additional Python packages that aren't included in the base inference-perf container. For example, ",(0,a.jsx)(n.code,{children:"sentencepiece"})," is needed for Mistral and Llama models. Qwen3 models use tiktoken which is already included, so no additional packages are required."]}),(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Two approaches:"})}),(0,a.jsx)(n.h4,{id:"approach-a-runtime-installation-recommended---simple",children:"Approach A: Runtime Installation (Recommended - Simple)"}),(0,a.jsx)(n.p,{children:"Install dependencies as part of the main container startup before running the benchmark:"}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-yaml",children:'spec:\n  backoffLimit: 2\n  ttlSecondsAfterFinished: 3600\n  template:\n    metadata:\n      labels:\n        app: inference-perf\n    spec:\n      restartPolicy: Never\n      serviceAccountName: inference-perf-sa\n\n      ...\n\n      containers:\n      - name: inference-perf\n        image: quay.io/inference-perf/inference-perf:v0.2.0\n        command: ["/bin/sh", "-c"]\n        args:\n          - |\n            echo "Installing dependencies..."\n            pip install --no-cache-dir sentencepiece==0.2.0 protobuf==5.29.2\n            echo "Dependencies installed successfully"\n            echo "Starting inference-perf..."\n            inference-perf --config_file /workspace/config.yml\n\n'})}),(0,a.jsx)(n.h4,{id:"approach-b-custom-container-image-advanced",children:"Approach B: Custom Container Image (Advanced)"}),(0,a.jsx)(n.p,{children:"Build a custom image with dependencies pre-installed:"}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-dockerfile",children:"FROM quay.io/inference-perf/inference-perf:v0.2.0\n\nRUN pip install --no-cache-dir sentencepiece==0.2.0 protobuf==5.29.2\n"})}),(0,a.jsx)(n.h4,{id:"when-to-use-each-approach",children:"When to use each approach:"}),(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["Use ",(0,a.jsx)(n.strong,{children:"Approach A"})," for quick testing and flexibility"]}),"\n",(0,a.jsxs)(n.li,{children:["Use ",(0,a.jsx)(n.strong,{children:"Approach B"})," for production repeatability and faster startup"]}),"\n"]}),(0,a.jsx)(n.h3,{id:"create-namespace-and-service-account",children:"Create Namespace and Service Account"}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"cat <<EOF | kubectl apply -f -\n# Namespace for benchmark workloads\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: benchmarking\n\n---\n# Service Account (linked to AWS IAM via Pod Identity)\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: inference-perf-sa\n  namespace: benchmarking\nEOF\n"})}),(0,a.jsx)(n.h3,{id:"create-huggingface-token-secret-optional-but-recommended",children:"Create HuggingFace Token Secret (Optional but Recommended)"}),(0,a.jsx)(n.p,{children:"If your model requires authentication to download tokenizers from HuggingFace, create a secret using the command below. This approach is more secure than defining secrets in YAML files that might accidentally be committed to version control."}),(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Step 1: Obtain your HuggingFace token"})}),(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["Go to ",(0,a.jsx)(n.a,{href:"https://huggingface.co/settings/tokens",children:"https://huggingface.co/settings/tokens"})]}),"\n",(0,a.jsx)(n.li,{children:"Create a read token if you don't have one"}),"\n"]}),(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Step 2: Create the secret"})}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"kubectl create secret generic hf-token \\\n  --from-literal=token=YOUR_HUGGINGFACE_TOKEN_HERE \\\n  --namespace=benchmarking\n"})}),(0,a.jsx)(n.p,{children:(0,a.jsx)(n.strong,{children:"Step 3: Verify the secret was created"})}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"kubectl get secret hf-token -n benchmarking\n"})}),(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"\u26a0\ufe0f Security Note:"})," Never commit secrets to Git repositories. Always use imperative commands or external secret management tools (AWS Secrets Manager, HashiCorp Vault, etc.) for production deployments."]}),(0,a.jsx)(n.h3,{id:"create-configmap-and-job",children:"Create ConfigMap and Job"}),(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:'cat <<EOF | kubectl apply -f -\n# Benchmark Configuration\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: inference-perf-config\n  namespace: benchmarking\ndata:\n  config.yml: |\n    # API Configuration\n    api:\n      type: completion\n      streaming: true\n    # Data Generation - synthetic with realistic distributions\n    data:\n      type: synthetic\n      input_distribution:\n        mean: 512\n        std_dev: 128\n        min: 128\n        max: 2048\n      output_distribution:\n        mean: 256\n        std_dev: 64\n        min: 32\n        max: 512\n    # Load Pattern - Poisson arrivals at 10 QPS for 5 minutes\n    load:\n      type: poisson\n      stages:\n        - rate: 10\n          duration: 300\n      num_workers: 4\n    # Model Server\n    server:\n      type: vllm\n      model_name: qwen3-8b\n      base_url: http://qwen3-vllm.default:8000\n      ignore_eos: true\n\n    # Tokenizer\n    tokenizer:\n      pretrained_model_name_or_path: Qwen/Qwen3-8B\n\n    # Storage - Results automatically saved to S3\n    storage:\n      simple_storage_service:\n        bucket_name: "inference-perf-results"\n        path: "inference-perf/results"\n    # Optional: Prometheus metrics collection\n    # metrics:\n    #   type: prometheus\n    #   prometheus:\n    #     url: http://kube-prometheus-stack-prometheus.monitoring:9090\n    #     scrape_interval: 15\nEOF\n---\n\ncat <<EOF | kubectl apply -f -\n# Benchmark Job\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: inference-perf-run\n  namespace: benchmarking\n  labels:\n    app: inference-perf\nspec:\n  backoffLimit: 2\n  ttlSecondsAfterFinished: 3600\n  template:\n    metadata:\n      labels:\n        app: inference-perf\n    spec:\n      restartPolicy: Never\n      serviceAccountName: inference-perf-sa\n\n      # Same-AZ placement with inference pods for reproducible results\n      affinity:\n        podAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                app.kubernetes.io/component: qwen3-vllm\n            topologyKey: topology.kubernetes.io/zone\n\n      containers:\n      - name: inference-perf\n        image: quay.io/inference-perf/inference-perf:v0.2.0\n        command: ["/bin/sh", "-c"]\n        args:\n          - |\n            echo "Starting inference-perf..."\n            inference-perf --config_file /workspace/config.yml\n        volumeMounts:\n          - name: config\n            mountPath: /workspace/config.yml\n            subPath: config.yml\n        env:\n          - name: HF_TOKEN\n            valueFrom:\n              secretKeyRef:\n                name: hf-token\n                key: token\n                optional: true\n        resources:\n          requests:\n            cpu: "2"\n            memory: "4Gi"\n          limits:\n            cpu: "4"\n            memory: "8Gi"\n      volumes:\n        - name: config\n          configMap:\n            name: inference-perf-config\nEOF\n'})}),(0,a.jsxs)(n.p,{children:[(0,a.jsx)(n.strong,{children:"\ud83d\udca1 Tip:"})," The resource values shown are starting points. For higher concurrency levels or longer test durations, monitor pod resource usage with ",(0,a.jsx)(n.code,{children:"kubectl top pod -n benchmarking"})," and adjust accordingly."]})]}),"\n",(0,a.jsx)(n.hr,{}),"\n",(0,a.jsx)(n.h2,{id:"step-4-deploy-and-monitor",children:"STEP 4: Deploy and Monitor"}),"\n",(0,a.jsx)(n.h3,{id:"for-helm-deployments",children:"For Helm Deployments:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"# Monitor job progress\nkubectl get jobs -n benchmarking -w\n\n# Follow logs to see benchmark progress\nkubectl logs -n benchmarking -l app.kubernetes.io/component=benchmark -f\n\n# Check Helm release status\nhelm status production-test -n benchmarking\n"})}),"\n",(0,a.jsx)(n.h3,{id:"for-manual-yaml-deployments",children:"For Manual YAML Deployments:"}),"\n",(0,a.jsxs)(n.p,{children:["Save the manifests from Option B above as ",(0,a.jsx)(n.code,{children:"inference-perf-complete.yaml"})," and deploy:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"# Deploy all resources\nkubectl apply -f inference-perf-complete.yaml\n\n# Monitor job progress\nkubectl get jobs -n benchmarking -w\n\n# Follow logs to see benchmark progress\nkubectl logs -n benchmarking -l app=inference-perf -f\n"})}),"\n",(0,a.jsx)(n.h2,{id:"step-5-retrieve-results",children:"STEP 5: Retrieve Results"}),"\n",(0,a.jsx)(n.h3,{id:"with-s3-storage-recommended",children:"With S3 Storage (Recommended):"}),"\n",(0,a.jsx)(n.p,{children:"Results are automatically uploaded to your S3 bucket. Access them directly:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"# List results in S3 (use bucket name from STEP 2)\naws s3 ls s3://${BUCKET_NAME}/inference-perf/ --recursive\n\n# Download specific report\naws s3 cp s3://${BUCKET_NAME}/inference-perf/20251020-143000/summary_lifecycle_metrics.json ./\n"})}),"\n",(0,a.jsx)(n.h3,{id:"with-local-storage-alternative",children:"With Local Storage (Alternative):"}),"\n",(0,a.jsxs)(n.p,{children:["If using ",(0,a.jsx)(n.code,{children:"local_storage"})," instead of S3, you must manually copy results before the pod terminates:"]}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"# In config.yml, use:\n\nstorage:\n\n  local_storage:\n\n    path: \"reports-results\"\n\n\n\n# Get pod name\n\nPOD_NAME=$(kubectl get pods -n benchmarking -l app=inference-perf -o jsonpath='{.items[0].metadata.name}')\n\n\n\n# Copy results from pod\n\nkubectl cp benchmarking/$POD_NAME:/reports-* ./local-reports/\n"})}),"\n",(0,a.jsx)(n.h3,{id:"storage-comparison",children:"Storage Comparison:"}),"\n",(0,a.jsxs)(n.table,{children:[(0,a.jsx)(n.thead,{children:(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.th,{children:"Feature"}),(0,a.jsx)(n.th,{children:"Local Storage"}),(0,a.jsx)(n.th,{children:"S3 Storage"})]})}),(0,a.jsxs)(n.tbody,{children:[(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"Setup"}),(0,a.jsx)(n.td,{children:"None required"}),(0,a.jsx)(n.td,{children:"AWS credentials needed"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"Persistence"}),(0,a.jsx)(n.td,{children:"Manual copy required"}),(0,a.jsx)(n.td,{children:"Automatic"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"Best for"}),(0,a.jsx)(n.td,{children:"Quick tests, experimentation"}),(0,a.jsx)(n.td,{children:"Production, automation"})]}),(0,a.jsxs)(n.tr,{children:[(0,a.jsx)(n.td,{children:"Results access"}),(0,a.jsx)(n.td,{children:"kubectl cp command"}),(0,a.jsx)(n.td,{children:"AWS S3 commands/console"})]})]})]})]})}function p(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}}}]);