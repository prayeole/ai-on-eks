"use strict";(self.webpackChunkdoeks_website=self.webpackChunkdoeks_website||[]).push([[3887],{28453:(e,n,s)=>{s.d(n,{R:()=>o,x:()=>a});var l=s(96540);const r={},i=l.createContext(r);function o(e){const n=l.useContext(i);return l.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),l.createElement(i.Provider,{value:n},e.children)}},89220:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>h,frontMatter:()=>o,metadata:()=>l,toc:()=>t});const l=JSON.parse('{"id":"blueprints/inference/inference-charts","title":"AI on EKS Inference Charts","description":"The AI on EKS Inference Charts provide a streamlined Helm-based approach to deploy AI/ML inference workloads on both GPU","source":"@site/docs/blueprints/inference/inference-charts.md","sourceDirName":"blueprints/inference","slug":"/blueprints/inference/inference-charts","permalink":"/ai-on-eks/docs/blueprints/inference/inference-charts","draft":false,"unlisted":false,"editUrl":"https://github.com/awslabs/ai-on-eks/blob/main/website/docs/blueprints/inference/inference-charts.md","tags":[],"version":"current","frontMatter":{"sidebar_label":"Inference Charts"},"sidebar":"blueprints","previous":{"title":"Overview","permalink":"/ai-on-eks/docs/blueprints/inference/"},"next":{"title":"Envoy Gateway implementation on EKS","permalink":"/ai-on-eks/docs/blueprints/gateways/envoy-gateway"}}');var r=s(74848),i=s(28453);const o={sidebar_label:"Inference Charts"},a="AI on EKS Inference Charts",c={},t=[{value:"Overview",id:"overview",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Quick Start",id:"quick-start",level:2},{value:"1. Create Hugging Face Token Secret",id:"1-create-hugging-face-token-secret",level:3},{value:"2. Deploy a Pre-configured Model",id:"2-deploy-a-pre-configured-model",level:3},{value:"Supported Models",id:"supported-models",level:2},{value:"Language Models",id:"language-models",level:3},{value:"Diffusion Models",id:"diffusion-models",level:3},{value:"Neuron-Optimized Models",id:"neuron-optimized-models",level:3},{value:"Deployment Examples",id:"deployment-examples",level:2},{value:"Language Model Deployments",id:"language-model-deployments",level:3},{value:"Diffusion Model Deployments",id:"diffusion-model-deployments",level:3},{value:"Neuron Deployments",id:"neuron-deployments",level:3},{value:"S3 Model Copy",id:"s3-model-copy",level:3},{value:"Custom S3 Model Copy",id:"custom-s3-model-copy",level:4},{value:"Configuration",id:"configuration",level:2},{value:"Key Parameters",id:"key-parameters",level:3},{value:"Custom Configuration",id:"custom-configuration",level:3},{value:"API Usage",id:"api-usage",level:2},{value:"VLLM/Ray-VLLM",id:"vllmray-vllm",level:3},{value:"Triton-VLLM",id:"triton-vllm",level:3},{value:"Diffusers",id:"diffusers",level:3},{value:"Example Usage",id:"example-usage",level:3},{value:"Troubleshooting",id:"troubleshooting",level:2},{value:"Common Issues",id:"common-issues",level:3},{value:"Logs",id:"logs",level:3},{value:"Check Logs",id:"check-logs",level:3},{value:"Next Steps",id:"next-steps",level:2}];function d(e){const n={a:"a",admonition:"admonition",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,i.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"ai-on-eks-inference-charts",children:"AI on EKS Inference Charts"})}),"\n",(0,r.jsx)(n.p,{children:"The AI on EKS Inference Charts provide a streamlined Helm-based approach to deploy AI/ML inference workloads on both GPU\nand AWS Neuron (Inferentia/Trainium) hardware. This chart supports multiple deployment configurations and comes with\npre-configured values for popular models."}),"\n",(0,r.jsx)(n.admonition,{title:"Advanced Usage",type:"info",children:(0,r.jsxs)(n.p,{children:["For detailed configuration options, advanced deployment scenarios, and comprehensive parameter documentation, see\nthe ",(0,r.jsx)(n.a,{href:"https://github.com/awslabs/ai-on-eks-charts/blob/main/charts/inference-charts/README.md",children:"complete README"}),"."]})}),"\n",(0,r.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,r.jsx)(n.p,{children:"The inference charts support multiple deployment frameworks:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"VLLM"})," - Single-node inference with fast startup"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Ray-VLLM"})," - Distributed inference with autoscaling capabilities"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Triton-VLLM"})," - Production-ready inference server with advanced features"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"AIBrix"})," - VLLM with AIBrix-specific configurations"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"LeaderWorkerSet-VLLM"})," - Multi-node inference for large models"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Diffusers"})," - Hugging Face Diffusers for image generation"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"S3 Model Copy"})," - Download models from Hugging Face to S3 storage"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Both GPU and AWS Neuron (Inferentia/Trainium) accelerators are supported across these frameworks."}),"\n",(0,r.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,r.jsx)(n.p,{children:"Before deploying the inference charts, ensure you have:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Amazon EKS cluster with GPU or AWS Neuron\nnodes (",(0,r.jsx)(n.a,{href:"/ai-on-eks/docs/infra/inference-ready-cluster",children:"inference-ready cluster"})," for a quick start)"]}),"\n",(0,r.jsx)(n.li,{children:"Helm 3.0+"}),"\n",(0,r.jsx)(n.li,{children:"For GPU deployments: NVIDIA device plugin installed"}),"\n",(0,r.jsx)(n.li,{children:"For Neuron deployments: AWS Neuron device plugin installed"}),"\n",(0,r.jsx)(n.li,{children:"For LeaderWorkerSet deployments: LeaderWorkerSet CRD installed"}),"\n",(0,r.jsxs)(n.li,{children:["Hugging Face Hub token (stored as a Kubernetes secret named ",(0,r.jsx)(n.code,{children:"hf-token"}),")"]}),"\n",(0,r.jsx)(n.li,{children:"For Ray: KubeRay Infrastructure"}),"\n",(0,r.jsx)(n.li,{children:"For AIBrix: AIBrix Infrastructure"}),"\n",(0,r.jsx)(n.li,{children:"For S3 Model Copy: Service account with S3 write permissions"}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"quick-start",children:"Quick Start"}),"\n",(0,r.jsx)(n.h3,{id:"1-create-hugging-face-token-secret",children:"1. Create Hugging Face Token Secret"}),"\n",(0,r.jsxs)(n.p,{children:["Create a Kubernetes secret with your ",(0,r.jsx)(n.a,{href:"https://huggingface.co/docs/hub/en/security-tokens",children:"Hugging Face token"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"kubectl create secret generic hf-token --from-literal=token=your_huggingface_token\n"})}),"\n",(0,r.jsx)(n.h3,{id:"2-deploy-a-pre-configured-model",children:"2. Deploy a Pre-configured Model"}),"\n",(0,r.jsx)(n.p,{children:"Choose from the available pre-configured models and deploy:"}),"\n",(0,r.jsx)(n.admonition,{type:"warning",children:(0,r.jsxs)(n.p,{children:["These deployments will need GPU/Neuron resources which need to\nbe ",(0,r.jsx)(n.a,{href:"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-resource-limits.html",children:"enabled"})," and cost more than CPU only\ninstances."]})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Add the charts repository\nhelm repo add ai-on-eks https://awslabs.github.io/ai-on-eks-charts/\nhelm repo update\n\n# Deploy Qwen 3 1.7B on GPU with vLLM\nhelm install qwen3-inference ai-on-eks/inference-charts \\\n  --values https://raw.githubusercontent.com/awslabs/ai-on-eks-charts/refs/heads/main/charts/inference-charts/values-qwen3-1.7b-vllm.yaml\n\n# Deploy DeepSeek R1 Distill on GPU with Ray-vLLM\nhelm install deepseek-inference ai-on-eks/inference-charts \\\n  --values https://raw.githubusercontent.com/awslabs/ai-on-eks-charts/refs/heads/main/charts/inference-charts/values-deepseek-r1-distill-llama-8b-ray-vllm-gpu.yaml\n"})}),"\n",(0,r.jsx)(n.h2,{id:"supported-models",children:"Supported Models"}),"\n",(0,r.jsx)(n.p,{children:"The inference charts include pre-configured values files for popular models across different categories:"}),"\n",(0,r.jsx)(n.h3,{id:"language-models",children:"Language Models"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"DeepSeek R1 Distill Llama 8B"})," - Advanced reasoning model"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Llama 3.2 1B"})," - Lightweight language model"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Llama 4 Scout 17B"})," - Mid-size language model"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Mistral Small 24B"})," - Efficient large language model"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"GPT OSS 20B"})," - Open-source GPT variant"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Qwen3 1.7B"})," - Compact multilingual language model"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"diffusion-models",children:"Diffusion Models"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"FLUX.1 Schnell"})," - Fast text-to-image generation"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Stable Diffusion XL"})," - High-quality image generation"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Stable Diffusion 3.5"})," - Latest SD model with enhanced capabilities"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Kolors"})," - Artistic image generation"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"OmniGen"})," - Multi-modal generation"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"neuron-optimized-models",children:"Neuron-Optimized Models"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Llama 2 13B"})," - Optimized for AWS Inferentia"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Llama 3 70B"})," - Large model on Inferentia"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Llama 3.1 8B"})," - Efficient Inferentia deployment"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Each model comes with optimized configurations for different frameworks (VLLM, Ray-VLLM, Triton-VLLM, etc.)."}),"\n",(0,r.jsx)(n.h2,{id:"deployment-examples",children:"Deployment Examples"}),"\n",(0,r.jsx)(n.h3,{id:"language-model-deployments",children:"Language Model Deployments"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Add the charts repository\nhelm repo add ai-on-eks https://awslabs.github.io/ai-on-eks-charts/\nhelm repo update\n\n# Deploy Qwen 3 1.7B on GPU with vLLM\nhelm install qwen3-inference ai-on-eks/inference-charts \\\n  --values https://raw.githubusercontent.com/awslabs/ai-on-eks-charts/refs/heads/main/charts/inference-charts/values-qwen3-1.7b-vllm.yaml\n\n# Deploy DeepSeek R1 Distill on GPU with Ray-vLLM\nhelm install deepseek-inference ai-on-eks/inference-charts \\\n  --values https://raw.githubusercontent.com/awslabs/ai-on-eks-charts/refs/heads/main/charts/inference-charts/values-deepseek-r1-distill-llama-8b-ray-vllm-gpu.yaml\n\n# Deploy Llama 4 Scout 17B with LeaderWorkerSet-VLLM\nhelm install llama4-lws ai-on-eks/inference-charts \\\n  --values https://raw.githubusercontent.com/awslabs/ai-on-eks-charts/refs/heads/main/charts/inference-charts/values-llama-4-scout-17b-lws-vllm.yaml\n"})}),"\n",(0,r.jsx)(n.h3,{id:"diffusion-model-deployments",children:"Diffusion Model Deployments"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Deploy FLUX.1 Schnell for image generation\nhelm install flux-diffusers ai-on-eks/inference-charts \\\n  --values https://raw.githubusercontent.com/awslabs/ai-on-eks-charts/refs/heads/main/charts/inference-charts/values-flux-1-diffusers.yaml\n\n# Deploy Stable Diffusion XL\nhelm install sdxl-diffusers ai-on-eks/inference-charts \\\n  --values https://raw.githubusercontent.com/awslabs/ai-on-eks-charts/refs/heads/main/charts/inference-charts/values-stable-diffusion-xl-base-1-diffusers.yaml\n"})}),"\n",(0,r.jsx)(n.h3,{id:"neuron-deployments",children:"Neuron Deployments"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Deploy Llama 3.1 8B on Inferentia\nhelm install llama31-neuron ai-on-eks/inference-charts \\\n  --values https://raw.githubusercontent.com/awslabs/ai-on-eks-charts/refs/heads/main/charts/inference-charts/values-llama-31-8b-vllm-neuron.yaml\n\n# Deploy Llama 3 70B with Ray-VLLM on Inferentia\nhelm install llama3-70b-neuron ai-on-eks/inference-charts \\\n  --values https://raw.githubusercontent.com/awslabs/ai-on-eks-charts/refs/heads/main/charts/inference-charts/values-llama-3-70b-ray-vllm-neuron.yaml\n"})}),"\n",(0,r.jsx)(n.h3,{id:"s3-model-copy",children:"S3 Model Copy"}),"\n",(0,r.jsx)(n.p,{children:"The S3 Model Copy feature allows you to download models from Hugging Face Hub and upload them to S3 storage. This is\nuseful for:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Pre-staging models in S3 for faster deployment"}),"\n",(0,r.jsx)(n.li,{children:"Creating model repositories in private S3 buckets"}),"\n",(0,r.jsx)(n.li,{children:"Reducing inference startup time by leveraging AWS internal network"}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Copy Llama 3 8B model from Hugging Face to S3\nhelm install s3-copy-llama3 ai-on-eks/inference-charts \\\n  --values https://raw.githubusercontent.com/awslabs/ai-on-eks-charts/refs/heads/main/charts/inference-charts/values-s3-copy-llama3-8b.yaml\n"})}),"\n",(0,r.jsx)(n.h4,{id:"custom-s3-model-copy",children:"Custom S3 Model Copy"}),"\n",(0,r.jsx)(n.p,{children:"Create a custom values file for copying any model to S3:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:"s3ModelCopy:\n  namespace: default\n  model: deepseek-ai/DeepSeek-R1\n  s3Path: my-models-bucket/ # Model will be copied as s3://my-models-bucket/deepseek-ai/DeepSeek-R1\n\nserviceAccountName: s3-copy-service-account  # Service account with S3 write permissions\n"})}),"\n",(0,r.jsx)(n.p,{children:"Deploy the S3 copy job:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"helm install custom-s3-copy ai-on-eks/inference-charts \\\n  --values custom-s3-copy-values.yaml\n"})}),"\n",(0,r.jsx)(n.admonition,{title:"S3 Permissions",type:"info",children:(0,r.jsxs)(n.p,{children:["The service account needs IAM permissions to write to your target S3 bucket. Consider\nusing ",(0,r.jsx)(n.a,{href:"https://docs.aws.amazon.com/eks/latest/userguide/pod-identities.html",children:"Pod Identity"})," to grant the service account\npermission to S3."]})}),"\n",(0,r.jsx)(n.h2,{id:"configuration",children:"Configuration"}),"\n",(0,r.jsx)(n.h3,{id:"key-parameters",children:"Key Parameters"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Parameter"}),(0,r.jsx)(n.th,{children:"Description"}),(0,r.jsx)(n.th,{children:"Default"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"inference.accelerator"})}),(0,r.jsxs)(n.td,{children:["Accelerator type (",(0,r.jsx)(n.code,{children:"gpu"})," or ",(0,r.jsx)(n.code,{children:"neuron"}),")"]}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"gpu"})})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"inference.framework"})}),(0,r.jsxs)(n.td,{children:["Framework (",(0,r.jsx)(n.code,{children:"vllm"}),", ",(0,r.jsx)(n.code,{children:"ray-vllm"}),", ",(0,r.jsx)(n.code,{children:"triton-vllm"}),", ",(0,r.jsx)(n.code,{children:"aibrix"}),", etc.)"]}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"vllm"})})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"inference.serviceName"})}),(0,r.jsx)(n.td,{children:"Name of the inference service"}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"inference"})})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"inference.modelServer.deployment.replicas"})}),(0,r.jsx)(n.td,{children:"Number of replicas"}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"1"})})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"model"})}),(0,r.jsx)(n.td,{children:"Model ID from Hugging Face Hub"}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"NousResearch/Llama-3.2-1B"})})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"modelParameters.gpuMemoryUtilization"})}),(0,r.jsx)(n.td,{children:"GPU memory utilization"}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"0.8"})})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"modelParameters.maxModelLen"})}),(0,r.jsx)(n.td,{children:"Maximum model sequence length"}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"8192"})})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"modelParameters.tensorParallelSize"})}),(0,r.jsx)(n.td,{children:"Tensor parallel size"}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"1"})})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"modelParameters.pipelineParallelSize"})}),(0,r.jsx)(n.td,{children:"Pipeline parallel size"}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"1"})})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"s3ModelCopy.namespace"})}),(0,r.jsx)(n.td,{children:"Namespace for S3 model copy job"}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"default"})})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"s3ModelCopy.model"})}),(0,r.jsx)(n.td,{children:"Hugging Face model ID to copy to S3"}),(0,r.jsx)(n.td,{children:"Not set"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"s3ModelCopy.s3Path"})}),(0,r.jsx)(n.td,{children:"S3 path where model should be uploaded"}),(0,r.jsx)(n.td,{children:"Not set"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"serviceAccountName"})}),(0,r.jsx)(n.td,{children:"Service account name"}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.code,{children:"default"})})]})]})]}),"\n",(0,r.jsx)(n.h3,{id:"custom-configuration",children:"Custom Configuration"}),"\n",(0,r.jsx)(n.p,{children:"Create a custom values file:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:'inference:\n  accelerator: gpu  # or neuron\n  framework: vllm   # vllm, ray-vllm, triton-vllm, aibrix, lws-vllm, diffusers\n  serviceName: my-inference\n  modelServer:\n    deployment:\n      replicas: 1\n      instanceType: g5.2xlarge\n\nmodel: "NousResearch/Llama-3.2-1B"\nmodelParameters:\n  gpuMemoryUtilization: 0.8\n  maxModelLen: 8192\n  tensorParallelSize: 1\n'})}),"\n",(0,r.jsx)(n.p,{children:"Deploy with custom values:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"helm install my-inference ai-on-eks/inference-charts \\\n  --values custom-values.yaml\n"})}),"\n",(0,r.jsx)(n.h2,{id:"api-usage",children:"API Usage"}),"\n",(0,r.jsx)(n.p,{children:"The deployed services expose different API endpoints based on the framework:"}),"\n",(0,r.jsx)(n.h3,{id:"vllmray-vllm",children:"VLLM/Ray-VLLM"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"/v1/models"})," - List available models"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"/v1/chat/completions"})," - Chat completion API"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"/v1/completions"})," - Text completion API"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"/metrics"})," - Prometheus metrics"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"triton-vllm",children:"Triton-VLLM"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"/v2/models"})," - List available models"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"/v2/models/vllm_model/generate"})," - Model inference"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"/v2/health/ready"})," - Health checks"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"diffusers",children:"Diffusers"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.code,{children:"/v1/generations"})," - Image generation API"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"example-usage",children:"Example Usage"}),"\n",(0,r.jsx)(n.p,{children:"Access your service via port-forward:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"kubectl port-forward svc/<service-name> 8000\n"})}),"\n",(0,r.jsx)(n.p,{children:"Test the API:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'# Chat completion (VLLM/Ray-VLLM)\ncurl -X POST http://localhost:8000/v1/chat/completions \\\n  -H "Content-Type: application/json" \\\n  -d \'{\n    "model": "your-model-name",\n    "messages": [{"role": "user", "content": "Hello!"}],\n    "max_tokens": 100\n  }\'\n\n# Image generation (Diffusers)\ncurl -X POST http://localhost:8000/v1/generations \\\n  -H \'Content-Type: application/json\' \\\n  -d \'{"prompt": "A beautiful sunset over mountains"}\'\n'})}),"\n",(0,r.jsx)(n.h2,{id:"troubleshooting",children:"Troubleshooting"}),"\n",(0,r.jsx)(n.h3,{id:"common-issues",children:"Common Issues"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Pod stuck in Pending state"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Check if GPU/Neuron nodes are available"}),"\n",(0,r.jsx)(n.li,{children:"Verify resource requests match available hardware"}),"\n",(0,r.jsx)(n.li,{children:"For LeaderWorkerSet deployments: Ensure LeaderWorkerSet CRD is installed"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Model download failures"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Ensure Hugging Face token is correctly configured as secret ",(0,r.jsx)(n.code,{children:"hf-token"})]}),"\n",(0,r.jsx)(n.li,{children:"Check network connectivity to Hugging Face Hub"}),"\n",(0,r.jsx)(n.li,{children:"Verify model ID is correct and accessible"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Out of memory errors"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Adjust ",(0,r.jsx)(n.code,{children:"gpuMemoryUtilization"})," parameter (try reducing from 0.8 to 0.7)"]}),"\n",(0,r.jsx)(n.li,{children:"Consider using tensor parallelism for larger models"}),"\n",(0,r.jsx)(n.li,{children:"For large models, use LeaderWorkerSet or Ray deployments with multiple GPUs"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Ray deployment issues"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Ensure KubeRay infrastructure is installed"}),"\n",(0,r.jsx)(n.li,{children:"Check Ray cluster status and worker connectivity"}),"\n",(0,r.jsx)(n.li,{children:"Verify Ray version compatibility"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Triton deployment issues"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Check Triton server logs for model loading errors"}),"\n",(0,r.jsx)(n.li,{children:"Verify model repository configuration"}),"\n",(0,r.jsx)(n.li,{children:"Ensure proper health check endpoints are accessible"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"logs",children:"Logs"}),"\n",(0,r.jsx)(n.p,{children:"Check deployment logs based on framework:"}),"\n",(0,r.jsx)(n.h3,{id:"check-logs",children:"Check Logs"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# VLLM deployments\nkubectl logs -l app.kubernetes.io/component=<service-name>\n\n# Ray deployments\nkubectl logs -l ray.io/node-type=head\nkubectl logs -l ray.io/node-type=worker\n\n# LeaderWorkerSet deployments\nkubectl logs -l leaderworkerset.sigs.k8s.io/role=leader\n"})}),"\n",(0,r.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["Explore ",(0,r.jsx)(n.a,{href:"/docs/category/gpu-inference-on-eks",children:"GPU-specific configurations"})," for GPU deployments"]}),"\n",(0,r.jsxs)(n.li,{children:["Learn about ",(0,r.jsx)(n.a,{href:"/docs/category/neuron-inference-on-eks",children:"Neuron-specific configurations"})," for Inferentia deployments"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}}}]);