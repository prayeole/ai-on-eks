"use strict";(self.webpackChunkdoeks_website=self.webpackChunkdoeks_website||[]).push([[5273],{28453:(e,n,r)=>{r.d(n,{R:()=>s,x:()=>c});var i=r(96540);const a={},t=i.createContext(a);function s(e){const n=i.useContext(t);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:s(e.components),i.createElement(t.Provider,{value:n},e.children)}},91303:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>o,contentTitle:()=>c,default:()=>d,frontMatter:()=>s,metadata:()=>i,toc:()=>l});const i=JSON.parse('{"id":"guidance/benchmarking/resources/index","title":"Resources","description":"Helm Chart Repository","source":"@site/docs/guidance/benchmarking/5-resources/index.md","sourceDirName":"guidance/benchmarking/5-resources","slug":"/guidance/benchmarking/resources/","permalink":"/ai-on-eks/docs/guidance/benchmarking/resources/","draft":false,"unlisted":false,"editUrl":"https://github.com/awslabs/ai-on-eks/blob/main/website/docs/guidance/benchmarking/5-resources/index.md","tags":[],"version":"current","frontMatter":{"sidebar_label":"Resources"},"sidebar":"guidance","previous":{"title":"Scenario 5 - Real Dataset Testing","permalink":"/ai-on-eks/docs/guidance/benchmarking/test-scenarios/real-dataset-testing"},"next":{"title":"Solving cold start challenges for AI/ML inference applications on Amazon EKS","permalink":"/ai-on-eks/docs/guidance/container-startup-time/"}}');var a=r(74848),t=r(28453);const s={sidebar_label:"Resources"},c="Resources",o={},l=[{value:"Helm Chart Repository",id:"helm-chart-repository",level:2},{value:"Customizing via values.yaml",id:"customizing-via-valuesyaml",level:3},{value:"Alternative: Custom Container with SentencePiece",id:"alternative-custom-container-with-sentencepiece",level:2},{value:"Alternative: Complete Kubernetes Manifest",id:"alternative-complete-kubernetes-manifest",level:2},{value:"Execution",id:"execution",level:2}];function u(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"resources",children:"Resources"})}),"\n",(0,a.jsx)(n.h2,{id:"helm-chart-repository",children:"Helm Chart Repository"}),"\n",(0,a.jsxs)(n.p,{children:["The official benchmark charts are maintained in the ",(0,a.jsx)(n.a,{href:"https://github.com/awslabs/ai-on-eks-charts/tree/main/charts/benchmark-charts",children:"AI on EKS Charts repository"}),". This repository contains:"]}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"values.yaml"}),": Complete configuration reference with all available options"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"templates/"}),": Kubernetes resource templates for jobs, configmaps, and service accounts"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"scenarios/"}),": Pre-configured scenario definitions (baseline, saturation, sweep, production)"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"README.md"}),": Detailed usage instructions and examples"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"customizing-via-valuesyaml",children:"Customizing via values.yaml"}),"\n",(0,a.jsx)(n.p,{children:"Create a custom values file to override defaults:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-yaml",children:'# custom-benchmark.yaml\nbenchmark:\n  scenario: saturation\n  target:\n    baseUrl: http://your-model.your-namespace:8000\n    modelName: your-model-name\n\n  # Override scenario-specific settings\n  scenarios:\n    saturation:\n      load:\n        stages:\n          - rate: 10\n            duration: 300\n          - rate: 50\n            duration: 300\n\n  # Resource allocation\n  resources:\n    requests:\n      cpu: "4"\n      memory: "8Gi"\n\n  # Pod affinity customization\n  affinity:\n    enabled: true\n    targetLabels:\n      app: your-inference-service\n'})}),"\n",(0,a.jsx)(n.p,{children:"Deploy with custom values:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"helm install my-benchmark ai-on-eks/benchmark-charts -f custom-benchmark.yaml -n benchmarking\n"})}),"\n",(0,a.jsx)(n.h2,{id:"alternative-custom-container-with-sentencepiece",children:"Alternative: Custom Container with SentencePiece"}),"\n",(0,a.jsx)(n.p,{children:"For custom deployments outside the Helm chart, you can build a container image with pre-installed dependencies:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:'# Create a custom Dockerfile\ncat > Dockerfile <<\'EOF\'\nFROM quay.io/inference-perf/inference-perf:v0.2.0\n\n# Install sentencepiece\nRUN pip install --no-cache-dir sentencepiece protobuf\n\nUSER 1000\nEOF\n\n# Build and push to your registry\ndocker build -t <your-registry>/inference-perf:v0.2.0-sentencepiece .\ndocker push <your-registry>/inference-perf:v0.2.0-sentencepiece\n\n# Update your Job to use the new image\nkubectl patch job inference-perf-run -n benchmarking \\\n  --type=\'json\' \\\n  -p=\'[{"op": "replace", "path": "/spec/template/spec/containers/0/image", "value":"<your-registry>/inference-perf:v0.2.0-sentencepiece"}]\'\n'})}),"\n",(0,a.jsx)(n.h2,{id:"alternative-complete-kubernetes-manifest",children:"Alternative: Complete Kubernetes Manifest"}),"\n",(0,a.jsx)(n.p,{children:"For manual deployments or educational purposes, here's a complete YAML manifest with runtime dependency installation:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:'cat > inference-perf-fixed.yaml <<\'EOF\'\n---\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: benchmarking\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: inference-perf-sa\n  namespace: benchmarking\n---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: inference-perf-config\n  namespace: benchmarking\ndata:\n  config.yml: |\n    load_generator:\n      concurrency: 10\n      duration: 60\n\n    model:\n      model_name: qwen3-8b\n      base_url: http://qwen3-vllm.default:8000\n      ignore_eos: true\n\n      tokenizer:\n        pretrained_model_name_or_path: Qwen/Qwen3-8B\n\n    storage:\n      simple_storage_service:\n        bucket_name: "inference-perf-results"\n        path: "inference-perf/results"\n---\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: inference-perf-run\n  namespace: benchmarking\n  labels:\n    app: inference-perf\nspec:\n  backoffLimit: 2\n  ttlSecondsAfterFinished: 3600\n  template:\n    metadata:\n      labels:\n        app: inference-perf\n    spec:\n      restartPolicy: Never\n      serviceAccountName: inference-perf-sa\n\n      # Same-AZ placement with inference pods for reproducible results\n      affinity:\n        podAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                app.kubernetes.io/component: qwen3-vllm\n            topologyKey: topology.kubernetes.io/zone\n\n      containers:\n      - name: inference-perf\n        image: quay.io/inference-perf/inference-perf:v0.2.0\n        command: ["/bin/sh", "-c"]\n        args:\n          - |\n            echo "Installing dependencies..."\n            pip install --no-cache-dir sentencepiece==0.2.0 protobuf==5\n            echo "Dependencies installed successfully"\n            echo "Starting inference-perf..."\n            inference-perf --config_file /workspace/config.yml\n        volumeMounts:\n          - name: config\n            mountPath: /workspace/config.yml\n            subPath: config.yml\n        resources:\n          requests:\n            cpu: "2"\n            memory: "4Gi"\n          limits:\n            cpu: "4"\n            memory: "8Gi"\n\n      volumes:\n        - name: config\n          configMap:\n            name: inference-perf-config\nEOF\n'})}),"\n",(0,a.jsx)(n.h2,{id:"execution",children:"Execution"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"kubectl apply -f inference-perf-fixed.yaml\n"})})]})}function d(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(u,{...e})}):u(e)}}}]);