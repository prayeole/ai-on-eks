"use strict";(self.webpackChunkdoeks_website=self.webpackChunkdoeks_website||[]).push([[9501],{28453:(e,n,r)=>{r.d(n,{R:()=>a,x:()=>c});var i=r(96540);const s={},t=i.createContext(s);function a(e){const n=i.useContext(t);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),i.createElement(t.Provider,{value:n},e.children)}},66001:(e,n,r)=>{r.r(n),r.d(n,{assets:()=>o,contentTitle:()=>c,default:()=>m,frontMatter:()=>a,metadata:()=>i,toc:()=>f});const i=JSON.parse('{"id":"guidance/benchmarking/benchmarking-with-inference-perf/inference-perf","title":"Benchmarking with Inference Perf","description":"To make benchmarking easier and more consistent, the Inference Perf tool provides a standardized way to measure and compare LLM inference performance across different systems.","source":"@site/docs/guidance/benchmarking/3-benchmarking-with-inference-perf/1-inference-perf.md","sourceDirName":"guidance/benchmarking/3-benchmarking-with-inference-perf","slug":"/guidance/benchmarking/benchmarking-with-inference-perf/inference-perf","permalink":"/ai-on-eks/docs/guidance/benchmarking/benchmarking-with-inference-perf/inference-perf","draft":false,"unlisted":false,"editUrl":"https://github.com/awslabs/ai-on-eks/blob/main/website/docs/guidance/benchmarking/3-benchmarking-with-inference-perf/1-inference-perf.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_label":"Benchmarking with Inference Perf"},"sidebar":"guidance","previous":{"title":"Key Metrics for Benchmarking LLMs","permalink":"/ai-on-eks/docs/guidance/benchmarking/key-metrics-for-benchmarking-llms/"},"next":{"title":"Running Inference Perf on EKS","permalink":"/ai-on-eks/docs/guidance/benchmarking/benchmarking-with-inference-perf/running-inference-perf-on-eks"}}');var s=r(74848),t=r(28453);const a={sidebar_label:"Benchmarking with Inference Perf"},c="Benchmarking with Inference Perf",o={},f=[];function d(e){const n={h1:"h1",header:"header",img:"img",p:"p",...(0,t.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"benchmarking-with-inference-perf",children:"Benchmarking with Inference Perf"})}),"\n",(0,s.jsx)(n.p,{children:"To make benchmarking easier and more consistent, the Inference Perf tool provides a standardized way to measure and compare LLM inference performance across different systems."}),"\n",(0,s.jsx)(n.p,{children:"Inference Perf (GitHub - kubernetes-sigs/inference-perf: GenAI inference performance b...) is an open-source, model-server-agnostic tool for benchmarking GenAI inference workloads. It enables apples-to-apples comparisons across GPUs, CPUs, and custom accelerators, making benchmarking self-hosted LLMs easier and more consistent. The tool supports real-world and synthetic datasets, multiple APIs and model servers (including vLLM, SGLang, and TGI), and large deployments with frameworks like llm-d, Dynamo, and Inference Gateway."}),"\n",(0,s.jsx)(n.p,{children:"Users can define input/output distributions (Gaussian, fixed-length, min-max) and simulate different load patterns, such as burst traffic, saturation, or autoscaling scenarios. Part of the wg-serving standardization effort, Inference Perf collects metrics like Time to First Token, Intertoken Latency, and Tokens per Second, helping teams compare performance, throughput, and cost efficiency across systems, moving from guesswork to data-driven decisions."}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.img,{src:r(81011).A+"",width:"1020",height:"527"})})]})}function m(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},81011:(e,n,r)=>{r.d(n,{A:()=>i});const i=r.p+"assets/images/architecture-e44aea344d20adf9e949f41ff32969fc.png"}}]);