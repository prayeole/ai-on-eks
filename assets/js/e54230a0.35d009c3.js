"use strict";(self.webpackChunkdoeks_website=self.webpackChunkdoeks_website||[]).push([[201],{28453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>a});var r=i(96540);const s={},t=r.createContext(s);function o(e){const n=r.useContext(t);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),r.createElement(t.Provider,{value:n},e.children)}},75051:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>o,metadata:()=>r,toc:()=>c});const r=JSON.parse('{"id":"guidance/benchmarking/benchmarking-with-inference-perf/running-inference-perf-on-eks","title":"Running Inference Perf on EKS","description":"Inference Perf is a GenAI inference performance benchmarking tool designed specifically for measuring LLM inference endpoint performance on Kubernetes. It runs as a Kubernetes Job and supports multiple model servers (vLLM, SGLang, TGI) with standardized metrics.","source":"@site/docs/guidance/benchmarking/3-benchmarking-with-inference-perf/2-running-inference-perf-on-eks.md","sourceDirName":"guidance/benchmarking/3-benchmarking-with-inference-perf","slug":"/guidance/benchmarking/benchmarking-with-inference-perf/running-inference-perf-on-eks","permalink":"/ai-on-eks/docs/guidance/benchmarking/benchmarking-with-inference-perf/running-inference-perf-on-eks","draft":false,"unlisted":false,"editUrl":"https://github.com/awslabs/ai-on-eks/blob/main/website/docs/guidance/benchmarking/3-benchmarking-with-inference-perf/2-running-inference-perf-on-eks.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_label":"Running Inference Perf on EKS"},"sidebar":"guidance","previous":{"title":"Benchmarking with Inference Perf","permalink":"/ai-on-eks/docs/guidance/benchmarking/benchmarking-with-inference-perf/inference-perf"},"next":{"title":"Complete Deployment Example","permalink":"/ai-on-eks/docs/guidance/benchmarking/benchmarking-with-inference-perf/complete-deployment-example-guide"}}');var s=i(74848),t=i(28453);const o={sidebar_label:"Running Inference Perf on EKS"},a="Running Inference Perf on EKS",l={},c=[{value:"Prerequisites",id:"prerequisites",level:2},{value:"Model-Specific Dependencies",id:"model-specific-dependencies",level:2},{value:"Understanding the Inference Benchmark Framework architecture",id:"understanding-the-inference-benchmark-framework-architecture",level:2},{value:"API Configuration",id:"api-configuration",level:3},{value:"Data Generation",id:"data-generation",level:3},{value:"Load Generation",id:"load-generation",level:3},{value:"Server Configuration",id:"server-configuration",level:3},{value:"Storage Configuration",id:"storage-configuration",level:3},{value:"Metrics Collection (Optional)",id:"metrics-collection-optional",level:3},{value:"Infrastructure Topology for Reproducible Results",id:"infrastructure-topology-for-reproducible-results",level:2},{value:"Why This Matters:",id:"why-this-matters",level:3},{value:"Example of the Problem:",id:"example-of-the-problem",level:3},{value:"Required Configuration:",id:"required-configuration",level:3},{value:"Verification:",id:"verification",level:3},{value:"Optional: Instance Type Consistency",id:"optional-instance-type-consistency",level:3},{value:"Troubleshooting:",id:"troubleshooting",level:3}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"running-inference-perf-on-eks",children:"Running Inference Perf on EKS"})}),"\n",(0,s.jsxs)(n.p,{children:["Inference Perf is a GenAI inference performance benchmarking tool designed specifically for measuring LLM inference endpoint performance on Kubernetes. It runs as a Kubernetes Job and supports multiple model servers (",(0,s.jsx)(n.a,{href:"https://github.com/vllm-project/vllm",children:"vLLM"}),", ",(0,s.jsx)(n.a,{href:"https://github.com/sgl-project/sglang",children:"SGLang"}),", ",(0,s.jsx)(n.a,{href:"https://github.com/huggingface/text-generation-inference",children:"TGI"}),") with standardized metrics."]}),"\n",(0,s.jsxs)(n.p,{children:["Why use a ",(0,s.jsx)(n.a,{href:"https://kubernetes.io/docs/concepts/workloads/controllers/job/",children:"Job"}),"? Jobs run once and terminate when complete, making them ideal for benchmarking tasks. Results are stored locally or in cloud storage (",(0,s.jsx)(n.a,{href:"https://aws.amazon.com/s3/",children:"S3"}),")."]}),"\n",(0,s.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Kubernetes cluster with kubectl access (version 1.21+)"}),"\n",(0,s.jsx)(n.li,{children:"A deployed inference endpoint with OpenAI-compatible API (vLLM, SGLang, TGI, or compatible)"}),"\n",(0,s.jsx)(n.li,{children:"Namespace for running benchmarks"}),"\n",(0,s.jsxs)(n.li,{children:["Container image: quay.io/inference-perf/inference-perf",":v0",".2.0"]}),"\n",(0,s.jsx)(n.li,{children:"(Optional) HuggingFace token for downloading tokenizers"}),"\n",(0,s.jsx)(n.li,{children:"(Optional) AWS credentials for S3 storage"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"model-specific-dependencies",children:"Model-Specific Dependencies"}),"\n",(0,s.jsx)(n.p,{children:"\u26a0\ufe0f IMPORTANT: Different models require different tokenizer packages:"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Model Family"}),(0,s.jsx)(n.th,{children:"Requires sentencepiece?"}),(0,s.jsx)(n.th,{children:"Examples"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Mistral (all versions)"}),(0,s.jsx)(n.td,{children:"\u2705 YES"}),(0,s.jsx)(n.td,{children:"mistralai/Mistral-7B-Instruct-v0.3"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Llama 2"}),(0,s.jsx)(n.td,{children:"\u2705 YES"}),(0,s.jsx)(n.td,{children:"meta-llama/Llama-2-7b-hf"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"Llama 3.1"}),(0,s.jsx)(n.td,{children:"\u2705 YES"}),(0,s.jsx)(n.td,{children:"meta-llama/Meta-Llama-3.1-8B"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"SmolLM2"}),(0,s.jsx)(n.td,{children:"\u274c NO"}),(0,s.jsx)(n.td,{children:"HuggingFaceTB/SmolLM2-135M-Instruct"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"GPT models"}),(0,s.jsx)(n.td,{children:"\u274c NO"}),(0,s.jsx)(n.td,{children:"Various GPT variants"})]})]})]}),"\n",(0,s.jsx)(n.p,{children:'If using Mistral or Llama models, you must install the sentencepiece package. See "Handling Model Dependencies" section below for implementation.'}),"\n",(0,s.jsx)(n.h2,{id:"understanding-the-inference-benchmark-framework-architecture",children:"Understanding the Inference Benchmark Framework architecture"}),"\n",(0,s.jsx)(n.p,{children:"Before deploying, it's important to understand the key configuration components that define your benchmark test."}),"\n",(0,s.jsx)(n.h3,{id:"api-configuration",children:"API Configuration"}),"\n",(0,s.jsx)(n.p,{children:"Defines how the tool communicates with your inference endpoint. You'll specify whether you're using completion or chat API, and whether streaming is enabled (required for measuring TTFT and ITL metrics)."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:"api:\n  type: completion # completion or chat\n  streaming: true # Enable for TTFT/ITL metrics\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Determining the Correct API Type:"})}),"\n",(0,s.jsx)(n.p,{children:"The inference-charts deployment automatically configures API endpoints based on the model's capabilities. To identify which endpoints are available:"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Method 1: Check vLLM Deployment Logs (Recommended)"})}),"\n",(0,s.jsx)(n.p,{children:"Check your vLLM server logs to see which API endpoints were enabled at startup:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'# View vLLM startup logs showing enabled endpoints\nkubectl logs -n default -l app.kubernetes.io/name=inference-charts --tail=100 | grep -i "route\\|endpoint\\|application"\n'})}),"\n",(0,s.jsx)(n.p,{children:"Look for output indicating enabled routes:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"Route: /v1/completions"})," \u2192 Use ",(0,s.jsx)(n.code,{children:"type: completion"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"Route: /v1/chat/completions"})," \u2192 Use ",(0,s.jsx)(n.code,{children:"type: chat"})]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["If both routes appear, use ",(0,s.jsx)(n.code,{children:"completion"})," (simpler for benchmarking)."]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Method 2: Check Model Capabilities (Optional)"})}),"\n",(0,s.jsx)(n.p,{children:"For understanding the model's theoretical capabilities, review the Hugging Face model card for your model. Models with a defined chat template typically support the chat completion API, but the actual deployment configuration determines what's enabled."}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Note:"})," The OpenAI completion API (",(0,s.jsx)(n.code,{children:"v1/completions"}),") is deprecated by OpenAI but remains widely supported by vLLM, SGLang, and TGI. Most inference-charts deployments enable it by default without additional configuration."]}),"\n",(0,s.jsx)(n.h3,{id:"data-generation",children:"Data Generation"}),"\n",(0,s.jsx)(n.p,{children:"Controls what data is sent to your inference endpoint. You can use real datasets (ShareGPT) or synthetic data with controlled distributions. Synthetic data is useful when you need specific input/output length patterns for testing."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:"data:\n\n  type: synthetic # shareGPT, synthetic, random, shared_prefix, etc.\n\n  input_distribution:\n    mean: 512      # Average input prompt length in tokens\n    std_dev: 128   # Variation in prompt length (68% within \xb1128 tokens of mean)\n    min: 128       # Minimum input tokens (clips distribution lower bound)\n    max: 2048      # Maximum input tokens (clips distribution upper bound)\n\n  output_distribution:\n    mean: 256      # Average generated response length in tokens\n    std_dev: 64    # Variation in response length (68% within \xb164 tokens of mean)\n    min: 32        # Minimum output tokens (clips distribution lower bound)\n    max: 512       # Maximum output tokens (clips distribution upper bound)\n"})}),"\n",(0,s.jsx)(n.h3,{id:"load-generation",children:"Load Generation"}),"\n",(0,s.jsx)(n.p,{children:"Defines your load pattern - how many requests per second and for how long. You can use multiple stages to test different load levels, or use sweep mode for automatic saturation detection."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:"load:\n  type: constant              # Use 'constant' for uniform arrival (predictable load) or 'poisson' for bursty traffic (realistic production)\n  stages:\n    - rate: 10                # Requests per second (QPS) - increase to test higher throughput, decrease for baseline/minimal load\n      duration: 300           # How long to sustain this rate in seconds - longer durations (300-600s) ensure stable measurements\n  num_workers: 4              # Concurrent workers generating load - increase if inference-perf can't achieve target rate (check scheduling delay in results)\n"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Note on num_workers:"})," This controls the benchmark tool's internal parallelism, not concurrent users. The default value of 4 works for most scenarios. Only increase if results show high ",(0,s.jsx)(n.code,{children:"schedule_delay"})," (> 10ms), indicating the tool cannot maintain the target rate."]}),"\n",(0,s.jsx)(n.h3,{id:"server-configuration",children:"Server Configuration"}),"\n",(0,s.jsx)(n.p,{children:"Specifies your inference endpoint details - server type, model name, and URL."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:"server:\n\n  type: vllm # vllm, sglang, or tgi\n\n  model_name: qwen3-8b\n\n  base_url: http://qwen3-vllm.default:8000\n\n  ignore_eos: true\n"})}),"\n",(0,s.jsx)(n.h3,{id:"storage-configuration",children:"Storage Configuration"}),"\n",(0,s.jsx)(n.p,{children:"Determines where benchmark results are saved. Local storage saves to the pod filesystem (requires manual copy), while S3 storage automatically persists results to your AWS bucket."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'storage:\n\n  local_storage: # Default: saves in pod\n\n    path: "reports-results"\n\n  # \u26a0\ufe0f Warning: local_storage results are lost when pod terminates\n  # To retrieve results, add \'&& sleep infinity\' to the Job args and use:\n  # kubectl cp <pod-name>:/workspace/reports-* ./local-results -n benchmarking\n\n  # OR\n\n  simple_storage_service: # S3: automatic persistence\n\n    bucket_name: "my-results-bucket"\n\n    path: "inference-perf/results"\n'})}),"\n",(0,s.jsx)(n.h3,{id:"metrics-collection-optional",children:"Metrics Collection (Optional)"}),"\n",(0,s.jsx)(n.p,{children:"Enables advanced metrics collection from Prometheus if your inference server exposes metrics."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:"metrics:\n\n  type: prometheus\n\n  prometheus:\n\n    url: http://kube-prometheus-stack-prometheus.monitoring:9090 # For ai-on-eks Path A; adjust service name/namespace for custom Prometheus\n\n    scrape_interval: 15\n"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Note:"})," The Prometheus URL uses Kubernetes DNS format: ",(0,s.jsx)(n.code,{children:"http://<service-name>.<namespace>:<port>"}),". If your Prometheus is deployed in a different namespace (e.g., ",(0,s.jsx)(n.code,{children:"monitoring"}),", ",(0,s.jsx)(n.code,{children:"observability"}),"), update the URL accordingly. The benchmark Job runs in the ",(0,s.jsx)(n.code,{children:"benchmarking"})," namespace, so cross-namespace service access must be specified."]}),"\n",(0,s.jsx)(n.h2,{id:"infrastructure-topology-for-reproducible-results",children:"Infrastructure Topology for Reproducible Results"}),"\n",(0,s.jsxs)(n.p,{children:["For accurate, comparable benchmarks across multiple runs, the inference-perf Job ",(0,s.jsx)(n.strong,{children:"MUST"})," be placed in the same AZ as your inference deployment."]}),"\n",(0,s.jsx)(n.h3,{id:"why-this-matters",children:"Why This Matters:"}),"\n",(0,s.jsx)(n.p,{children:"Without proper placement, benchmark results become unreliable:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Cross-AZ network latency adds 1-2ms per request"}),"\n",(0,s.jsx)(n.li,{children:"Results vary unpredictably across benchmark runs"}),"\n",(0,s.jsx)(n.li,{children:"You cannot determine if performance changes are real or due to infrastructure placement"}),"\n",(0,s.jsx)(n.li,{children:"Optimization decisions become impossible"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"example-of-the-problem",children:"Example of the Problem:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{children:"First benchmark run:\n- Benchmark pod in us-west-2a \u2192 Inference pod in us-west-2a\n- Result: TTFT = 800ms\n\nSecond benchmark run (after pod restart):\n- Benchmark pod in us-west-2b \u2192 Inference pod in us-west-2a\n- Result: TTFT = 850ms\n"})}),"\n",(0,s.jsx)(n.p,{children:"The 50ms difference is cross-AZ latency, not actual performance change."}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Note on Cross-AZ Testing:"})," While same-AZ placement is recommended for baseline benchmarking and performance optimization, cross-AZ testing is valuable for validating high-availability (HA) deployments where your inference service spans multiple availability zones. If your production deployment uses multi-AZ load balancing for fault tolerance, conduct separate benchmarks with cross-AZ placement to understand the latency impact users may experience during zone-specific routing."]}),"\n",(0,s.jsx)(n.h3,{id:"required-configuration",children:"Required Configuration:"}),"\n",(0,s.jsxs)(n.p,{children:["All benchmark Job examples in this guide include ",(0,s.jsx)(n.code,{children:"affinity"})," configuration to enforce same-AZ placement using the standard Kubernetes topology label ",(0,s.jsx)(n.code,{children:"topology.kubernetes.io/zone"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:"spec:\n  template:\n    spec:\n      affinity:\n        podAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                app.kubernetes.io/component: qwen3-vllm\n            topologyKey: topology.kubernetes.io/zone\n"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"IMPORTANT:"})," The ",(0,s.jsx)(n.code,{children:"matchLabels"})," must match your actual vLLM deployment labels. Check your deployment's pod labels with:"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"kubectl get deployment qwen3-vllm -n default -o jsonpath='{.spec.template.metadata.labels}' && echo\n"})}),"\n",(0,s.jsx)(n.p,{children:"Common label patterns:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:["Standard deployments: ",(0,s.jsx)(n.code,{children:"app: <service-name>"})," (simple pattern)"]}),"\n",(0,s.jsxs)(n.li,{children:["inference-charts deployments: ",(0,s.jsx)(n.code,{children:"app.kubernetes.io/component: <service-name>"})," (used in this guide's examples)"]}),"\n",(0,s.jsxs)(n.li,{children:["Other Helm charts: ",(0,s.jsx)(n.code,{children:"app.kubernetes.io/name: <service-name>"})]}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:["Update the ",(0,s.jsx)(n.code,{children:"matchLabels"})," section in the examples to match your deployment's actual pod labels."]}),"\n",(0,s.jsx)(n.h3,{id:"verification",children:"Verification:"}),"\n",(0,s.jsx)(n.p,{children:"After deploying, confirm both pods are in the same AZ:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Check both pods - they should show the same zone\nkubectl get pods -n default -o wide -l app.kubernetes.io/component=qwen3-vllm\nkubectl get pods -n benchmarking -o wide -l app=inference-perf\n\n# Expected output - both in same zone:\n# qwen3-vllm-xxx      ip-10-0-1-100.us-west-2a...\n# inference-perf-yyy  ip-10-0-1-200.us-west-2a...\n"})}),"\n",(0,s.jsx)(n.h3,{id:"optional-instance-type-consistency",children:"Optional: Instance Type Consistency"}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"Instance Sizing for Benchmark Pods"})}),"\n",(0,s.jsx)(n.p,{children:"The benchmark pod runs on a separate CPU node from your GPU-based inference deployment. The m6i.2xlarge instance type (8 vCPU, 32 GB RAM) provides sufficient capacity for load generation without competing for GPU node resources."}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Important:"})," The pod affinity configuration (",(0,s.jsx)(n.code,{children:"topology.kubernetes.io/zone"}),") ensures both pods are in the same availability zone for network consistency, NOT on the same physical node. Your cluster must have capacity for both:"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"GPU nodes for inference (e.g., g5.2xlarge for models like Qwen3-8B)"}),"\n",(0,s.jsx)(n.li,{children:"CPU nodes for benchmarking (e.g., m6i.2xlarge)"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"If using Karpenter, it will automatically provision the appropriate node types in the same AZ."}),"\n",(0,s.jsx)(n.p,{children:"For maximum reproducibility (baseline benchmarks, CI/CD pipelines), you can specify the instance type:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:"spec:\n  template:\n    spec:\n      nodeSelector:\n        node.kubernetes.io/instance-type: m6i.2xlarge\n      affinity:\n        podAffinity:\n          # ... same as above\n"})}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"When to use instance type selectors:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Creating benchmark baselines for documentation"}),"\n",(0,s.jsx)(n.li,{children:"CI/CD pipelines requiring consistent results"}),"\n",(0,s.jsx)(n.li,{children:"Preventing Karpenter from provisioning different instance families"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:(0,s.jsx)(n.strong,{children:"When NOT needed:"})}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Homogeneous CPU node pools"}),"\n",(0,s.jsx)(n.li,{children:"Comparative testing (before/after on same infrastructure)"}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"troubleshooting",children:"Troubleshooting:"}),"\n",(0,s.jsxs)(n.p,{children:["If your benchmark Job stays in ",(0,s.jsx)(n.code,{children:"Pending"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"kubectl describe pod -n benchmarking <pod-name>\n"})}),"\n",(0,s.jsx)(n.p,{children:"Common issues:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"No capacity in target AZ"}),": Scale cluster or use ",(0,s.jsx)(n.code,{children:"preferredDuringSchedulingIgnoredDuringExecution"})]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Label mismatch"}),": Verify deployment labels match podAffinity selector"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}}}]);