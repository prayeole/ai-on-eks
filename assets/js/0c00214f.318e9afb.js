"use strict";(self.webpackChunkdoeks_website=self.webpackChunkdoeks_website||[]).push([[4777],{28453:(e,n,s)=>{s.d(n,{R:()=>i,x:()=>o});var a=s(96540);const t={},r=a.createContext(t);function i(e){const n=a.useContext(r);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:i(e.components),a.createElement(r.Provider,{value:n},e.children)}},79458:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>m,frontMatter:()=>i,metadata:()=>a,toc:()=>l});const a=JSON.parse('{"id":"guidance/benchmarking/test-scenarios/baseline-performance","title":"SCENARIO 1: Baseline Performance","description":"When to use this scenario:","source":"@site/docs/guidance/benchmarking/4-test-scenarios/1-baseline-performance.md","sourceDirName":"guidance/benchmarking/4-test-scenarios","slug":"/guidance/benchmarking/test-scenarios/baseline-performance","permalink":"/ai-on-eks/docs/guidance/benchmarking/test-scenarios/baseline-performance","draft":false,"unlisted":false,"editUrl":"https://github.com/awslabs/ai-on-eks/blob/main/website/docs/guidance/benchmarking/4-test-scenarios/1-baseline-performance.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_label":"Scenario 1 - Baseline Performance"},"sidebar":"guidance","previous":{"title":"Choosing Between Synthetic and Real Data","permalink":"/ai-on-eks/docs/guidance/benchmarking/test-scenarios/choosing-synthetic-vs-real"},"next":{"title":"Scenario 2 - Saturation Testing","permalink":"/ai-on-eks/docs/guidance/benchmarking/test-scenarios/saturation-testing"}}');var t=s(74848),r=s(28453);const i={sidebar_label:"Scenario 1 - Baseline Performance"},o="SCENARIO 1: Baseline Performance",c={},l=[{value:"When to use this scenario:",id:"when-to-use-this-scenario",level:2},{value:"Deployment",id:"deployment",level:2},{value:"Using Helm Chart (Recommended)",id:"using-helm-chart-recommended",level:3},{value:"Customizing Configuration",id:"customizing-configuration",level:3},{value:"Key Configuration:",id:"key-configuration",level:2},{value:"Understanding the results:",id:"understanding-the-results",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components},{Details:s}=n;return s||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"scenario-1-baseline-performance",children:"SCENARIO 1: Baseline Performance"})}),"\n",(0,t.jsx)(n.h2,{id:"when-to-use-this-scenario",children:"When to use this scenario:"}),"\n",(0,t.jsx)(n.p,{children:"Use baseline testing when establishing your system's optimal performance with zero contention, essentially taking your infrastructure's vital signs. This is your starting point before any capacity planning or optimization work, ideal when you've just deployed a new endpoint or made infrastructure changes. It answers \"what's the best performance this system can deliver?\" without queueing or resource competition, giving you a clean reference point for all future testing."}),"\n",(0,t.jsx)(n.h2,{id:"deployment",children:"Deployment"}),"\n",(0,t.jsx)(n.h3,{id:"using-helm-chart-recommended",children:"Using Helm Chart (Recommended)"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Add the AI on EKS Helm repository\nhelm repo add ai-on-eks https://awslabs.github.io/ai-on-eks-charts/\nhelm repo update\n\n# Install baseline scenario\nhelm install baseline-test ai-on-eks/benchmark-charts \\\n  --set benchmark.scenario=baseline \\\n  --set benchmark.target.baseUrl=http://qwen3-vllm.default:8000 \\\n  --set benchmark.target.modelName=qwen3-8b \\\n  --set benchmark.target.tokenizerPath=Qwen/Qwen3-8B \\\n  --namespace benchmarking --create-namespace\n\n# Monitor progress\nkubectl logs -n benchmarking -l benchmark.scenario=baseline -f\n"})}),"\n",(0,t.jsx)(n.h3,{id:"customizing-configuration",children:"Customizing Configuration"}),"\n",(0,t.jsxs)(n.p,{children:["Override specific values using ",(0,t.jsx)(n.code,{children:"--set"})," or a custom values file:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Adjust test duration or resources\nhelm install baseline-test ai-on-eks/benchmark-charts \\\n  --set benchmark.scenario=baseline \\\n  --set benchmark.target.baseUrl=http://your-model.your-namespace:8000 \\\n  --set benchmark.scenarios.baseline.load.stages[0].duration=600 \\\n  --set benchmark.resources.main.requests.cpu=4 \\\n  --namespace benchmarking\n"})}),"\n",(0,t.jsxs)(n.p,{children:["Or create a custom ",(0,t.jsx)(n.code,{children:"my-values.yaml"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:"benchmark:\n  scenario: baseline\n  target:\n    baseUrl: http://your-model.your-namespace:8000\n    modelName: your-model-name\n  scenarios:\n    baseline:\n      load:\n        stages:\n          - rate: 1\n            duration: 600  # Longer test\n"})}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"helm install baseline-test ai-on-eks/benchmark-charts -f my-values.yaml -n benchmarking\n"})}),"\n",(0,t.jsx)(n.h2,{id:"key-configuration",children:"Key Configuration:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Fixed-length synthetic data (512 input / 128 output tokens)"}),"\n",(0,t.jsx)(n.li,{children:"Constant load at 1 QPS for 300 seconds"}),"\n",(0,t.jsx)(n.li,{children:"Streaming enabled"}),"\n",(0,t.jsx)(n.li,{children:"Pod affinity ensures same-AZ placement with inference pods"}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"understanding-the-results",children:"Understanding the results:"}),"\n",(0,t.jsxs)(n.p,{children:["Your TTFT and ITL at 1 QPS represent the theoretical minimum latency, the absolute fastest your system can respond with zero queueing or contention. If baseline TTFT is 800ms, users will never see faster response times regardless of optimizations, like adding replicas, load balancers, or autoscaling, because these improve ",(0,t.jsx)(n.strong,{children:"throughput and concurrency"}),", not single-request speed. Focus on these metrics as your performance floor: schedule delay should be near zero (",(0,t.jsx)(n.code,{children:"<10ms"}),"), and any deviation indicates the test runner itself needs more resources. Compare baseline numbers against your Service Level Agreement (SLA) targets; if baseline performance doesn't meet requirements, you need model/hardware optimization before worrying about scale, as adding capacity won't improve fundamental inference speed."]}),"\n",(0,t.jsxs)(s,{children:[(0,t.jsxs)("summary",{children:[(0,t.jsx)("strong",{children:"Alternative: Raw Kubernetes YAML"})," (for educational purposes or custom deployments)"]}),(0,t.jsx)(n.p,{children:"If you prefer not to use Helm or need to customize beyond values, here's the complete Kubernetes manifest:"}),(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-yaml",children:'---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: inference-perf-baseline\n  namespace: benchmarking\ndata:\n  config.yml: |\n    api:\n      type: completion\n      streaming: true\n\n    data:\n      type: synthetic\n      input_distribution:\n        mean: 512\n        std_dev: 0\n        min: 512\n        max: 512\n      output_distribution:\n        mean: 128\n        std_dev: 0\n        min: 128\n        max: 128\n\n    load:\n      type: constant\n      stages:\n        - rate: 1\n          duration: 300\n      num_workers: 4\n\n    server:\n      type: vllm\n      model_name: qwen3-8b\n      base_url: http://qwen3-vllm.default:8000\n      ignore_eos: true\n\n    tokenizer:\n      pretrained_model_name_or_path: Qwen/Qwen3-8B\n\n    storage:\n      simple_storage_service:\n        bucket_name: "inference-perf-results"\n        path: "baseline-test/results"\n---\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: inference-perf-baseline\n  namespace: benchmarking\nspec:\n  backoffLimit: 2\n  ttlSecondsAfterFinished: 3600\n  template:\n    spec:\n      restartPolicy: Never\n      serviceAccountName: inference-perf-sa\n\n      affinity:\n        podAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                app.kubernetes.io/component: qwen3-vllm\n            topologyKey: topology.kubernetes.io/zone\n\n      containers:\n      - name: inference-perf\n        image: quay.io/inference-perf/inference-perf:v0.2.0\n        command: ["/bin/sh", "-c"]\n        args:\n        - |\n          inference-perf --config_file /workspace/config.yml\n        volumeMounts:\n        - name: config\n          mountPath: /workspace/config.yml\n          subPath: config.yml\n        resources:\n          requests:\n            cpu: "2"\n            memory: "4Gi"\n          limits:\n            cpu: "4"\n            memory: "8Gi"\n\n      volumes:\n      - name: config\n        configMap:\n          name: inference-perf-baseline\n'})}),(0,t.jsxs)(n.p,{children:["Apply with: ",(0,t.jsx)(n.code,{children:"kubectl apply -f 01-scenario-baseline.yaml"})]})]})]})}function m(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}}}]);