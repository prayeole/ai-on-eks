"use strict";(self.webpackChunkdoeks_website=self.webpackChunkdoeks_website||[]).push([[1896],{20285:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>u,frontMatter:()=>i,metadata:()=>a,toc:()=>l});const a=JSON.parse('{"id":"guidance/benchmarking/test-scenarios/real-dataset-testing","title":"SCENARIO 5: Real Dataset Testing","description":"When to use this scenario:","source":"@site/docs/guidance/benchmarking/4-test-scenarios/5-real-dataset-testing.md","sourceDirName":"guidance/benchmarking/4-test-scenarios","slug":"/guidance/benchmarking/test-scenarios/real-dataset-testing","permalink":"/ai-on-eks/docs/guidance/benchmarking/test-scenarios/real-dataset-testing","draft":false,"unlisted":false,"editUrl":"https://github.com/awslabs/ai-on-eks/blob/main/website/docs/guidance/benchmarking/4-test-scenarios/5-real-dataset-testing.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"sidebar_label":"Scenario 5 - Real Dataset Testing"},"sidebar":"guidance","previous":{"title":"Scenario 4 - Production Simulation","permalink":"/ai-on-eks/docs/guidance/benchmarking/test-scenarios/production-simulation"},"next":{"title":"Resources","permalink":"/ai-on-eks/docs/guidance/benchmarking/resources/"}}');var s=t(74848),r=t(28453);const i={sidebar_label:"Scenario 5 - Real Dataset Testing"},o="SCENARIO 5: Real Dataset Testing",c={},l=[{value:"When to use this scenario:",id:"when-to-use-this-scenario",level:2},{value:"Deployment",id:"deployment",level:2},{value:"Using Helm Chart (Recommended)",id:"using-helm-chart-recommended",level:3},{value:"Using Custom Dataset",id:"using-custom-dataset",level:3},{value:"Key Configuration:",id:"key-configuration",level:2},{value:"Understanding the results:",id:"understanding-the-results",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components},{Details:t}=n;return t||function(e,n){throw new Error("Expected "+(n?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"scenario-5-real-dataset-testing",children:"SCENARIO 5: Real Dataset Testing"})}),"\n",(0,s.jsx)(n.h2,{id:"when-to-use-this-scenario",children:"When to use this scenario:"}),"\n",(0,s.jsx)(n.p,{children:'Use real dataset testing to validate production-ready performance with actual user prompts and query patterns. This is essential when your model is fine-tuned for specific conversation patterns, when comparing model versions with real-world performance guarantees, or when you need to tell stakeholders "this is how it performs on actual conversations, not theoretical data." The tradeoff is less control over distributions, but you gain authenticity and the ability to discover edge cases that synthetic data misses.'}),"\n",(0,s.jsx)(n.h2,{id:"deployment",children:"Deployment"}),"\n",(0,s.jsx)(n.h3,{id:"using-helm-chart-recommended",children:"Using Helm Chart (Recommended)"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Add the AI on EKS Helm repository\nhelm repo add ai-on-eks https://awslabs.github.io/ai-on-eks-charts/\nhelm repo update\n\n# ShareGPT dataset testing uses same scenarios but with real data\nhelm install sharegpt-test ai-on-eks/benchmark-charts \\\n  --set benchmark.scenario=baseline \\\n  --set benchmark.target.baseUrl=http://qwen3-vllm.default:8000 \\\n  --set benchmark.target.modelName=qwen3-8b \\\n  --set benchmark.target.tokenizerPath=Qwen/Qwen3-8B \\\n  --namespace benchmarking --create-namespace\n\n# Monitor for natural conversation complexity patterns\nkubectl logs -n benchmarking -l app.kubernetes.io/component=benchmark -f\n"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Note:"})," Real dataset testing uses the same load patterns as scenarios 1-4, but with ",(0,s.jsx)(n.code,{children:"data.type: shareGPT"})," instead of ",(0,s.jsx)(n.code,{children:"data.type: synthetic"}),". You can apply real data to any scenario (baseline, saturation, sweep, or production)."]}),"\n",(0,s.jsx)(n.h3,{id:"using-custom-dataset",children:"Using Custom Dataset"}),"\n",(0,s.jsx)(n.p,{children:"Provide your own conversation dataset:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:"# custom-dataset.yaml\nbenchmark:\n  scenario: saturation  # Or any scenario\n  target:\n    baseUrl: http://your-model.your-namespace:8000\n  # Override data configuration to use custom dataset\n  customData:\n    enabled: true\n    type: custom\n    path: /path/to/your/conversations.json\n    format: sharegpt  # or openai, alpaca, etc.\n"})}),"\n",(0,s.jsx)(n.p,{children:"For custom datasets, you'll need to mount the data file into the benchmark pod using a ConfigMap or PersistentVolume."}),"\n",(0,s.jsx)(n.h2,{id:"key-configuration",children:"Key Configuration:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"ShareGPT real conversation dataset (varies by scenario choice)"}),"\n",(0,s.jsx)(n.li,{children:"Any load pattern (constant/poisson, depends on selected scenario)"}),"\n",(0,s.jsx)(n.li,{children:"Streaming enabled"}),"\n",(0,s.jsx)(n.li,{children:"Natural conversation complexity and length variance"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"understanding-the-results",children:"Understanding the results:"}),"\n",(0,s.jsx)(n.p,{children:'Real conversations reveal natural complexity patterns and edge cases absent from synthetic data\u2014look for latency outliers that expose problematic conversation structures or phrasings causing slow processing. Compare real data performance against synthetic tests at similar QPS; significant degradation suggests real conversations are more complex than your synthetic parameters assumed, helping calibrate future synthetic tests. TTFT variability will be higher due to natural context length variance from multi-turn dialogues, and any consistent error patterns with specific conversation types reveal production vulnerabilities worth targeted optimization. Use these results as ground truth for stakeholder commitments\u2014base your "P99 latency will be X" promises on real data, not synthetic.'}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"\u26a0\ufe0f Critical:"})," Regularly update your test dataset with recent anonymized production samples to prevent drift. If your benchmark dataset is 6 months old but user behavior has shifted to longer prompts, your performance predictions will be inaccurate."]}),"\n",(0,s.jsxs)(t,{children:[(0,s.jsx)("summary",{children:(0,s.jsx)("strong",{children:"Alternative: Raw Kubernetes YAML"})}),(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-yaml",children:'---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: inference-perf-sharegpt\n  namespace: benchmarking\ndata:\n  config.yml: |\n    api:\n      type: completion\n      streaming: true\n\n    data:\n      type: shareGPT  # Real conversation data\n\n    load:\n      type: constant\n      stages:\n        - rate: 10\n          duration: 300\n      num_workers: 4\n\n    server:\n      type: vllm\n      model_name: qwen3-8b\n      base_url: http://qwen3-vllm.default:8000\n      ignore_eos: true\n\n    tokenizer:\n      pretrained_model_name_or_path: Qwen/Qwen3-8B\n\n    storage:\n      simple_storage_service:\n        bucket_name: "inference-perf-results"\n        path: "sharegpt-test/results"\n---\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: inference-perf-sharegpt\n  namespace: benchmarking\nspec:\n  backoffLimit: 2\n  ttlSecondsAfterFinished: 3600\n  template:\n    spec:\n      restartPolicy: Never\n      serviceAccountName: inference-perf-sa\n\n      affinity:\n        podAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                app.kubernetes.io/component: qwen3-vllm\n            topologyKey: topology.kubernetes.io/zone\n\n      containers:\n      - name: inference-perf\n        image: quay.io/inference-perf/inference-perf:v0.2.0\n        command: ["/bin/sh", "-c"]\n        args:\n        - |\n          inference-perf --config_file /workspace/config.yml\n        volumeMounts:\n        - name: config\n          mountPath: /workspace/config.yml\n          subPath: config.yml\n        resources:\n          requests:\n            cpu: "2"\n            memory: "4Gi"\n          limits:\n            cpu: "4"\n            memory: "8Gi"\n\n      volumes:\n      - name: config\n        configMap:\n          name: inference-perf-sharegpt\n'})})]})]})}function u(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},28453:(e,n,t)=>{t.d(n,{R:()=>i,x:()=>o});var a=t(96540);const s={},r=a.createContext(s);function i(e){const n=a.useContext(r);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:i(e.components),a.createElement(r.Provider,{value:n},e.children)}}}]);