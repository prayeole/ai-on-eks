"use strict";(self.webpackChunkdoeks_website=self.webpackChunkdoeks_website||[]).push([[5638],{28453:(e,n,s)=>{s.d(n,{R:()=>l,x:()=>a});var r=s(96540);const i={},t=r.createContext(i);function l(e){const n=r.useContext(t);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:l(e.components),r.createElement(t.Provider,{value:n},e.children)}},42450:(e,n,s)=>{s.d(n,{A:()=>u});var r=s(96540),i=s(5556),t=s.n(i),l=s(34164);const a="collapsibleContent_q3kw",o="header_QCEw",c="icon_PckA",d="content_qLC1",h="expanded_iGsi";var p=s(74848);function x({children:e,header:n}){const[s,i]=(0,r.useState)(!1);return(0,p.jsxs)("div",{className:a,children:[(0,p.jsxs)("div",{className:(0,l.A)(o,{[h]:s}),onClick:()=>{i(!s)},children:[n,(0,p.jsx)("span",{className:(0,l.A)(c,{[h]:s}),children:s?"\ud83d\udc47":"\ud83d\udc48"})]}),s&&(0,p.jsx)("div",{className:d,children:e})]})}x.propTypes={children:t().node.isRequired,header:t().node.isRequired};const u=x},55036:(e,n,s)=>{s.d(n,{A:()=>r});const r=s.p+"assets/images/nvidia-deep-research-arch-bcb1892391f776b7923c3f6d22def766.png"},56849:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>p,frontMatter:()=>a,metadata:()=>r,toc:()=>d});const r=JSON.parse('{"id":"blueprints/inference/GPUs/nvidia-deep-research","title":"NVIDIA Enterprise RAG and AI-Q Research Assistant on EKS","description":"Deployment of Enterprise RAG and AI-Q on EKS requires access to GPU instances (g5, p4, or p5 families). This blueprint relies on Karpenter autoscaling for dynamic GPU provisioning.","source":"@site/docs/blueprints/inference/GPUs/nvidia-deep-research.md","sourceDirName":"blueprints/inference/GPUs","slug":"/blueprints/inference/GPUs/nvidia-deep-research","permalink":"/ai-on-eks/docs/blueprints/inference/GPUs/nvidia-deep-research","draft":false,"unlisted":false,"editUrl":"https://github.com/awslabs/ai-on-eks/blob/main/website/docs/blueprints/inference/GPUs/nvidia-deep-research.md","tags":[],"version":"current","sidebarPosition":9,"frontMatter":{"title":"NVIDIA Enterprise RAG and AI-Q Research Assistant on EKS","sidebar_position":9},"sidebar":"blueprints","previous":{"title":"NVIDIA Dynamo on Amazon EKS","permalink":"/ai-on-eks/docs/blueprints/inference/GPUs/nvidia-dynamo"},"next":{"title":"AIBrix on EKS","permalink":"/ai-on-eks/docs/blueprints/inference/GPUs/aibrix-deepseek-distill"}}');var i=s(74848),t=s(28453),l=s(42450);const a={title:"NVIDIA Enterprise RAG and AI-Q Research Assistant on EKS",sidebar_position:9},o="NVIDIA Enterprise RAG & AI-Q Research Assistant on Amazon EKS",c={},d=[{value:"What is NVIDIA AI-Q Research Assistant?",id:"what-is-nvidia-ai-q-research-assistant",level:2},{value:"Key Capabilities",id:"key-capabilities",level:3},{value:"AI-Q Components",id:"ai-q-components",level:3},{value:"What is NVIDIA Enterprise RAG Blueprint?",id:"what-is-nvidia-enterprise-rag-blueprint",level:2},{value:"Key Features",id:"key-features",level:3},{value:"Enterprise RAG Use Cases",id:"enterprise-rag-use-cases",level:3},{value:"Overview",id:"overview",level:2},{value:"Deployment Options",id:"deployment-options",level:3},{value:"Deployment Approach",id:"deployment-approach",level:3},{value:"Key Features",id:"key-features-1",level:3},{value:"Architecture",id:"architecture",level:2},{value:"AI-Q Research Assistant Architecture",id:"ai-q-research-assistant-architecture",level:3},{value:"Enterprise RAG Blueprint Architecture",id:"enterprise-rag-blueprint-architecture",level:3},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Required API Tokens",id:"required-api-tokens",level:3},{value:"GPU Instance Access",id:"gpu-instance-access",level:3},{value:"Getting Started",id:"getting-started",level:2},{value:"Deployment",id:"deployment",level:2},{value:"Step 1: Deploy Infrastructure",id:"step-1-deploy-infrastructure",level:3},{value:"Step 2: Setup Environment",id:"step-2-setup-environment",level:3},{value:"Step 3: Build OpenSearch Images",id:"step-3-build-opensearch-images",level:3},{value:"Step 4: Deploy Applications",id:"step-4-deploy-applications",level:3},{value:"1) Deploy Enterprise RAG Only",id:"1-deploy-enterprise-rag-only",level:4},{value:"2) Deploy AI-Q Research Assistant",id:"2-deploy-ai-q-research-assistant",level:4},{value:"Option A: Deploy All at Once (Recommended - Faster)",id:"option-a-deploy-all-at-once-recommended---faster",level:5},{value:"Option B: Deploy Sequentially",id:"option-b-deploy-sequentially",level:5},{value:"Step 1: Deploy Infrastructure",id:"step-1-deploy-infrastructure-1",level:3},{value:"Step 2: Setup Environment",id:"step-2-setup-environment-1",level:3},{value:"Step 3: Configure Karpenter NodePool Limits",id:"step-3-configure-karpenter-nodepool-limits",level:3},{value:"Step 4: Integrate OpenSearch and Build Docker Images",id:"step-4-integrate-opensearch-and-build-docker-images",level:3},{value:"Step 5: Deploy Enterprise RAG Blueprint",id:"step-5-deploy-enterprise-rag-blueprint",level:3},{value:"Step 6: Deploy AI-Q Components",id:"step-6-deploy-ai-q-components",level:3},{value:"Access Services",id:"access-services",level:2},{value:"Using the Applications",id:"using-the-applications",level:3},{value:"Data Ingestion",id:"data-ingestion",level:2},{value:"Supported File Types",id:"supported-file-types",level:3},{value:"Ingestion Methods",id:"ingestion-methods",level:3},{value:"Method 1: UI Upload (Testing/Small Datasets)",id:"method-1-ui-upload-testingsmall-datasets",level:4},{value:"Method 2: S3 Batch Ingestion (Production/Large Datasets)",id:"method-2-s3-batch-ingestion-productionlarge-datasets",level:4},{value:"Verifying Ingestion",id:"verifying-ingestion",level:3},{value:"Observability",id:"observability",level:2},{value:"Access Monitoring Services",id:"access-monitoring-services",level:3},{value:"Monitoring UIs",id:"monitoring-uis",level:3},{value:"Cleanup",id:"cleanup",level:2},{value:"Uninstall Applications Only",id:"uninstall-applications-only",level:3},{value:"Clean Up Infrastructure",id:"clean-up-infrastructure",level:3},{value:"Cost Considerations",id:"cost-considerations",level:2},{value:"Estimated Monthly Costs",id:"estimated-monthly-costs",level:3},{value:"GPU Instance Cost Breakdown",id:"gpu-instance-cost-breakdown",level:3},{value:"References",id:"references",level:2},{value:"Official NVIDIA Resources",id:"official-nvidia-resources",level:3},{value:"AI-on-EKS Blueprint Resources",id:"ai-on-eks-blueprint-resources",level:3},{value:"Related Technologies",id:"related-technologies",level:3},{value:"Next Steps",id:"next-steps",level:2}];function h(e){const n={a:"a",admonition:"admonition",blockquote:"blockquote",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",h5:"h5",header:"header",hr:"hr",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",span:"span",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.admonition,{type:"warning",children:(0,i.jsxs)(n.p,{children:["Deployment of Enterprise RAG and AI-Q on EKS requires access to GPU instances (g5, p4, or p5 families). This blueprint relies on ",(0,i.jsx)(n.a,{href:"https://karpenter.sh/",children:"Karpenter"})," autoscaling for dynamic GPU provisioning."]})}),"\n",(0,i.jsxs)(n.admonition,{type:"info",children:[(0,i.jsxs)(n.p,{children:["This blueprint provides two deployment options: ",(0,i.jsx)(n.strong,{children:"Enterprise RAG Blueprint"})," (multi-modal document processing with NVIDIA Nemotron and NeMo Retriever Models) or the full ",(0,i.jsx)(n.strong,{children:"AI-Q Research Assistant"})," (adds automated research reports with web search). Both run on Amazon EKS with dynamic GPU autoscaling."]}),(0,i.jsxs)(n.p,{children:["Sources: ",(0,i.jsx)(n.a,{href:"https://github.com/NVIDIA-AI-Blueprints/rag",children:"NVIDIA RAG Blueprint"})," | ",(0,i.jsx)(n.a,{href:"https://github.com/NVIDIA-AI-Blueprints/aiq-research-assistant",children:"NVIDIA AI-Q Research Assistant"})]})]}),"\n",(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"nvidia-enterprise-rag--ai-q-research-assistant-on-amazon-eks",children:"NVIDIA Enterprise RAG & AI-Q Research Assistant on Amazon EKS"})}),"\n",(0,i.jsx)(n.h2,{id:"what-is-nvidia-ai-q-research-assistant",children:"What is NVIDIA AI-Q Research Assistant?"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.a,{href:"https://build.nvidia.com/nvidia/aiq",children:"NVIDIA AI-Q Research Assistant"})," is an AI-powered research assistant that creates custom AI researchers capable of operating anywhere, informed by your own data sources, synthesizing hours of research in minutes. The AI-Q NVIDIA Blueprint enables developers to connect AI agents to enterprise data and use reasoning and tools to distill in-depth source materials with efficiency and precision."]}),"\n",(0,i.jsx)(n.h3,{id:"key-capabilities",children:"Key Capabilities"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Advanced Research Automation:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"5x faster token generation"})," for rapid report synthesis"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"15x faster data ingestion"})," with better semantic accuracy"]}),"\n",(0,i.jsx)(n.li,{children:"Summarize diverse data sets with efficiency and precision"}),"\n",(0,i.jsx)(n.li,{children:"Generate comprehensive research reports automatically"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"NVIDIA NeMo Agent Toolkit:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Ease development and optimization of agentic workflows"}),"\n",(0,i.jsx)(n.li,{children:"Unify, evaluate, audit, and debug workflows across different frameworks"}),"\n",(0,i.jsx)(n.li,{children:"Identify opportunities for optimization"}),"\n",(0,i.jsx)(n.li,{children:"Flexibly choose and connect agents and tools best suited for each task"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Advanced Semantic Query with NVIDIA NeMo Retriever:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Multimodal PDF data extraction and retrieval (text, tables, charts, infographics)"}),"\n",(0,i.jsx)(n.li,{children:"15x faster ingestion of enterprise data"}),"\n",(0,i.jsx)(n.li,{children:"3x lower retrieval latency"}),"\n",(0,i.jsx)(n.li,{children:"Multilingual and cross-lingual support"}),"\n",(0,i.jsx)(n.li,{children:"Reranking to further improve accuracy"}),"\n",(0,i.jsx)(n.li,{children:"GPU-accelerated index creation and search"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Fast Reasoning with Llama Nemotron:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Highest accuracy and lowest latency reasoning capabilities"}),"\n",(0,i.jsxs)(n.li,{children:["Uses ",(0,i.jsx)(n.a,{href:"https://build.nvidia.com/nvidia/llama-3_3-nemotron-super-49b-v1_5",children:"Llama-3.3-Nemotron-Super-49B-v1.5"})," reasoning model"]}),"\n",(0,i.jsx)(n.li,{children:"Analyze data sources and identify patterns"}),"\n",(0,i.jsx)(n.li,{children:"Propose solutions based on comprehensive research"}),"\n",(0,i.jsx)(n.li,{children:"Context-aware generation backed by enterprise data"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Web Search Integration:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Real-time web search powered by Tavily API"}),"\n",(0,i.jsx)(n.li,{children:"Supplements on-premise sources with current information"}),"\n",(0,i.jsx)(n.li,{children:"Expands research beyond internal documents"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"ai-q-components",children:"AI-Q Components"}),"\n",(0,i.jsxs)(n.p,{children:["Per the ",(0,i.jsx)(n.a,{href:"https://github.com/NVIDIA-AI-Blueprints/aiq-research-assistant",children:"official AI-Q architecture"}),":"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"1. NVIDIA AI Workbench"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Simplified development environment for agentic workflows"}),"\n",(0,i.jsx)(n.li,{children:"Local testing and customization"}),"\n",(0,i.jsx)(n.li,{children:"Easy configuration of different LLMs"}),"\n",(0,i.jsx)(n.li,{children:"NVIDIA NeMo Agent Toolkit integration"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"2. NVIDIA RAG Blueprint"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Solution for querying large sets of on-premise multi-modal documents"}),"\n",(0,i.jsx)(n.li,{children:"Supports text, images, tables, and charts extraction"}),"\n",(0,i.jsx)(n.li,{children:"Semantic search and retrieval with GPU acceleration"}),"\n",(0,i.jsx)(n.li,{children:"Foundation for AI-Q's research capabilities"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"3. NVIDIA NeMo Retriever Microservices"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Multi-modal document ingestion"}),"\n",(0,i.jsx)(n.li,{children:"Graphic elements detection"}),"\n",(0,i.jsx)(n.li,{children:"Table structure extraction"}),"\n",(0,i.jsx)(n.li,{children:"PaddleOCR for text recognition"}),"\n",(0,i.jsx)(n.li,{children:"15x faster data ingestion"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"4. NVIDIA NIM Microservices"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Optimized inference containers for LLMs and vision models"}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://build.nvidia.com/nvidia/llama-3_3-nemotron-super-49b-v1_5",children:"Llama-3.3-Nemotron-Super-49B-v1.5"})," reasoning model"]}),"\n",(0,i.jsx)(n.li,{children:"Llama-3.3-70B-Instruct model for report generation"}),"\n",(0,i.jsx)(n.li,{children:"GPU-accelerated inference"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"5. Web Search (Tavily)"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Supplements on-premise sources with real-time web search"}),"\n",(0,i.jsx)(n.li,{children:"Expands research beyond internal documents"}),"\n",(0,i.jsx)(n.li,{children:"Powers web-augmented research reports"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"what-is-nvidia-enterprise-rag-blueprint",children:"What is NVIDIA Enterprise RAG Blueprint?"}),"\n",(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.a,{href:"https://build.nvidia.com/nvidia/build-an-enterprise-rag-pipeline",children:"NVIDIA Enterprise RAG Blueprint"})," is a production-ready reference workflow that provides a complete foundation for building scalable, customizable pipelines for both retrieval and generation. Powered by NVIDIA NeMo Retriever models and NVIDIA Llama Nemotron models, the blueprint is optimized for high accuracy, strong reasoning, and enterprise-scale throughput."]}),"\n",(0,i.jsx)(n.p,{children:"With built-in support for multimodal data ingestion, advanced retrieval, reranking, and reflection techniques, and seamless integration into LLM-powered workflows, it connects language models to enterprise data across text, tables, charts, audio, and infographics from millions of documents\u2014enabling truly context-aware and generative responses."}),"\n",(0,i.jsx)(n.h3,{id:"key-features",children:"Key Features"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Data Ingestion and Processing:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Multimodal PDF data extraction"})," with text, tables, charts and infographics"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Audio file ingestion"})," support"]}),"\n",(0,i.jsx)(n.li,{children:"Custom metadata support"}),"\n",(0,i.jsx)(n.li,{children:"Document summarization"}),"\n",(0,i.jsx)(n.li,{children:"Support for millions of documents at enterprise scale"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Vector Database and Retrieval:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Multi-collection searchability across document sets"}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Hybrid search"})," with dense and sparse search"]}),"\n",(0,i.jsx)(n.li,{children:"Reranking to further improve accuracy"}),"\n",(0,i.jsx)(n.li,{children:"GPU-accelerated index creation and search"}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Pluggable vector database"})," architecture:","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"ElasticSearch support"}),"\n",(0,i.jsx)(n.li,{children:"Milvus support"}),"\n",(0,i.jsx)(n.li,{children:"OpenSearch Serverless support (used in this deployment)"}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.li,{children:"Query decomposition for complex queries"}),"\n",(0,i.jsx)(n.li,{children:"Dynamic metadata filter generation"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Multimodal and Advanced Generation:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Optional ",(0,i.jsx)(n.strong,{children:"Vision Language Model (VLM)"})," support in answer generation"]}),"\n",(0,i.jsx)(n.li,{children:"Opt-in image captioning with VLMs"}),"\n",(0,i.jsx)(n.li,{children:"Multi-turn conversations for interactive Q&A"}),"\n",(0,i.jsx)(n.li,{children:"Multi-session support for concurrent users"}),"\n",(0,i.jsx)(n.li,{children:"Improve accuracy with optional reflection"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Governance and Safety:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Improve content safety with optional programmable guardrails"}),"\n",(0,i.jsx)(n.li,{children:"Enterprise-grade security features"}),"\n",(0,i.jsx)(n.li,{children:"Data privacy and compliance controls"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Observability and Telemetry:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Evaluation scripts included (RAGAS framework)"}),"\n",(0,i.jsx)(n.li,{children:"OpenTelemetry support for distributed tracing"}),"\n",(0,i.jsx)(n.li,{children:"Zipkin integration for trace visualization"}),"\n",(0,i.jsx)(n.li,{children:"Grafana dashboards for metrics and monitoring"}),"\n",(0,i.jsx)(n.li,{children:"Performance profiling and optimization tools"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Developer Features:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"User interface included for testing and demos"}),"\n",(0,i.jsx)(n.li,{children:"NIM Operator support for GPU sharing using DRA"}),"\n",(0,i.jsx)(n.li,{children:"Native Python library support"}),"\n",(0,i.jsx)(n.li,{children:"OpenAI-compatible APIs for easy integration"}),"\n",(0,i.jsx)(n.li,{children:"Decomposable and customizable architecture"}),"\n",(0,i.jsx)(n.li,{children:"Plug-in system for extending functionality"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"enterprise-rag-use-cases",children:"Enterprise RAG Use Cases"}),"\n",(0,i.jsx)(n.p,{children:"The Enterprise RAG Blueprint can be used standalone or as a component in larger systems:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Enterprise search"})," across document repositories"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Knowledge assistants"})," for organizational knowledge bases"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Generative copilots"})," for domain-specific applications"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Vertical AI workflows"})," customized for specific industries"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Foundational component"})," in agentic workflows (like AI-Q Research Assistant)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Customer support automation"})," with context-aware responses"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Document analysis"})," and summarization at scale"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Whether you're building enterprise search, knowledge assistants, generative copilots, or vertical AI workflows, the NVIDIA AI Blueprint for RAG delivers everything needed to move from prototype to production with confidence. It can be used standalone, combined with other NVIDIA Blueprints, or integrated into an agentic workflow to support more advanced reasoning-driven applications."}),"\n",(0,i.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,i.jsxs)(n.p,{children:["This blueprint implements the ",(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.a,{href:"https://github.com/NVIDIA-AI-Blueprints/aiq-research-assistant",children:"NVIDIA AI-Q Research Assistant"})})," on Amazon EKS, combining the ",(0,i.jsx)(n.a,{href:"https://github.com/NVIDIA-AI-Blueprints/rag",children:"NVIDIA RAG Blueprint"})," with AI-Q components for comprehensive research capabilities."]}),"\n",(0,i.jsx)(n.h3,{id:"deployment-options",children:"Deployment Options"}),"\n",(0,i.jsx)(n.p,{children:"This blueprint supports two deployment modes based on your use case:"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Option 1: Enterprise RAG Blueprint"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Deploy NVIDIA Enterprise RAG Blueprint with multi-modal document processing"}),"\n",(0,i.jsx)(n.li,{children:"Includes NeMo Retriever microservices and OpenSearch integration"}),"\n",(0,i.jsx)(n.li,{children:"Best for: Building custom RAG applications, document Q&A systems, knowledge bases"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Option 2: Full AI-Q Research Assistant"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Includes everything from Option 1 plus AI-Q components"}),"\n",(0,i.jsx)(n.li,{children:"Adds automated research report generation with web search capabilities via Tavily API"}),"\n",(0,i.jsx)(n.li,{children:"Best for: Comprehensive research tasks, automated report generation, web-augmented research"}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:["Both deployments include ",(0,i.jsx)(n.a,{href:"https://karpenter.sh/",children:"Karpenter"})," autoscaling and enterprise security features. You can start with Option 1 and add AI-Q components later as your needs evolve."]}),"\n",(0,i.jsx)(n.h3,{id:"deployment-approach",children:"Deployment Approach"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Why This Setup Process?"}),"\nWhile this implementation involves multiple steps, it provides several advantages:"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Complete Infrastructure"}),": Automatically provisions VPC, EKS cluster, OpenSearch Serverless, and monitoring stack"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Enterprise Features"}),": Includes security, monitoring, and scalability features"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"AWS Integration"}),": Leverages ",(0,i.jsx)(n.a,{href:"https://karpenter.sh/",children:"Karpenter"})," autoscaling, EKS Pod Identity authentication, and managed AWS services"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Reproducible"}),": Infrastructure as Code ensures consistent deployments across environments"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"key-features-1",children:"Key Features"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Performance Optimizations:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsxs)(n.strong,{children:[(0,i.jsx)(n.a,{href:"https://karpenter.sh/",children:"Karpenter"})," Autoscaling"]}),": Dynamic GPU node provisioning based on workload demands"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Intelligent Instance Selection"}),": Automatically chooses optimal GPU instance types (G5, P4, P5)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Bin-Packing"}),": Efficient GPU utilization across multiple workloads"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Enterprise Ready:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"OpenSearch Serverless"}),": Managed vector database with automatic scaling"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Pod Identity Authentication"}),": EKS Pod Identity for secure AWS IAM access from pods"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Observability Stack"}),": Prometheus, Grafana, and DCGM for GPU monitoring"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Secure Access"}),": Kubernetes port-forwarding for controlled service access"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"architecture",children:"Architecture"}),"\n",(0,i.jsx)(n.h3,{id:"ai-q-research-assistant-architecture",children:"AI-Q Research Assistant Architecture"}),"\n",(0,i.jsxs)(n.p,{children:["The deployment uses Amazon EKS with ",(0,i.jsx)(n.a,{href:"https://karpenter.sh/",children:"Karpenter"}),"-based dynamic provisioning:"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"NVIDIA AI-Q on EKS",src:s(55036).A+"",width:"2161",height:"1220"})}),"\n",(0,i.jsx)(n.h3,{id:"enterprise-rag-blueprint-architecture",children:"Enterprise RAG Blueprint Architecture"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.img,{alt:"RAG Pipeline with OpenSearch",src:s(67841).A+"",width:"2129",height:"1700"})}),"\n",(0,i.jsxs)(n.p,{children:["The ",(0,i.jsx)(n.a,{href:"https://github.com/NVIDIA-AI-Blueprints/rag",children:"RAG pipeline"})," processes documents through multiple specialized NIM microservices:"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"1. Llama-3.3-Nemotron-Super-49B-v1.5"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://build.nvidia.com/nvidia/llama-3_3-nemotron-super-49b-v1_5",children:"Advanced reasoning model"})}),"\n",(0,i.jsx)(n.li,{children:"Primary reasoning and generation for both RAG and report writing"}),"\n",(0,i.jsx)(n.li,{children:"Query rewriting and decomposition"}),"\n",(0,i.jsx)(n.li,{children:"Filter expression generation"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"2. Embedding & Reranking"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"LLama 3.2 NV-EmbedQA: 2048-dim embeddings"}),"\n",(0,i.jsx)(n.li,{children:"LLama 3.2 NV-RerankQA: Relevance scoring"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"3. NV-Ingest Pipeline"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"PaddleOCR"}),": Text extraction from images"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Page Elements"}),": Document layout understanding"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Graphic Elements"}),": Chart and diagram detection"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Table Structure"}),": Tabular data extraction"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"4. AI-Q Research Assistant Components"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Llama-3.3-70B-Instruct model for report generation (optional, 2 GPUs)"}),"\n",(0,i.jsx)(n.li,{children:"Web search via Tavily API"}),"\n",(0,i.jsx)(n.li,{children:"Backend orchestration for research workflows"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,i.jsx)(n.admonition,{title:"Important - Cost Information",type:"info",children:(0,i.jsxs)(n.p,{children:["This deployment uses GPU instances which can incur significant costs. See ",(0,i.jsx)(n.a,{href:"#cost-considerations",children:"Cost Considerations"})," at the end of this guide for detailed cost estimates. ",(0,i.jsx)(n.strong,{children:"Always clean up resources when not in use."})]})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"System Requirements"}),": Any Linux/macOS system with AWS CLI access"]}),"\n",(0,i.jsx)(n.p,{children:"Install the following tools:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"AWS CLI"}),": Configured with appropriate permissions (",(0,i.jsx)(n.a,{href:"https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html",children:"installation guide"}),")"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"kubectl"}),": Kubernetes command-line tool (",(0,i.jsx)(n.a,{href:"https://kubernetes.io/docs/tasks/tools/install-kubectl/",children:"installation guide"}),")"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"helm"}),": Kubernetes package manager (",(0,i.jsx)(n.a,{href:"https://helm.sh/docs/intro/install/",children:"installation guide"}),")"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"terraform"}),": Infrastructure as code tool (",(0,i.jsx)(n.a,{href:"https://learn.hashicorp.com/tutorials/terraform/install-cli",children:"installation guide"}),")"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"git"}),": Version control (",(0,i.jsx)(n.a,{href:"https://git-scm.com/book/en/v2/Getting-Started-Installing-Git",children:"installation guide"}),")"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"required-api-tokens",children:"Required API Tokens"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"NGC API Token"}),": Required for accessing NVIDIA NIM containers and AI Foundation models","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"First, sign up through one of these options"})," (your API key will only work if you have one of these accounts):","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Option 1 - NVIDIA Developer Program"})," (Quick Start):","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Sign up ",(0,i.jsx)(n.a,{href:"https://build.nvidia.com/",children:"here"})]}),"\n",(0,i.jsx)(n.li,{children:"Free account for POCs and development workloads"}),"\n",(0,i.jsx)(n.li,{children:"Ideal for testing and evaluation"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Option 2 - NVIDIA AI Enterprise"})," (Production):","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Subscribe via ",(0,i.jsx)(n.a,{href:"https://aws.amazon.com/marketplace/pp/prodview-ozgjkov6vq3l6",children:"AWS Marketplace"})]}),"\n",(0,i.jsx)(n.li,{children:"Enterprise license with full support and SLAs"}),"\n",(0,i.jsx)(n.li,{children:"Required for production deployments"}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Then, generate your API key"}),":","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["After signing up through Option 1 or 2, generate your API key at ",(0,i.jsx)(n.a,{href:"https://org.ngc.nvidia.com/setup/personal-keys",children:"NGC Personal Keys"})]}),"\n",(0,i.jsx)(n.li,{children:"Keep this key handy - it will be needed at deployment time"}),"\n"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.a,{href:"https://tavily.com/",children:"Tavily API Key"})}),": ",(0,i.jsx)(n.strong,{children:"Optional for AI-Q Research Assistant"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Enables web search capabilities in AI-Q"}),"\n",(0,i.jsx)(n.li,{children:"AI-Q can work in RAG-only mode without it"}),"\n",(0,i.jsx)(n.li,{children:"Not needed for Enterprise RAG only deployment"}),"\n",(0,i.jsxs)(n.li,{children:["Create account at ",(0,i.jsx)(n.a,{href:"https://tavily.com/",children:"Tavily"})]}),"\n",(0,i.jsx)(n.li,{children:"Generate API key from dashboard"}),"\n",(0,i.jsx)(n.li,{children:"Keep this key handy - it will be needed at deployment time if you want web search in AI-Q"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"gpu-instance-access",children:"GPU Instance Access"}),"\n",(0,i.jsxs)(n.p,{children:["Ensure your AWS account has access to GPU instances. This blueprint supports multiple instance families through ",(0,i.jsx)(n.a,{href:"https://karpenter.sh/",children:"Karpenter"})," NodePools:"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Supported GPU Instance Families:"})}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Instance Family"}),(0,i.jsx)(n.th,{children:"GPU Type"}),(0,i.jsx)(n.th,{children:"Performance Profile"}),(0,i.jsx)(n.th,{children:"Use Case"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsxs)(n.td,{children:[(0,i.jsx)(n.strong,{children:"G5"})," (default)"]}),(0,i.jsx)(n.td,{children:"NVIDIA A10G"}),(0,i.jsx)(n.td,{children:"Cost-effective, 24GB VRAM"}),(0,i.jsx)(n.td,{children:"General workloads, development"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"G6e"})}),(0,i.jsx)(n.td,{children:"NVIDIA L40S"}),(0,i.jsx)(n.td,{children:"Balanced, 48GB VRAM"}),(0,i.jsx)(n.td,{children:"High-memory models"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"P4d/P4de"})}),(0,i.jsx)(n.td,{children:"NVIDIA A100"}),(0,i.jsx)(n.td,{children:"High-performance, 40/80GB VRAM"}),(0,i.jsx)(n.td,{children:"Large-scale deployments"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"P5/P5e/P5en"})}),(0,i.jsx)(n.td,{children:"NVIDIA H100"}),(0,i.jsx)(n.td,{children:"Ultra-high performance, 80GB VRAM"}),(0,i.jsx)(n.td,{children:"Maximum performance"})]})]})]}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Note"}),": G5 instances are pre-configured in the Helm values to provide an accessible starting point. You can switch to P4/P5/G6e instances by editing the ",(0,i.jsx)(n.code,{children:"nodeSelector"})," in the Helm values files - no infrastructure changes required."]}),"\n"]}),"\n",(0,i.jsxs)(l.A,{header:(0,i.jsx)(n.h4,{children:(0,i.jsx)(n.span,{children:"Customizing GPU Instance Types (Optional)"})}),children:[(0,i.jsx)(n.admonition,{title:"GPU Instance Flexibility",type:"tip",children:(0,i.jsxs)(n.p,{children:["This blueprint is pre-configured with ",(0,i.jsx)(n.strong,{children:"G5 instances (A10G GPUs)"})," to provide a cost-effective starting point. However, ",(0,i.jsx)(n.strong,{children:"you can easily switch to P4 (A100) or P5 (H100) instances"})," by modifying the Helm values files. The infrastructure includes Karpenter NodePools for G5, G6, G6e, P4, and P5 instance families - simply change the ",(0,i.jsx)(n.code,{children:"nodeSelector"})," labels to match your performance and budget requirements."]})}),(0,i.jsxs)(n.p,{children:["All components use Karpenter labels for automatic provisioning. ",(0,i.jsx)(n.strong,{children:"Default configuration (G5 instances)"}),":"]}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-yaml",children:"# Example: 8-GPU workloads (49B/70B models)\nnodeSelector:\n  karpenter.k8s.aws/instance-family: g5  # Use G5 (A10G GPUs)\n  karpenter.k8s.aws/instance-size: 48xlarge  # 8x A10G\n  karpenter.sh/capacity-type: on-demand\n\n# Example: 1-GPU workloads (embedding, reranking, OCR)\nnodeSelector:\n  karpenter.k8s.aws/instance-family: g5  # Use G5 (A10G GPUs)\n  karpenter.k8s.aws/instance-size: 12xlarge  # Up to 4x A10G\n"})}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"To use different GPU types"}),", update the ",(0,i.jsx)(n.code,{children:"instance-family"})," in your Helm values:"]}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-yaml",children:"# For P5 (H100 GPUs) - highest performance\nnodeSelector:\n  karpenter.k8s.aws/instance-family: p5\n  karpenter.k8s.aws/instance-size: 48xlarge  # 8x H100\n\n# For P4 (A100 GPUs) - high performance\nnodeSelector:\n  karpenter.k8s.aws/instance-family: p4d\n  karpenter.k8s.aws/instance-size: 24xlarge  # 8x A100\n\n# For G6e (L40S GPUs) - balanced performance\nnodeSelector:\n  karpenter.k8s.aws/instance-family: g6e\n  karpenter.k8s.aws/instance-size: 48xlarge  # 8x L40S\n"})}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"No manual node creation required"})," - Karpenter automatically provisions the right instances based on your ",(0,i.jsx)(n.code,{children:"nodeSelector"})," configuration!"]})]}),"\n",(0,i.jsx)(n.h2,{id:"getting-started",children:"Getting Started"}),"\n",(0,i.jsx)(n.p,{children:"Clone the repository to begin:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"git clone https://github.com/awslabs/ai-on-eks.git\ncd ai-on-eks\n"})}),"\n",(0,i.jsx)(n.h2,{id:"deployment",children:"Deployment"}),"\n",(0,i.jsx)(n.p,{children:"This blueprint provides two deployment methods:"}),"\n",(0,i.jsxs)(l.A,{header:(0,i.jsx)(n.h2,{children:(0,i.jsx)(n.span,{children:"Option A: Automated Deployment (Recommended)"})}),children:[(0,i.jsx)(n.p,{children:"Use the provided bash scripts to automate the complete deployment process."}),(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"\ud83d\udca1 Tip"}),": For detailed manual deployment steps with full configuration control, see ",(0,i.jsx)(n.a,{href:"#option-b-manual-deployment",children:"Option B: Manual Deployment"})," below."]}),"\n"]}),(0,i.jsx)(n.h3,{id:"step-1-deploy-infrastructure",children:"Step 1: Deploy Infrastructure"}),(0,i.jsx)(n.p,{children:"Navigate to the infrastructure directory and run the installation script:"}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"cd infra/nvidia-deep-research\n./install.sh\n"})}),(0,i.jsx)(n.p,{children:"This provisions your complete environment:"}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"VPC"}),": Subnets, security groups, NAT gateways"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"EKS Cluster"}),": With ",(0,i.jsx)(n.a,{href:"https://karpenter.sh/",children:"Karpenter"})," for dynamic GPU provisioning"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"OpenSearch Serverless"}),": Vector database with Pod Identity authentication"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Monitoring Stack"}),": Prometheus, Grafana, and AI/ML observability"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsxs)(n.strong,{children:[(0,i.jsx)(n.a,{href:"https://karpenter.sh/",children:"Karpenter"})," NodePools"]}),": G5, G6, G6e, P4, P5 instance support"]}),"\n"]}),(0,i.jsxs)(n.p,{children:["\u23f1\ufe0f ",(0,i.jsx)(n.strong,{children:"Duration"}),": 15-20 minutes"]}),(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"\u2705 Infrastructure Ready"}),": Once Terraform completes successfully, your infrastructure is deployed and ready."]}),"\n"]}),(0,i.jsx)(n.h3,{id:"step-2-setup-environment",children:"Step 2: Setup Environment"}),(0,i.jsx)(n.p,{children:"Run the setup script to configure your environment:"}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"./deploy.sh setup\n"})}),(0,i.jsx)(n.p,{children:"This script will:"}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Configure kubectl to access your EKS cluster"}),"\n",(0,i.jsx)(n.li,{children:"Collect NGC and Tavily API keys"}),"\n",(0,i.jsx)(n.li,{children:"Verify cluster readiness (Karpenter, NodePools, OpenSearch)"}),"\n",(0,i.jsx)(n.li,{children:"Patch Karpenter limits for GPU nodes"}),"\n",(0,i.jsxs)(n.li,{children:["Save configuration to ",(0,i.jsx)(n.code,{children:".env"})," file"]}),"\n"]}),(0,i.jsx)(n.h3,{id:"step-3-build-opensearch-images",children:"Step 3: Build OpenSearch Images"}),(0,i.jsx)(n.p,{children:"Clone RAG source, integrate OpenSearch, and build custom Docker images:"}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"./deploy.sh build\n"})}),(0,i.jsxs)(n.p,{children:["\u23f1\ufe0f ",(0,i.jsx)(n.strong,{children:"Wait time"}),": 10-15 minutes for image builds"]}),(0,i.jsx)(n.h3,{id:"step-4-deploy-applications",children:"Step 4: Deploy Applications"}),(0,i.jsx)(n.p,{children:"Choose based on your use case:"}),(0,i.jsx)(n.h4,{id:"1-deploy-enterprise-rag-only",children:"1) Deploy Enterprise RAG Only"}),(0,i.jsx)(n.p,{children:"For document Q&A without AI-Q research capabilities:"}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"./deploy.sh rag\n"})}),(0,i.jsxs)(n.p,{children:["\u23f1\ufe0f ",(0,i.jsx)(n.strong,{children:"Wait time"}),": 15-25 minutes"]}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Components deployed:"})}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"49B Nemotron Model"})," (8 GPUs) - ",(0,i.jsx)(n.a,{href:"https://karpenter.sh/",children:"Karpenter"})," will provision g5.48xlarge"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Embedding & Reranking Models"})," (1 GPU each)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Data Ingestion Models"})," (1 GPU each)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"RAG Server"})," with OpenSearch Serverless integration"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Frontend"})," for user interaction"]}),"\n"]}),(0,i.jsx)(n.hr,{}),(0,i.jsx)(n.h4,{id:"2-deploy-ai-q-research-assistant",children:"2) Deploy AI-Q Research Assistant"}),(0,i.jsx)(n.p,{children:"AI-Q includes the Enterprise RAG Blueprint plus automated research report generation with optional web search capabilities."}),(0,i.jsx)(n.h5,{id:"option-a-deploy-all-at-once-recommended---faster",children:"Option A: Deploy All at Once (Recommended - Faster)"}),(0,i.jsx)(n.p,{children:"Deploy both RAG and AI-Q in parallel:"}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"./deploy.sh all\n"})}),(0,i.jsxs)(n.p,{children:["\u23f1\ufe0f ",(0,i.jsx)(n.strong,{children:"Wait time"}),": 25-30 minutes"]}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"All components deployed:"})}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"RAG"}),": 49B Nemotron Model, Embedding & Reranking Models, Data Ingestion Models, RAG Server, Frontend"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"AI-Q"}),": 70B Instruct Model, AIRA Backend, Frontend, Web Search (if Tavily API key provided)"]}),"\n"]}),(0,i.jsx)(n.h5,{id:"option-b-deploy-sequentially",children:"Option B: Deploy Sequentially"}),(0,i.jsx)(n.p,{children:"Deploy RAG first, then add AI-Q:"}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Step 1: Deploy RAG\n./deploy.sh rag\n\n# Step 2: Deploy AI-Q\n# AI-Q can work with or without web search (Tavily API is optional)\n./deploy.sh aira\n"})}),(0,i.jsxs)(n.p,{children:["\u23f1\ufe0f ",(0,i.jsx)(n.strong,{children:"Wait time"}),": 15-25 minutes for RAG, then 20-30 minutes for AI-Q (35-55 minutes total)"]}),(0,i.jsx)(n.hr,{})]}),"\n",(0,i.jsxs)(l.A,{header:(0,i.jsx)(n.h2,{children:(0,i.jsx)(n.span,{children:"Option B: Manual Deployment"})}),children:[(0,i.jsx)(n.p,{children:"Follow detailed manual steps to understand each component and configuration. Ideal for learning, customizing, or troubleshooting."}),(0,i.jsx)(n.h3,{id:"step-1-deploy-infrastructure-1",children:"Step 1: Deploy Infrastructure"}),(0,i.jsx)(n.p,{children:"Navigate to the infrastructure directory:"}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"cd infra/nvidia-deep-research\n"})}),(0,i.jsx)(n.p,{children:"Run the installation script:"}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"./install.sh\n"})}),(0,i.jsxs)(n.p,{children:["\u23f1\ufe0f ",(0,i.jsx)(n.strong,{children:"Duration"}),": 15-20 minutes"]}),(0,i.jsx)(n.p,{children:"This provisions:"}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"VPC with public and private subnets"}),"\n",(0,i.jsxs)(n.li,{children:["EKS Cluster with ",(0,i.jsx)(n.a,{href:"https://karpenter.sh/",children:"Karpenter"})]}),"\n",(0,i.jsx)(n.li,{children:"OpenSearch Serverless collection"}),"\n",(0,i.jsx)(n.li,{children:"Monitoring stack (Prometheus, Grafana)"}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://karpenter.sh/",children:"Karpenter"})," NodePools for GPU instances"]}),"\n"]}),(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"\u2705 Infrastructure Ready"}),": Once Terraform completes successfully, your infrastructure is deployed and ready."]}),"\n"]}),(0,i.jsx)(n.h3,{id:"step-2-setup-environment-1",children:"Step 2: Setup Environment"}),(0,i.jsx)(n.p,{children:"Configure kubectl and set required environment variables:"}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'# Cluster configuration\nexport CLUSTER_NAME="nvidia-deep-research"\nexport REGION="us-west-2"\n\n# Configure kubectl\naws eks update-kubeconfig --region $REGION --name $CLUSTER_NAME\n\n# Verify cluster connection\nkubectl get nodes\n\n# Get AWS Account ID\nexport ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)\n\n# OpenSearch Configuration\nexport OPENSEARCH_SERVICE_ACCOUNT="opensearch-access-sa"\nexport OPENSEARCH_NAMESPACE="rag"\nexport COLLECTION_NAME="osv-vector-dev"\n\n# Get OpenSearch endpoint from Terraform output\nexport OPENSEARCH_ENDPOINT=$(cd terraform/_LOCAL && terraform output -raw opensearch_collection_endpoint)\n\necho "OpenSearch Endpoint: $OPENSEARCH_ENDPOINT"\n\n# NGC API Key (required)\nexport NGC_API_KEY="<YOUR_NGC_API_KEY>"\n\n# Tavily API Key for AI-Q (optional - enables web search)\nexport TAVILY_API_KEY="<YOUR_TAVILY_API_KEY>"  # Skip if deploying RAG only, or for AI-Q without web search\n'})}),(0,i.jsxs)(n.h3,{id:"step-3-configure-karpenter-nodepool-limits",children:["Step 3: Configure ",(0,i.jsx)(n.a,{href:"https://karpenter.sh/",children:"Karpenter"})," NodePool Limits"]}),(0,i.jsx)(n.p,{children:"Increase the memory limit for the G5 GPU NodePool:"}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'kubectl patch nodepool g5-gpu-karpenter --type=\'json\' -p=\'[{"op": "replace", "path": "/spec/limits/memory", "value": "2000Gi"}]\'\n'})}),(0,i.jsxs)(n.p,{children:["This allows ",(0,i.jsx)(n.a,{href:"https://karpenter.sh/",children:"Karpenter"})," to provision sufficient GPU nodes for all models (from 1000Gi to 2000Gi)."]}),(0,i.jsx)(n.h3,{id:"step-4-integrate-opensearch-and-build-docker-images",children:"Step 4: Integrate OpenSearch and Build Docker Images"}),(0,i.jsx)(n.p,{children:"Clone the RAG source code and add OpenSearch implementation:"}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'# Clone RAG source code\ngit clone -b v2.3.0 https://github.com/NVIDIA-AI-Blueprints/rag.git rag\n\n# Download OpenSearch implementation\nCOMMIT_HASH="47cd8b345e5049d49d8beb406372de84bd005abe"\ncurl -L https://github.com/NVIDIA/nim-deploy/archive/${COMMIT_HASH}.tar.gz | tar xz --strip=5 nim-deploy-${COMMIT_HASH}/cloud-service-providers/aws/blueprints/deep-research-blueprint-eks/opensearch\n\n# Copy OpenSearch implementation into RAG source\ncp -r opensearch/vdb/opensearch rag/src/nvidia_rag/utils/vdb/\ncp opensearch/main.py rag/src/nvidia_rag/ingestor_server/main.py\ncp opensearch/vdb/__init__.py rag/src/nvidia_rag/utils/vdb/__init__.py\ncp opensearch/pyproject.toml rag/pyproject.toml\n\n# Login to NGC registry\ndocker login nvcr.io  # username: $oauthtoken, password: NGC API Key\n\n# Login to ECR\naws ecr get-login-password --region ${REGION} | docker login --username AWS --password-stdin ${ACCOUNT_ID}.dkr.ecr.${REGION}.amazonaws.com\n\n# Build and push OpenSearch-enabled RAG images to ECR\n./opensearch/build-opensearch-images.sh\n'})}),(0,i.jsxs)(n.p,{children:["\u23f1\ufe0f ",(0,i.jsx)(n.strong,{children:"Wait time"}),": 10-15 minutes for image builds"]}),(0,i.jsx)(n.h3,{id:"step-5-deploy-enterprise-rag-blueprint",children:"Step 5: Deploy Enterprise RAG Blueprint"}),(0,i.jsx)(n.p,{children:"Deploy the RAG Blueprint using OpenSearch-enabled images:"}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'# Set deployment variables\nexport ECR_REGISTRY="${ACCOUNT_ID}.dkr.ecr.${REGION}.amazonaws.com"\nexport IMAGE_TAG="2.3.0-opensearch"\n\n# Deploy RAG with OpenSearch configuration\nhelm upgrade --install rag -n rag \\\n  https://helm.ngc.nvidia.com/nvidia/blueprint/charts/nvidia-blueprint-rag-v2.3.0.tgz \\\n  --username \'$oauthtoken\' \\\n  --password "${NGC_API_KEY}" \\\n  --create-namespace \\\n  --set imagePullSecret.password=$NGC_API_KEY \\\n  --set ngcApiSecret.password=$NGC_API_KEY \\\n  --set serviceAccount.create=false \\\n  --set serviceAccount.name=$OPENSEARCH_SERVICE_ACCOUNT \\\n  --set image.repository="${ECR_REGISTRY}/nvidia-rag-server" \\\n  --set image.tag="${IMAGE_TAG}" \\\n  --set ingestor-server.image.repository="${ECR_REGISTRY}/nvidia-rag-ingestor" \\\n  --set ingestor-server.image.tag="${IMAGE_TAG}" \\\n  --set envVars.APP_VECTORSTORE_URL="${OPENSEARCH_ENDPOINT}" \\\n  --set envVars.APP_VECTORSTORE_AWS_REGION="${REGION}" \\\n  --set ingestor-server.envVars.APP_VECTORSTORE_URL="${OPENSEARCH_ENDPOINT}" \\\n  --set ingestor-server.envVars.APP_VECTORSTORE_AWS_REGION="${REGION}" \\\n  -f helm/rag-values-os.yaml\n\n# Patch ingestor-server to use OpenSearch service account\nkubectl patch deployment ingestor-server -n rag \\\n  -p "{\\"spec\\":{\\"template\\":{\\"spec\\":{\\"serviceAccountName\\":\\"$OPENSEARCH_SERVICE_ACCOUNT\\"}}}}"\n'})}),(0,i.jsxs)(n.p,{children:["\u23f1\ufe0f ",(0,i.jsx)(n.strong,{children:"Wait time"}),": 10-20 minutes for model downloads and GPU provisioning"]}),(0,i.jsx)(n.p,{children:"Verify RAG deployment:"}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Check all pods in RAG namespace\nkubectl get all -n rag\n\n# Wait for all pods to be ready\nkubectl wait --for=condition=ready pod -l app.kubernetes.io/instance=rag -n rag --timeout=600s\n\n# Verify service accounts\nkubectl get pod -n rag -l app.kubernetes.io/component=rag-server -o jsonpath='{.items[0].spec.serviceAccountName}'\nkubectl get pod -n rag -l app=ingestor-server -o jsonpath='{.items[0].spec.serviceAccountName}'\n"})}),(0,i.jsx)(n.p,{children:"Deploy DCGM ServiceMonitor for GPU metrics:"}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Deploy ServiceMonitor to connect RAG's Prometheus to infrastructure DCGM Exporter\nkubectl apply -f - <<EOF\napiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  name: dcgm-exporter\n  namespace: rag\n  labels:\n    release: rag\nspec:\n  namespaceSelector:\n    matchNames:\n      - monitoring\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: dcgm-exporter\n  endpoints:\n    - port: metrics\n      interval: 15s\n      path: /metrics\nEOF\n"})}),(0,i.jsxs)(n.p,{children:["This ServiceMonitor allows the Prometheus instance in the ",(0,i.jsx)(n.code,{children:"rag"})," namespace to discover and scrape GPU metrics from the DCGM Exporter running in the ",(0,i.jsx)(n.code,{children:"monitoring"})," namespace."]}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Deploy NVIDIA DCGM Grafana Dashboard (Optional but Recommended):"})}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'# Download and deploy the official NVIDIA DCGM dashboard (with datasource fix)\ncurl -s https://grafana.com/api/dashboards/12239 | jq -r \'.json\' | \\\n    jq \'walk(if type == "object" and has("datasource") and (.datasource | type == "string") then .datasource = {"type": "prometheus", "uid": "prometheus"} else . end)\' \\\n    > /tmp/dcgm-dashboard.json\nkubectl create configmap nvidia-dcgm-exporter-dashboard \\\n    -n rag \\\n    --from-file=nvidia-dcgm-exporter.json=/tmp/dcgm-dashboard.json \\\n    --dry-run=client -o yaml | \\\n    kubectl label --local -f - grafana_dashboard=1 --dry-run=client -o yaml | \\\n    kubectl apply -f -\n'})}),(0,i.jsx)(n.p,{children:"This dashboard will be automatically loaded by Grafana's sidecar and will display GPU utilization, temperature, memory usage, and other GPU metrics."}),(0,i.jsx)(n.hr,{}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Deploy AI-Q Research Assistant (Optional)"})}),(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"\ud83d\udcdd Deployment Choice"}),": Deploy these components if you need automated research report generation with web search capabilities. If your use case only requires the Enterprise RAG Blueprint for document Q&A, proceed to ",(0,i.jsx)(n.a,{href:"#access-services",children:"Access Services"}),"."]}),"\n"]}),(0,i.jsx)(n.h3,{id:"step-6-deploy-ai-q-components",children:"Step 6: Deploy AI-Q Components"}),(0,i.jsx)(n.p,{children:"Deploy the AI-Q Research Assistant:"}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'# Verify TAVILY_API_KEY is set\necho "Tavily API Key: ${TAVILY_API_KEY:0:10}..."\n\n# Deploy AIRA using NGC Helm chart\nhelm upgrade --install aira https://helm.ngc.nvidia.com/nvidia/blueprint/charts/aiq-aira-v1.2.0.tgz \\\n  --username=\'$oauthtoken\' \\\n  --password="${NGC_API_KEY}" \\\n  -n nv-aira --create-namespace \\\n  -f helm/aira-values.eks.yaml \\\n  --set imagePullSecret.password="$NGC_API_KEY" \\\n  --set ngcApiSecret.password="$NGC_API_KEY" \\\n  --set tavilyApiSecret.password="$TAVILY_API_KEY"\n'})}),(0,i.jsxs)(n.p,{children:["\u23f1\ufe0f ",(0,i.jsx)(n.strong,{children:"Wait time"}),": 15-20 minutes for 70B model download"]}),(0,i.jsx)(n.p,{children:"Verify AI-Q deployment:"}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Check all AIRA components\nkubectl get all -n nv-aira\n\n# Wait for all components to be ready\nkubectl wait --for=condition=ready pod -l app=aira -n nv-aira --timeout=1200s\n\n# Check pod distribution\nkubectl get pods -n nv-aira -o wide\n"})})]}),"\n",(0,i.jsx)(n.h2,{id:"access-services",children:"Access Services"}),"\n",(0,i.jsx)(n.p,{children:"Once deployment is complete, access the services locally using port-forwarding."}),"\n",(0,i.jsxs)(l.A,{header:(0,i.jsx)(n.h3,{children:(0,i.jsx)(n.span,{children:"Port Forwarding Commands"})}),children:[(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Start Port Forwarding for RAG Services:"})}),(0,i.jsx)(n.p,{children:"Navigate to the blueprints directory:"}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"cd ../../blueprints/inference/nvidia-deep-research\n"})}),(0,i.jsx)(n.p,{children:"Start RAG port-forwarding:"}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"./app.sh port start rag\n"})}),(0,i.jsx)(n.p,{children:"This enables access to:"}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"RAG Frontend"}),": ",(0,i.jsx)(n.a,{href:"http://localhost:3001",children:"http://localhost:3001"})," - Test RAG Q&A directly"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Ingestor API"}),": ",(0,i.jsx)(n.a,{href:"http://localhost:8082",children:"http://localhost:8082"})," - API docs at ",(0,i.jsx)(n.a,{href:"http://localhost:8082/docs",children:"http://localhost:8082/docs"})]}),"\n"]}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Start Port Forwarding for AI-Q Services"})," (if deployed):"]}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"./app.sh port start aira\n"})}),(0,i.jsx)(n.p,{children:"This enables access to:"}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"AIRA Research Assistant"}),": ",(0,i.jsx)(n.a,{href:"http://localhost:3000",children:"http://localhost:3000"})," - Generate comprehensive research reports with web search"]}),"\n"]}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Managing Port Forwarding:"})}),(0,i.jsx)(n.p,{children:"Check status:"}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"./app.sh port status\n"})}),(0,i.jsx)(n.p,{children:"Stop port forwarding:"}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"./app.sh port stop rag      # Stop RAG services\n./app.sh port stop aira     # Stop AI-Q services\n./app.sh port stop all      # Stop all services\n"})})]}),"\n",(0,i.jsx)(n.h3,{id:"using-the-applications",children:"Using the Applications"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsxs)(n.strong,{children:["RAG Frontend (",(0,i.jsx)(n.a,{href:"http://localhost:3001",children:"http://localhost:3001"}),"):"]})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Upload documents directly through the UI"}),"\n",(0,i.jsx)(n.li,{children:"Ask questions about your ingested documents"}),"\n",(0,i.jsx)(n.li,{children:"Test multi-turn conversations"}),"\n",(0,i.jsx)(n.li,{children:"View citations and sources"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsxs)(n.strong,{children:["AI-Q Research Assistant (",(0,i.jsx)(n.a,{href:"http://localhost:3000",children:"http://localhost:3000"}),"):"]})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Define research topics and questions"}),"\n",(0,i.jsx)(n.li,{children:"Leverage both uploaded documents and web search"}),"\n",(0,i.jsx)(n.li,{children:"Generate comprehensive research reports automatically"}),"\n",(0,i.jsx)(n.li,{children:"Export reports in various formats"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsxs)(n.strong,{children:["Ingestor API (",(0,i.jsx)(n.a,{href:"http://localhost:8082/docs",children:"http://localhost:8082/docs"}),"):"]})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Programmatic document ingestion"}),"\n",(0,i.jsx)(n.li,{children:"Batch upload capabilities"}),"\n",(0,i.jsx)(n.li,{children:"Collection management"}),"\n",(0,i.jsx)(n.li,{children:"View OpenAPI documentation"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"data-ingestion",children:"Data Ingestion"}),"\n",(0,i.jsx)(n.p,{children:"After deploying RAG (and optionally AI-Q), you can ingest documents into the OpenSearch vector database."}),"\n",(0,i.jsx)(n.h3,{id:"supported-file-types",children:"Supported File Types"}),"\n",(0,i.jsx)(n.p,{children:"The RAG pipeline supports multi-modal document ingestion including:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"PDF documents"}),"\n",(0,i.jsx)(n.li,{children:"Text files (.txt, .md)"}),"\n",(0,i.jsx)(n.li,{children:"Images (.jpg, .png)"}),"\n",(0,i.jsx)(n.li,{children:"Office documents (.docx, .pptx)"}),"\n",(0,i.jsx)(n.li,{children:"HTML files"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"The NeMo Retriever microservices will automatically extract text, tables, charts, and images from these documents."}),"\n",(0,i.jsx)(n.h3,{id:"ingestion-methods",children:"Ingestion Methods"}),"\n",(0,i.jsx)(n.p,{children:"You have two options for ingesting documents:"}),"\n",(0,i.jsx)(n.h4,{id:"method-1-ui-upload-testingsmall-datasets",children:"Method 1: UI Upload (Testing/Small Datasets)"}),"\n",(0,i.jsx)(n.p,{children:"Upload individual documents directly through the frontend interfaces:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"RAG Frontend"})," (",(0,i.jsx)(n.a,{href:"http://localhost:3001",children:"http://localhost:3001"}),") - Ideal for testing individual documents"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"AIRA Frontend"})," (",(0,i.jsx)(n.a,{href:"http://localhost:3000",children:"http://localhost:3000"}),") - Upload documents for research tasks"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"This method is perfect for:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Testing the RAG pipeline"}),"\n",(0,i.jsx)(n.li,{children:"Small document collections (< 100 documents)"}),"\n",(0,i.jsx)(n.li,{children:"Quick experimentation"}),"\n",(0,i.jsx)(n.li,{children:"Ad-hoc document uploads"}),"\n"]}),"\n",(0,i.jsx)(n.h4,{id:"method-2-s3-batch-ingestion-productionlarge-datasets",children:"Method 2: S3 Batch Ingestion (Production/Large Datasets)"}),"\n",(0,i.jsxs)(l.A,{header:(0,i.jsx)(n.h4,{children:(0,i.jsx)(n.span,{children:"S3 Batch Ingestion Commands"})}),children:[(0,i.jsx)(n.p,{children:"Use the data ingestion script to batch process documents from an S3 bucket. Recommended for:"}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Production deployments"}),"\n",(0,i.jsx)(n.li,{children:"Large document collections (hundreds to thousands of documents)"}),"\n",(0,i.jsx)(n.li,{children:"Automated ingestion workflows"}),"\n",(0,i.jsx)(n.li,{children:"Scheduled data updates"}),"\n"]}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Steps:"})}),(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Ensure the RAG port-forward is running:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"./app.sh port start rag\n"})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Run the data ingestion script (it will prompt for S3 bucket details):"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"./app.sh ingest\n"})}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsx)(n.p,{children:"Or set environment variables to skip prompts:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:'export S3_BUCKET_NAME="your-pdf-bucket-name"\nexport S3_PREFIX="documents/"  # Optional folder path\n./app.sh ingest\n'})}),"\n"]}),"\n"]}),(0,i.jsx)(n.p,{children:"The script will:"}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Download documents from your S3 bucket"}),"\n",(0,i.jsx)(n.li,{children:"Download batch ingestion tools from NVIDIA RAG repository"}),"\n",(0,i.jsx)(n.li,{children:"Process them through the NeMo Retriever pipeline"}),"\n",(0,i.jsx)(n.li,{children:"Store embeddings in OpenSearch Serverless"}),"\n",(0,i.jsx)(n.li,{children:"Display ingestion progress and statistics"}),"\n"]}),(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Additional Resources"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://github.com/NVIDIA-AI-Blueprints/rag/tree/v2.3.0/scripts",children:"RAG batch_ingestion.py documentation"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://github.com/NVIDIA-AI-Blueprints/aiq-research-assistant/blob/main/data/readme.md#bulk-upload-via-python",children:"AI-Q bulk data ingestion documentation"})}),"\n"]}),"\n"]})]}),"\n",(0,i.jsx)(n.h3,{id:"verifying-ingestion",children:"Verifying Ingestion"}),"\n",(0,i.jsx)(n.p,{children:"After ingestion, verify your documents are available:"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Via RAG Frontend"}),": Navigate to ",(0,i.jsx)(n.a,{href:"http://localhost:3001",children:"http://localhost:3001"})," and ask a question about your documents"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Via Ingestor API"}),": Check ",(0,i.jsx)(n.a,{href:"http://localhost:8082/docs",children:"http://localhost:8082/docs"})," for collection statistics"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Via OpenSearch"}),": Query the OpenSearch collection directly using the AWS Console"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"observability",children:"Observability"}),"\n",(0,i.jsx)(n.p,{children:"The RAG and AI-Q deployments include built-in observability tools for monitoring performance, tracing requests, and viewing metrics."}),"\n",(0,i.jsx)(n.h3,{id:"access-monitoring-services",children:"Access Monitoring Services"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Automated Approach (Recommended):"})}),"\n",(0,i.jsx)(n.p,{children:"Navigate to the blueprints directory and start port-forwarding:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"cd ../../blueprints/inference/nvidia-deep-research\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"./app.sh port start observability\n"})}),"\n",(0,i.jsx)(n.p,{children:"This automatically port-forwards:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Zipkin"}),": ",(0,i.jsx)(n.a,{href:"http://localhost:9411",children:"http://localhost:9411"})," - RAG distributed tracing"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Grafana"}),": ",(0,i.jsx)(n.a,{href:"http://localhost:8080",children:"http://localhost:8080"})," - RAG metrics and dashboards"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Phoenix"}),": ",(0,i.jsx)(n.a,{href:"http://localhost:6006",children:"http://localhost:6006"})," - AI-Q workflow tracing (if deployed)"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Check status:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"./app.sh port status\n"})}),"\n",(0,i.jsx)(n.p,{children:"Stop observability port-forwards:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"./app.sh port stop observability\n"})}),"\n",(0,i.jsxs)(l.A,{header:(0,i.jsx)(n.h4,{children:(0,i.jsx)(n.span,{children:"Manual kubectl Commands"})}),children:[(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"RAG Observability (Zipkin & Grafana):"})}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Port-forward Zipkin for distributed tracing (run in a separate terminal)\nkubectl port-forward -n rag svc/rag-zipkin 9411:9411\n\n# Port-forward Grafana for metrics and dashboards (run in another separate terminal)\nkubectl port-forward -n rag svc/rag-grafana 8080:80\n"})}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"AI-Q Observability (Phoenix):"})}),(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Port-forward Phoenix for AI-Q tracing (run in a separate terminal)\nkubectl port-forward -n nv-aira svc/aira-phoenix 6006:6006\n"})})]}),"\n",(0,i.jsx)(n.h3,{id:"monitoring-uis",children:"Monitoring UIs"}),"\n",(0,i.jsx)(n.p,{children:"Once port-forwarding is active:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Zipkin UI"})," (RAG tracing): ",(0,i.jsx)(n.a,{href:"http://localhost:9411",children:"http://localhost:9411"})]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"View end-to-end request traces"}),"\n",(0,i.jsx)(n.li,{children:"Analyze latency bottlenecks"}),"\n",(0,i.jsx)(n.li,{children:"Debug multi-service interactions"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Grafana UI"})," (RAG metrics): ",(0,i.jsx)(n.a,{href:"http://localhost:8080",children:"http://localhost:8080"})]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Default credentials: admin/admin"}),"\n",(0,i.jsx)(n.li,{children:"Pre-built dashboards for RAG metrics"}),"\n",(0,i.jsx)(n.li,{children:"GPU utilization and throughput monitoring"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Phoenix UI"})," (AI-Q tracing): ",(0,i.jsx)(n.a,{href:"http://localhost:6006",children:"http://localhost:6006"})]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Agent workflow visualization"}),"\n",(0,i.jsx)(n.li,{children:"LLM call tracing"}),"\n",(0,i.jsx)(n.li,{children:"Research report generation analysis"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Note"}),": For detailed information on using these observability tools, refer to:"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://github.com/NVIDIA-AI-Blueprints/rag/blob/main/docs/observability.md#view-traces-in-zipkin",children:"Viewing Traces in Zipkin"})}),"\n",(0,i.jsx)(n.li,{children:(0,i.jsx)(n.a,{href:"https://github.com/NVIDIA-AI-Blueprints/rag/blob/main/docs/observability.md#view-metrics-in-grafana",children:"Viewing Metrics in Grafana Dashboard"})}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Alternative"}),": If you need to expose monitoring services publicly, you can create an Ingress resource with appropriate authentication and security controls."]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"cleanup",children:"Cleanup"}),"\n",(0,i.jsx)(n.h3,{id:"uninstall-applications-only",children:"Uninstall Applications Only"}),"\n",(0,i.jsx)(n.p,{children:"To remove the RAG and AI-Q applications while keeping the infrastructure:"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Using Automation Script (Recommended):"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"cd ../../blueprints/inference/nvidia-deep-research\n"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"./app.sh cleanup\n"})}),"\n",(0,i.jsx)(n.p,{children:"The cleanup script will:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Stop all port-forwarding processes"}),"\n",(0,i.jsx)(n.li,{children:"Uninstall AIRA and RAG Helm releases"}),"\n",(0,i.jsx)(n.li,{children:"Remove local port-forward PID files"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Manual Application Cleanup:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Navigate to blueprints directory\ncd ../../blueprints/inference/nvidia-deep-research\n\n# Stop port-forwards\n./app.sh port stop all\n\n# Uninstall AIRA (if deployed)\nhelm uninstall aira -n nv-aira\n\n# Uninstall RAG\nhelm uninstall rag -n rag\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"(Optional) Clean up temporary files created during deployment:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"rm /tmp/.port-forward-*.pid\n"})}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Note"}),": This only removes the applications. The EKS cluster and infrastructure will remain running. GPU nodes will be terminated by ",(0,i.jsx)(n.a,{href:"https://karpenter.sh/",children:"Karpenter"})," within 5-10 minutes."]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"clean-up-infrastructure",children:"Clean Up Infrastructure"}),"\n",(0,i.jsx)(n.p,{children:"To remove the entire EKS cluster and all infrastructure components:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Navigate to infra directory\ncd ../../../infra/nvidia-deep-research\n\n# Run cleanup script\n./cleanup.sh\n"})}),"\n",(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Warning"}),": This will permanently delete:"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"EKS cluster and all workloads"}),"\n",(0,i.jsx)(n.li,{children:"OpenSearch Serverless collection and data"}),"\n",(0,i.jsx)(n.li,{children:"VPC and networking resources"}),"\n",(0,i.jsx)(n.li,{children:"All associated AWS resources"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Backup important data before proceeding."}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Duration"}),": ~10-15 minutes for complete teardown"]}),"\n",(0,i.jsx)(n.h2,{id:"cost-considerations",children:"Cost Considerations"}),"\n",(0,i.jsxs)(l.A,{header:(0,i.jsx)(n.h3,{children:(0,i.jsx)(n.span,{children:"Estimated Costs for This Deployment"})}),children:[(0,i.jsx)(n.admonition,{title:"Important",type:"warning",children:(0,i.jsxs)(n.p,{children:["GPU instances and supporting infrastructure can incur significant costs if left running. ",(0,i.jsx)(n.strong,{children:"Always clean up resources when not in use"})," to avoid unexpected charges."]})}),(0,i.jsx)(n.h3,{id:"estimated-monthly-costs",children:"Estimated Monthly Costs"}),(0,i.jsxs)(n.p,{children:["The following table shows approximate costs for the ",(0,i.jsx)(n.strong,{children:"default deployment"})," in US West 2 (Oregon) region. Actual costs will vary based on region, usage patterns, and workload duration."]}),(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Resource"}),(0,i.jsx)(n.th,{children:"Configuration"}),(0,i.jsx)(n.th,{children:"Estimated Monthly Cost"}),(0,i.jsx)(n.th,{children:"Notes"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"EKS Control Plane"})}),(0,i.jsx)(n.td,{children:"1 cluster"}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"~$73/month"})}),(0,i.jsx)(n.td,{children:"Fixed cost: $0.10/hour \xd7 730 hours"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"GPU Instances (RAG Only)"})}),(0,i.jsxs)(n.td,{children:["1x g5.48xlarge (8x A10G)",(0,i.jsx)("br",{}),"2x g5.12xlarge (4x A10G each)"]}),(0,i.jsxs)(n.td,{children:[(0,i.jsx)(n.strong,{children:"~$20,171/month"}),"*"]}),(0,i.jsxs)(n.td,{children:["Only when workloads are running",(0,i.jsx)("br",{}),"Karpenter scales down when idle"]})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"GPU Instances (RAG + AI-Q)"})}),(0,i.jsx)(n.td,{children:"Additional g5.48xlarge"}),(0,i.jsxs)(n.td,{children:[(0,i.jsx)(n.strong,{children:"~$32,061/month"}),"*"]}),(0,i.jsx)(n.td,{children:"Additional 70B model requires 8 more GPUs"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"OpenSearch Serverless"})}),(0,i.jsx)(n.td,{children:"2-4 OCUs (typical)"}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"~$350-700/month"})}),(0,i.jsxs)(n.td,{children:["$0.24/OCU-hour",(0,i.jsx)("br",{}),"Scales based on data volume and queries"]})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"NAT Gateway"})}),(0,i.jsx)(n.td,{children:"2 AZs"}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"~$66/month"})}),(0,i.jsxs)(n.td,{children:["Fixed: 2 gateways \xd7 $0.045/hour \xd7 730 hours",(0,i.jsx)("br",{}),"Plus data processing: $0.045/GB"]})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"ECR Storage"})}),(0,i.jsx)(n.td,{children:"Docker images"}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"~$5-10/month"})}),(0,i.jsxs)(n.td,{children:["50-100GB of custom images",(0,i.jsx)("br",{}),"ECR pricing: $0.10/GB/month"]})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"EBS Volumes"})}),(0,i.jsx)(n.td,{children:"Node storage"}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"~$72/month"})}),(0,i.jsxs)(n.td,{children:["300GB gp3 per node \xd7 3 nodes \xd7 $0.08/GB",(0,i.jsx)("br",{}),"Only charged when GPU nodes running"]})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Data Transfer"})}),(0,i.jsx)(n.td,{children:"Cross-AZ, Internet"}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Variable"})}),(0,i.jsxs)(n.td,{children:["Depends on usage patterns",(0,i.jsx)("br",{}),"Cross-AZ: $0.01/GB, Internet: $0.09/GB"]})]})]})]}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"*GPU Instance Costs assume continuous operation. See breakdown below."})}),(0,i.jsx)(n.h3,{id:"gpu-instance-cost-breakdown",children:"GPU Instance Cost Breakdown"}),(0,i.jsxs)(n.p,{children:["GPU instances are the ",(0,i.jsx)(n.strong,{children:"primary cost driver"}),". Costs depend on instance type and how long they run:"]}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Default Configuration (G5 Instances - RAG Only):"})}),(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Instance Type"}),(0,i.jsx)(n.th,{children:"GPUs"}),(0,i.jsx)(n.th,{children:"On-Demand Rate"}),(0,i.jsx)(n.th,{children:"Daily Cost (24hr)"}),(0,i.jsx)(n.th,{children:"Monthly Cost (730hr)"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"g5.48xlarge (\xd71)"}),(0,i.jsx)(n.td,{children:"8x A10G"}),(0,i.jsx)(n.td,{children:"$16.288/hr"}),(0,i.jsx)(n.td,{children:"$390.91"}),(0,i.jsx)(n.td,{children:"$11,890.24"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:"g5.12xlarge (\xd72)"}),(0,i.jsx)(n.td,{children:"4x A10G each"}),(0,i.jsx)(n.td,{children:"$5.672/hr each"}),(0,i.jsx)(n.td,{children:"$136.13 each"}),(0,i.jsx)(n.td,{children:"$4,140.56 each"})]})]})]}),(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Total for RAG"}),": ~$20,171/month if running 24/7 (1\xd7 g5.48xlarge + 2\xd7 g5.12xlarge = $11,890 + $8,281)"]}),(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"With AI-Q (Additional 70B Model):"})}),(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Additional g5.48xlarge: $11,890.24/month"}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Total"}),": ~$32,061/month if running 24/7 (2\xd7 g5.48xlarge + 2\xd7 g5.12xlarge)"]}),"\n"]}),(0,i.jsxs)(n.blockquote,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Note"}),": If using alternative instance types (G6e, P4, P5), costs will vary. Check ",(0,i.jsx)(n.a,{href:"https://aws.amazon.com/ec2/pricing/on-demand/",children:"AWS EC2 Pricing"})," for your region and instance type."]}),"\n"]})]}),"\n",(0,i.jsx)(n.h2,{id:"references",children:"References"}),"\n",(0,i.jsx)(n.h3,{id:"official-nvidia-resources",children:"Official NVIDIA Resources"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"\ud83d\udcda Documentation:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://github.com/NVIDIA-AI-Blueprints/aiq-research-assistant",children:"NVIDIA AI-Q Research Assistant GitHub"}),": Official AI-Q blueprint repository"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://build.nvidia.com/nvidia/aiq",children:"NVIDIA AI-Q on AI Foundation"}),": AI-Q blueprint card and hosted version"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://github.com/NVIDIA-AI-Blueprints/rag",children:"NVIDIA RAG Blueprint"}),": Complete RAG platform documentation"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://docs.nvidia.com/nim/",children:"NVIDIA NIM Documentation"}),": NIM microservices reference"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://www.nvidia.com/en-us/data-center/products/ai-enterprise/",children:"NVIDIA AI Enterprise"}),": Enterprise AI platform"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"\ud83e\udd16 Models:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://build.nvidia.com/nvidia/llama-3_3-nemotron-super-49b-v1_5",children:"Llama-3.3-Nemotron-Super-49B-v1.5"}),": Advanced reasoning model (49B parameters)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct",children:"Llama-3.3-70B-Instruct"}),": Instruction-following model"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"\ud83d\udce6 Container Images & Helm Charts:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://catalog.ngc.nvidia.com/",children:"NVIDIA NGC Catalog"}),": Official container registry"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://helm.ngc.nvidia.com/nvidia/blueprint/charts/nvidia-blueprint-rag",children:"RAG Blueprint Helm Chart"}),": Kubernetes deployment"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://catalog.ngc.nvidia.com/orgs/nim",children:"NVIDIA NIM Containers"}),": Optimized inference containers"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"ai-on-eks-blueprint-resources",children:"AI-on-EKS Blueprint Resources"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"\ud83c\udfd7\ufe0f AI-on-EKS Blueprint Resources:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://github.com/awslabs/ai-on-eks",children:"AI-on-EKS Repository"}),": Main blueprint repository"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://github.com/awslabs/ai-on-eks/tree/main/infra/nvidia-deep-research",children:"Infrastructure & Deployment Code"}),": Terraform automation with Karpenter and application deployment scripts"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://github.com/awslabs/ai-on-eks/tree/main/blueprints/inference/nvidia-deep-research",children:"Usage Guide"}),": Post-deployment usage, data ingestion, and observability"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"\ud83d\udcd6 Documentation:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://github.com/awslabs/ai-on-eks/tree/main/infra/nvidia-deep-research/README.md",children:"Infrastructure & Deployment Guide"}),": Step-by-step infrastructure and application deployment"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://github.com/awslabs/ai-on-eks/tree/main/blueprints/inference/nvidia-deep-research/README.md",children:"Usage Guide"}),": Accessing services, data ingestion, monitoring"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://github.com/awslabs/ai-on-eks/tree/main/infra/nvidia-deep-research/terraform/opensearch-serverless.tf",children:"OpenSearch Integration"}),": Pod Identity authentication setup"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://github.com/awslabs/ai-on-eks/tree/main/infra/nvidia-deep-research/terraform/custom_karpenter.tf",children:"Karpenter Configuration"}),": P4/P5 GPU support"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"related-technologies",children:"Related Technologies"}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"\u2638\ufe0f Kubernetes & AWS:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://aws.amazon.com/eks/",children:"Amazon EKS"}),": Managed Kubernetes service"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://karpenter.sh/",children:"Karpenter"}),": Kubernetes node autoscaling"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://docs.aws.amazon.com/opensearch-service/latest/developerguide/serverless.html",children:"OpenSearch Serverless"}),": Managed vector database"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://docs.aws.amazon.com/eks/latest/userguide/pod-identities.html",children:"EKS Pod Identity"}),": IAM authentication for pods"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"\ud83e\udd16 AI/ML Tools:"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://developer.nvidia.com/dcgm",children:"NVIDIA DCGM"}),": GPU monitoring"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://prometheus.io/",children:"Prometheus"}),": Metrics collection"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://grafana.com/",children:"Grafana"}),": Visualization dashboards"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Explore Features"}),": Test multi-modal document processing with various file types"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Scale Deployments"}),": Configure multi-region or multi-cluster setups"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Integrate Applications"}),": Connect your applications to the RAG API endpoints"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Monitor Performance"}),": Use Grafana dashboards for ongoing monitoring"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Custom Models"}),": Swap in your own fine-tuned models"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Security Hardening"}),": Add authentication, rate limiting, and disaster recovery"]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsxs)(n.p,{children:["This deployment provides the ",(0,i.jsx)(n.a,{href:"https://github.com/NVIDIA-AI-Blueprints/rag",children:"NVIDIA Enterprise RAG Blueprint"})," and ",(0,i.jsx)(n.a,{href:"https://github.com/NVIDIA-AI-Blueprints/aiq-research-assistant",children:"NVIDIA AI-Q Research Assistant"})," on Amazon EKS with enterprise-grade features including ",(0,i.jsx)(n.a,{href:"https://karpenter.sh/",children:"Karpenter"})," automatic scaling, OpenSearch Serverless integration, and seamless AWS service integration."]})]})}function p(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(h,{...e})}):h(e)}},67841:(e,n,s)=>{s.d(n,{A:()=>r});const r=s.p+"assets/images/nvidia-rag-opensearch-arch-5ed2b3a2d403da7d9a5758b4381282f5.png"}}]);