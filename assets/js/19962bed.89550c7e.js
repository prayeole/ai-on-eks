"use strict";(self.webpackChunkdoeks_website=self.webpackChunkdoeks_website||[]).push([[9419],{71985:e=>{e.exports=JSON.parse('{"version":{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"blueprints":[{"type":"link","href":"/ai-on-eks/docs/blueprints/","label":"Overview","docId":"blueprints/index","unlisted":false},{"type":"category","label":"Training on EKS","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"GPUs","collapsible":true,"collapsed":true,"items":[{"type":"link","href":"/ai-on-eks/docs/blueprints/training/GPUs/bionemo","label":"BioNeMo on EKS","docId":"blueprints/training/GPUs/bionemo","unlisted":false},{"type":"link","href":"/ai-on-eks/docs/blueprints/training/GPUs/slinky-slurm","label":"Slurm on EKS","docId":"blueprints/training/GPUs/slinky-slurm","unlisted":false}]},{"type":"category","label":"Neuron","collapsible":true,"collapsed":true,"items":[{"type":"link","href":"/ai-on-eks/docs/blueprints/training/Neuron/RayTrain-Llama2","label":"Llama-2 with RayTrain on Trn1","docId":"blueprints/training/Neuron/RayTrain-Llama2","unlisted":false},{"type":"link","href":"/ai-on-eks/docs/blueprints/training/Neuron/Llama2","label":"Llama-2 with Nemo-Megatron on Trn1","docId":"blueprints/training/Neuron/Llama2","unlisted":false},{"type":"link","href":"/ai-on-eks/docs/blueprints/training/Neuron/BERT-Large","label":"BERT-Large on Trainium","docId":"blueprints/training/Neuron/BERT-Large","unlisted":false},{"type":"link","href":"/ai-on-eks/docs/blueprints/training/Neuron/Llama-LoRA-Finetuning","label":"Llama 3 Fine-tuning with LoRA","docId":"blueprints/training/Neuron/Llama-LoRA-Finetuning","unlisted":false}]}],"href":"/ai-on-eks/docs/category/training-on-eks"},{"type":"category","label":"Inference on EKS","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"GPU Inference on EKS","collapsible":true,"collapsed":true,"items":[{"type":"link","href":"/ai-on-eks/docs/blueprints/inference/GPUs/vLLM-rayserve","label":"RayServe with vLLM","docId":"blueprints/inference/GPUs/vLLM-rayserve","unlisted":false},{"type":"link","href":"/ai-on-eks/docs/blueprints/inference/GPUs/vLLM-NVIDIATritonServer","label":"NVIDIA Triton Server with vLLM","docId":"blueprints/inference/GPUs/vLLM-NVIDIATritonServer","unlisted":false},{"type":"link","href":"/ai-on-eks/docs/blueprints/inference/GPUs/stablediffusion-gpus","label":"Stable Diffusion on GPU","docId":"blueprints/inference/GPUs/stablediffusion-gpus","unlisted":false},{"type":"link","href":"/ai-on-eks/docs/blueprints/inference/GPUs/nvidia-nim-llama3","label":"NVIDIA NIM LLM on Amazon EKS","docId":"blueprints/inference/GPUs/nvidia-nim-llama3","unlisted":false},{"type":"link","href":"/ai-on-eks/docs/blueprints/inference/GPUs/nvidia-nim-operator","label":"NVIDIA NIM Operator on EKS","docId":"blueprints/inference/GPUs/nvidia-nim-operator","unlisted":false},{"type":"link","href":"/ai-on-eks/docs/blueprints/inference/GPUs/ray-vllm-deepseek","label":"DeepSeek-R1 on EKS","docId":"blueprints/inference/GPUs/ray-vllm-deepseek","unlisted":false},{"type":"link","href":"/ai-on-eks/docs/blueprints/inference/GPUs/nvidia-dynamo","label":"NVIDIA Dynamo on Amazon EKS","docId":"blueprints/inference/GPUs/nvidia-dynamo","unlisted":false},{"type":"link","href":"/ai-on-eks/docs/blueprints/inference/GPUs/nvidia-deep-research","label":"NVIDIA Enterprise RAG and AI-Q Research Assistant on EKS","docId":"blueprints/inference/GPUs/nvidia-deep-research","unlisted":false},{"type":"link","href":"/ai-on-eks/docs/blueprints/inference/GPUs/aibrix-deepseek-distill","label":"AIBrix on EKS","docId":"blueprints/inference/GPUs/aibrix-deepseek-distill","unlisted":false}],"href":"/ai-on-eks/docs/category/gpu-inference-on-eks"},{"type":"category","label":"Neuron Inference on EKS","collapsible":true,"collapsed":true,"items":[{"type":"link","href":"/ai-on-eks/docs/blueprints/inference/Neuron/vllm-ray-inf2","label":"Llama-3-8B with vLLM on Inferentia2","docId":"blueprints/inference/Neuron/vllm-ray-inf2","unlisted":false},{"type":"link","href":"/ai-on-eks/docs/blueprints/inference/Neuron/Mistral-7b-inf2","label":"Mistral-7B on Inferentia2","docId":"blueprints/inference/Neuron/Mistral-7b-inf2","unlisted":false},{"type":"link","href":"/ai-on-eks/docs/blueprints/inference/Neuron/llama3-inf2","label":"Llama-3-8B on Inferentia2","docId":"blueprints/inference/Neuron/llama3-inf2","unlisted":false},{"type":"link","href":"/ai-on-eks/docs/blueprints/inference/Neuron/llama2-inf2","label":"Llama-2 on Inferentia2","docId":"blueprints/inference/Neuron/llama2-inf2","unlisted":false},{"type":"link","href":"/ai-on-eks/docs/blueprints/inference/Neuron/stablediffusion-inf2","label":"Stable Diffusion on Inferentia2","docId":"blueprints/inference/Neuron/stablediffusion-inf2","unlisted":false},{"type":"link","href":"/ai-on-eks/docs/blueprints/inference/Neuron/rayserve-ha","label":"Ray Serve High Availability","docId":"blueprints/inference/Neuron/rayserve-ha","unlisted":false}],"href":"/ai-on-eks/docs/category/neuron-inference-on-eks"},{"type":"link","href":"/ai-on-eks/docs/blueprints/inference/","label":"Overview","docId":"blueprints/inference/index","unlisted":false},{"type":"link","href":"/ai-on-eks/docs/blueprints/inference/inference-charts","label":"Inference Charts","docId":"blueprints/inference/inference-charts","unlisted":false}],"href":"/ai-on-eks/docs/category/inference-on-eks"},{"type":"category","label":"gateways","collapsible":true,"collapsed":true,"items":[{"type":"link","href":"/ai-on-eks/docs/blueprints/gateways/envoy-gateway","label":"Envoy Gateway implementation on EKS","docId":"blueprints/gateways/envoy-gateway","unlisted":false}]}],"infra":[{"type":"link","href":"/ai-on-eks/docs/infra/","label":"Introduction","docId":"infra/index","unlisted":false},{"type":"link","href":"/ai-on-eks/docs/infra/jark","label":"JARK on EKS","docId":"infra/jark","unlisted":false},{"type":"link","href":"/ai-on-eks/docs/infra/aibrix","label":"AIBrix on EKS","docId":"infra/aibrix","unlisted":false},{"type":"link","href":"/ai-on-eks/docs/infra/emr-spark-rapids","label":"EMR NVIDIA Spark-RAPIDS","docId":"infra/emr-spark-rapids","unlisted":false},{"type":"link","href":"/ai-on-eks/docs/infra/inference-ready-cluster","label":"Inference-Ready Cluster","docId":"infra/inference-ready-cluster","unlisted":false},{"type":"link","href":"/ai-on-eks/docs/infra/jupyterhub","label":"JupyterHub on EKS","docId":"infra/jupyterhub","unlisted":false},{"type":"link","href":"/ai-on-eks/docs/infra/trainium","label":"Trainium on EKS","docId":"infra/trainium","unlisted":false},{"type":"link","href":"/ai-on-eks/docs/infra/troubleshooting/","label":"Troubleshooting","docId":"infra/troubleshooting/troubleshooting","unlisted":false}],"guidance":[{"type":"link","href":"/ai-on-eks/docs/guidance/","label":"Guidance for AI workloads","docId":"guidance/index","unlisted":false},{"type":"link","href":"/ai-on-eks/docs/guidance/eks-best-practices","label":"EKS Best Practices","docId":"guidance/eks-best-practices","unlisted":false},{"type":"category","label":"Benchmarking LLM Inference Performance on Amazon EKS","collapsible":true,"collapsed":true,"items":[{"type":"link","href":"/ai-on-eks/docs/guidance/benchmarking/understanding-the-benchmark-challenge/","label":"Understanding the Benchmark Challenge","docId":"guidance/benchmarking/understanding-the-benchmark-challenge/index","unlisted":false},{"type":"link","href":"/ai-on-eks/docs/guidance/benchmarking/key-metrics-for-benchmarking-llms/","label":"Key Metrics for Benchmarking LLMs","docId":"guidance/benchmarking/key-metrics-for-benchmarking-llms/index","unlisted":false},{"type":"category","label":"benchmarking-with-inference-perf","collapsible":true,"collapsed":true,"items":[{"type":"link","href":"/ai-on-eks/docs/guidance/benchmarking/benchmarking-with-inference-perf/inference-perf","label":"Benchmarking with Inference Perf","docId":"guidance/benchmarking/benchmarking-with-inference-perf/inference-perf","unlisted":false},{"type":"link","href":"/ai-on-eks/docs/guidance/benchmarking/benchmarking-with-inference-perf/running-inference-perf-on-eks","label":"Running Inference Perf on EKS","docId":"guidance/benchmarking/benchmarking-with-inference-perf/running-inference-perf-on-eks","unlisted":false},{"type":"link","href":"/ai-on-eks/docs/guidance/benchmarking/benchmarking-with-inference-perf/complete-deployment-example-guide","label":"Complete Deployment Example","docId":"guidance/benchmarking/benchmarking-with-inference-perf/complete-deployment-example-guide","unlisted":false}]},{"type":"category","label":"Test Scenarios","collapsible":true,"collapsed":true,"items":[{"type":"link","href":"/ai-on-eks/docs/guidance/benchmarking/test-scenarios/choosing-synthetic-vs-real","label":"Choosing Between Synthetic and Real Data","docId":"guidance/benchmarking/test-scenarios/choosing-synthetic-vs-real","unlisted":false},{"type":"link","href":"/ai-on-eks/docs/guidance/benchmarking/test-scenarios/baseline-performance","label":"Scenario 1 - Baseline Performance","docId":"guidance/benchmarking/test-scenarios/baseline-performance","unlisted":false},{"type":"link","href":"/ai-on-eks/docs/guidance/benchmarking/test-scenarios/saturation-testing","label":"Scenario 2 - Saturation Testing","docId":"guidance/benchmarking/test-scenarios/saturation-testing","unlisted":false},{"type":"link","href":"/ai-on-eks/docs/guidance/benchmarking/test-scenarios/automatic-saturation-detection","label":"Scenario 3 - Automatic Saturation Detection","docId":"guidance/benchmarking/test-scenarios/automatic-saturation-detection","unlisted":false},{"type":"link","href":"/ai-on-eks/docs/guidance/benchmarking/test-scenarios/production-simulation","label":"Scenario 4 - Production Simulation","docId":"guidance/benchmarking/test-scenarios/production-simulation","unlisted":false},{"type":"link","href":"/ai-on-eks/docs/guidance/benchmarking/test-scenarios/real-dataset-testing","label":"Scenario 5 - Real Dataset Testing","docId":"guidance/benchmarking/test-scenarios/real-dataset-testing","unlisted":false}],"href":"/ai-on-eks/docs/guidance/benchmarking/test-scenarios/"},{"type":"link","href":"/ai-on-eks/docs/guidance/benchmarking/resources/","label":"Resources","docId":"guidance/benchmarking/resources/index","unlisted":false}],"href":"/ai-on-eks/docs/guidance/benchmarking/"},{"type":"category","label":"Solving cold start challenges for AI/ML inference applications on Amazon EKS","collapsible":true,"collapsed":true,"items":[{"type":"category","label":"Reducing container image size","collapsible":true,"collapsed":true,"items":[{"type":"link","href":"/ai-on-eks/docs/guidance/container-startup-time/reduce-container-image-size/optimize-image-size","label":"Optimizing container images size","docId":"guidance/container-startup-time/reduce-container-image-size/optimize-image-size","unlisted":false},{"type":"link","href":"/ai-on-eks/docs/guidance/container-startup-time/reduce-container-image-size/decoupling-model-artifacts","label":"Decoupling model artifacts from container image","docId":"guidance/container-startup-time/reduce-container-image-size/decoupling-model-artifacts","unlisted":false}],"href":"/ai-on-eks/docs/guidance/container-startup-time/reduce-container-image-size/"},{"type":"category","label":"Accelerating pull process","collapsible":true,"collapsed":true,"items":[{"type":"link","href":"/ai-on-eks/docs/guidance/container-startup-time/accelerate-pull-process/containerd-snapshotter","label":"Using containerd snapshotter","docId":"guidance/container-startup-time/accelerate-pull-process/containerd-snapshotter","unlisted":false},{"type":"link","href":"/ai-on-eks/docs/guidance/container-startup-time/accelerate-pull-process/prefecthing-images-on-br","label":"Preload container images into data volumes","docId":"guidance/container-startup-time/accelerate-pull-process/prefecthing-images-on-br","unlisted":false}],"href":"/ai-on-eks/docs/guidance/container-startup-time/accelerate-pull-process/"}],"href":"/ai-on-eks/docs/guidance/container-startup-time/"},{"type":"link","href":"/ai-on-eks/docs/guidance/dynamic-resource-allocation","label":"Dynamic Resource Allocation on EKS","docId":"guidance/dynamic-resource-allocation","unlisted":false},{"type":"link","href":"/ai-on-eks/docs/guidance/networking","label":"Networking for AI","docId":"guidance/networking","unlisted":false},{"type":"link","href":"/ai-on-eks/docs/guidance/observability","label":"Observability","docId":"guidance/observability","unlisted":false}]},"docs":{"blueprints/gateways/envoy-gateway":{"id":"blueprints/gateways/envoy-gateway","title":"Envoy gateway","description":"Organizations deploying AI applications face a fundamental challenge: no single model serves all needs. Developers may choose Claude for long-context analysis, OpenAI for reasoning tasks, and DeepSeek for cost-sensitive workloads. The problem is that each model provider uses different APIs. Without centralized control, teams can\'t easily switch providers, get visibility into utilization, or enforce quotas.","sidebar":"blueprints"},"blueprints/index":{"id":"blueprints/index","title":"AI on EKS","description":"Welcome to AI on Amazon Elastic Kubernetes Service (EKS), your gateway to harnessing the power of Large Language Models (LLMs) for a wide range of applications. This introduction page serves as your starting point to explore our architectural patterns and blueprints for Training, Fine-tuning, and Inference using the latest LLMs.","sidebar":"blueprints"},"blueprints/inference/GPUs/aibrix-deepseek-distill":{"id":"blueprints/inference/GPUs/aibrix-deepseek-distill","title":"AIBrix","description":"AIBrix is an open source initiative designed to provide essential building blocks to construct scalable GenAI inference infrastructure. AIBrix delivers a cloud-native solution optimized for deploying, managing, and scaling large language model (LLM) inference, tailored specifically to enterprise needs.","sidebar":"blueprints"},"blueprints/inference/GPUs/nvidia-deep-research":{"id":"blueprints/inference/GPUs/nvidia-deep-research","title":"NVIDIA Enterprise RAG and AI-Q Research Assistant on EKS","description":"Deployment of Enterprise RAG and AI-Q on EKS requires access to GPU instances (g5, p4, or p5 families). This blueprint relies on Karpenter autoscaling for dynamic GPU provisioning.","sidebar":"blueprints"},"blueprints/inference/GPUs/nvidia-dynamo":{"id":"blueprints/inference/GPUs/nvidia-dynamo","title":"NVIDIA Dynamo on Amazon EKS","description":"Deployment of ML models on EKS requires access to GPUs or Neuron instances. If your deployment isn\'t working, it\'s often due to missing access to these resources. Also, some deployment patterns rely on Karpenter autoscaling and static node groups; if nodes aren\'t initializing, check the logs for Karpenter or Node groups to resolve the issue.","sidebar":"blueprints"},"blueprints/inference/GPUs/nvidia-nim-llama3":{"id":"blueprints/inference/GPUs/nvidia-nim-llama3","title":"NVIDIA NIM LLM on Amazon EKS","description":"Deployment of ML models on EKS requires access to GPUs or Neuron instances. If your deployment isn\'t working, it\u2019s often due to missing access to these resources. Also, some deployment patterns rely on Karpenter autoscaling and static node groups; if nodes aren\'t initializing, check the logs for Karpenter or Node groups to resolve the issue.","sidebar":"blueprints"},"blueprints/inference/GPUs/nvidia-nim-operator":{"id":"blueprints/inference/GPUs/nvidia-nim-operator","title":"NVIDIA NIM Operator on EKS","description":"What is NVIDIA NIM?","sidebar":"blueprints"},"blueprints/inference/GPUs/ray-vllm-deepseek":{"id":"blueprints/inference/GPUs/ray-vllm-deepseek","title":"DeepSeek-R1 on EKS","description":"In this guide, we\'ll explore deploying DeepSeek-R1-Distill-Llama-8B model inference using Ray with a vLLM backend on Amazon EKS.","sidebar":"blueprints"},"blueprints/inference/GPUs/stablediffusion-gpus":{"id":"blueprints/inference/GPUs/stablediffusion-gpus","title":"Stable Diffusion on GPU","description":"Deployment of ML models on EKS requires access to GPUs or Neuron instances. If your deployment isn\'t working, it\u2019s often due to missing access to these resources. Also, some deployment patterns rely on Karpenter autoscaling and static node groups; if nodes aren\'t initializing, check the logs for Karpenter or Node groups to resolve the issue.","sidebar":"blueprints"},"blueprints/inference/GPUs/vLLM-NVIDIATritonServer":{"id":"blueprints/inference/GPUs/vLLM-NVIDIATritonServer","title":"NVIDIA Triton Server with vLLM","description":"Deployment of ML models on EKS requires access to GPUs or Neuron instances. If your deployment isn\'t working, it\u2019s often due to missing access to these resources. Also, some deployment patterns rely on Karpenter autoscaling and static node groups; if nodes aren\'t initializing, check the logs for Karpenter or Node groups to resolve the issue.","sidebar":"blueprints"},"blueprints/inference/GPUs/vLLM-rayserve":{"id":"blueprints/inference/GPUs/vLLM-rayserve","title":"RayServe with vLLM","description":"Deployment of ML models on EKS requires access to GPUs or Neuron instances. If your deployment isn\'t working, it\u2019s often due to missing access to these resources. Also, some deployment patterns rely on Karpenter autoscaling and static node groups; if nodes aren\'t initializing, check the logs for Karpenter or Node groups to resolve the issue.","sidebar":"blueprints"},"blueprints/inference/index":{"id":"blueprints/inference/index","title":"Inference on EKS","description":"AI on EKS provides comprehensive solutions for deploying AI/ML inference workloads on Amazon EKS, supporting both GPU and AWS Neuron (Inferentia/Trainium) hardware configurations.","sidebar":"blueprints"},"blueprints/inference/inference-charts":{"id":"blueprints/inference/inference-charts","title":"AI on EKS Inference Charts","description":"The AI on EKS Inference Charts provide a streamlined Helm-based approach to deploy AI/ML inference workloads on both GPU","sidebar":"blueprints"},"blueprints/inference/Neuron/llama2-inf2":{"id":"blueprints/inference/Neuron/llama2-inf2","title":"Llama-2 on Inferentia2","description":"Serve Llama-2 models on AWS Inferentia accelerators for efficient inference.","sidebar":"blueprints"},"blueprints/inference/Neuron/llama3-inf2":{"id":"blueprints/inference/Neuron/llama3-inf2","title":"Llama-3-8B on Inferentia2","description":"Serve Llama-3 models on AWS Inferentia accelerators for efficient inference.","sidebar":"blueprints"},"blueprints/inference/Neuron/Mistral-7b-inf2":{"id":"blueprints/inference/Neuron/Mistral-7b-inf2","title":"Mistral-7B on Inferentia2","description":"Deployment of ML models on EKS requires access to GPUs or Neuron instances. If your deployment isn\'t working, it\u2019s often due to missing access to these resources. Also, some deployment patterns rely on Karpenter autoscaling and static node groups; if nodes aren\'t initializing, check the logs for Karpenter or Node groups to resolve the issue.","sidebar":"blueprints"},"blueprints/inference/Neuron/rayserve-ha":{"id":"blueprints/inference/Neuron/rayserve-ha","title":"Ray Serve High Availability","description":"Deployment of ML models on EKS requires access to GPUs or Neuron instances. If your deployment isn\'t working, it\u2019s often due to missing access to these resources. Also, some deployment patterns rely on Karpenter autoscaling and static node groups; if nodes aren\'t initializing, check the logs for Karpenter or Node groups to resolve the issue.","sidebar":"blueprints"},"blueprints/inference/Neuron/stablediffusion-inf2":{"id":"blueprints/inference/Neuron/stablediffusion-inf2","title":"Stable Diffusion on Inferentia2","description":"Deployment of ML models on EKS requires access to GPUs or Neuron instances. If your deployment isn\'t working, it\u2019s often due to missing access to these resources. Also, some deployment patterns rely on Karpenter autoscaling and static node groups; if nodes aren\'t initializing, check the logs for Karpenter or Node groups to resolve the issue.","sidebar":"blueprints"},"blueprints/inference/Neuron/vllm-ray-inf2":{"id":"blueprints/inference/Neuron/vllm-ray-inf2","title":"Llama-3-8B with vLLM on Inferentia2","description":"Serving Meta-Llama-3-8B-Instruct model on AWS Inferentia2 using Ray and vLLM for optimized inference performance.","sidebar":"blueprints"},"blueprints/training/GPUs/bionemo":{"id":"blueprints/training/GPUs/bionemo","title":"BioNeMo on EKS","description":"Deployment of ML models on EKS requires access to GPUs or Neuron instances. If your deployment isn\'t working, it\u2019s often due to missing access to these resources. Also, some deployment patterns rely on Karpenter autoscaling and static node groups; if nodes aren\'t initializing, check the logs for Karpenter or Node groups to resolve the issue.","sidebar":"blueprints"},"blueprints/training/GPUs/slinky-slurm":{"id":"blueprints/training/GPUs/slinky-slurm","title":"Slurm on EKS","description":"Deployment of ML models on EKS requires access to GPUs or Neuron instances. If your deployment isn\'t working, it\u2019s often due to missing access to these resources. Also, some deployment patterns rely on Karpenter autoscaling and static node groups; if nodes aren\'t initializing, check the logs for Karpenter or Node groups to resolve the issue.","sidebar":"blueprints"},"blueprints/training/Neuron/BERT-Large":{"id":"blueprints/training/Neuron/BERT-Large","title":"BERT-Large on Trainium","description":"COMING SOON","sidebar":"blueprints"},"blueprints/training/Neuron/Llama-LoRA-Finetuning":{"id":"blueprints/training/Neuron/Llama-LoRA-Finetuning","title":"Llama-LoRA-Finetuning","description":"To deploy this example for fine-tuning a LLM on EKS, you need access to AWS Trainium ec2 instance. If deployment fails, check if you have access to this instance type. If nodes aren\'t starting, check Karpenter or Node group logs.","sidebar":"blueprints"},"blueprints/training/Neuron/Llama2":{"id":"blueprints/training/Neuron/Llama2","title":"Llama-2 with Nemo-Megatron on Trn1","description":"Training a Llama-2 Model using Trainium, Neuronx-Nemo-Megatron and MPI operator","sidebar":"blueprints"},"blueprints/training/Neuron/RayTrain-Llama2":{"id":"blueprints/training/Neuron/RayTrain-Llama2","title":"RayTrain-Llama2","description":"Deployment of ML models on EKS requires access to GPUs or Neuron instances. If your deployment isn\'t working, it\u2019s often due to missing access to these resources. Also, some deployment patterns rely on Karpenter autoscaling and static node groups; if nodes aren\'t initializing, check the logs for Karpenter or Node groups to resolve the issue.","sidebar":"blueprints"},"guidance/benchmarking/benchmarking-with-inference-perf/complete-deployment-example-guide":{"id":"guidance/benchmarking/benchmarking-with-inference-perf/complete-deployment-example-guide","title":"Complete Deployment Example","description":"This example demonstrates a production-ready deployment with S3 storage, realistic load testing, and proper AWS integration. Follow these steps to deploy your benchmark.","sidebar":"guidance"},"guidance/benchmarking/benchmarking-with-inference-perf/inference-perf":{"id":"guidance/benchmarking/benchmarking-with-inference-perf/inference-perf","title":"Benchmarking with Inference Perf","description":"To make benchmarking easier and more consistent, the Inference Perf tool provides a standardized way to measure and compare LLM inference performance across different systems.","sidebar":"guidance"},"guidance/benchmarking/benchmarking-with-inference-perf/running-inference-perf-on-eks":{"id":"guidance/benchmarking/benchmarking-with-inference-perf/running-inference-perf-on-eks","title":"Running Inference Perf on EKS","description":"Inference Perf is a GenAI inference performance benchmarking tool designed specifically for measuring LLM inference endpoint performance on Kubernetes. It runs as a Kubernetes Job and supports multiple model servers (vLLM, SGLang, TGI) with standardized metrics.","sidebar":"guidance"},"guidance/benchmarking/index":{"id":"guidance/benchmarking/index","title":"Benchmarking Guide (With Inference Perf)","description":"What This Guide Covers","sidebar":"guidance"},"guidance/benchmarking/key-metrics-for-benchmarking-llms/index":{"id":"guidance/benchmarking/key-metrics-for-benchmarking-llms/index","title":"Key Metrics for Benchmarking LLMs","description":"When benchmarking a self-hosted LLM, it\'s important to look beyond simple throughput numbers. Different metrics capture different aspects of model performance, from how quickly a response starts to how efficiently tokens are generated over time. Below are the key metrics every customer should understand before running performance tests.","sidebar":"guidance"},"guidance/benchmarking/resources/index":{"id":"guidance/benchmarking/resources/index","title":"Resources","description":"Helm Chart Repository","sidebar":"guidance"},"guidance/benchmarking/test-scenarios/automatic-saturation-detection":{"id":"guidance/benchmarking/test-scenarios/automatic-saturation-detection","title":"SCENARIO 3: Automatic Saturation Detection","description":"When to use this scenario:","sidebar":"guidance"},"guidance/benchmarking/test-scenarios/baseline-performance":{"id":"guidance/benchmarking/test-scenarios/baseline-performance","title":"SCENARIO 1: Baseline Performance","description":"When to use this scenario:","sidebar":"guidance"},"guidance/benchmarking/test-scenarios/choosing-synthetic-vs-real":{"id":"guidance/benchmarking/test-scenarios/choosing-synthetic-vs-real","title":"Choosing Between Synthetic and Real Dataset Testing","description":"Default Recommendation: Use Real Data Whenever Possible","sidebar":"guidance"},"guidance/benchmarking/test-scenarios/index":{"id":"guidance/benchmarking/test-scenarios/index","title":"Test Scenarios","description":"This section provides practical test scenarios for benchmarking LLM inference performance. Each scenario addresses specific testing objectives and use cases.","sidebar":"guidance"},"guidance/benchmarking/test-scenarios/production-simulation":{"id":"guidance/benchmarking/test-scenarios/production-simulation","title":"SCENARIO 4: Production Simulation","description":"When to use this scenario:","sidebar":"guidance"},"guidance/benchmarking/test-scenarios/real-dataset-testing":{"id":"guidance/benchmarking/test-scenarios/real-dataset-testing","title":"SCENARIO 5: Real Dataset Testing","description":"When to use this scenario:","sidebar":"guidance"},"guidance/benchmarking/test-scenarios/saturation-testing":{"id":"guidance/benchmarking/test-scenarios/saturation-testing","title":"SCENARIO 2: Saturation Testing","description":"When to use this scenario:","sidebar":"guidance"},"guidance/benchmarking/understanding-the-benchmark-challenge/index":{"id":"guidance/benchmarking/understanding-the-benchmark-challenge/index","title":"Understanding the Benchmark Challenge","description":"As more organizations deploy large language models (LLMs) on their own infrastructure, understanding how to measure performance has become a common challenge.","sidebar":"guidance"},"guidance/container-startup-time/accelerate-pull-process/containerd-snapshotter":{"id":"guidance/container-startup-time/accelerate-pull-process/containerd-snapshotter","title":"Using containerd snapshotter","description":"Using SOCI snapshotter","sidebar":"guidance"},"guidance/container-startup-time/accelerate-pull-process/index":{"id":"guidance/container-startup-time/accelerate-pull-process/index","title":"Accelerating pull process","description":"The solutions in this section reduce the container startup time by improving the image pull process. They do it by either relying on the images\u2019 layered internal structure and changing how their layers are retrieved over the network from a container registry or by skipping the registry altogether.","sidebar":"guidance"},"guidance/container-startup-time/accelerate-pull-process/prefecthing-images-on-br":{"id":"guidance/container-startup-time/accelerate-pull-process/prefecthing-images-on-br","title":"Preload container images into Bottlerocket data volumes with EBS Snapshots","description":"The purpose of this pattern is to reduce the cold start time of containers with large images by caching the images in the data volume of Bottlerocket OS.","sidebar":"guidance"},"guidance/container-startup-time/index":{"id":"guidance/container-startup-time/index","title":"Solving cold start challenges for AI/ML inference applications on Amazon EKS","description":"This guide will get regular updates based on observed patterns. New content might be added, and existing might be changed.","sidebar":"guidance"},"guidance/container-startup-time/reduce-container-image-size/decoupling-model-artifacts":{"id":"guidance/container-startup-time/reduce-container-image-size/decoupling-model-artifacts","title":"Decoupling model artifacts from container image","description":"While the main purpose of extracting model artifacts is to reduce the container image size, this approach also provides additional operational and functional advantages. The operational improvements include separate versioning, streamlined auditing, and lifecycle control for model artifacts, while lazy-loading, hot-swapping, and reuse across different applications offer additional functional flexibility and performance. Depending on the use case these advantages can be decisive factors when selecting among the solutions outlined in this section.","sidebar":"guidance"},"guidance/container-startup-time/reduce-container-image-size/index":{"id":"guidance/container-startup-time/reduce-container-image-size/index","title":"Reducing container image size","description":"Image compressed size \u2013 the size it occupies in the container image registry, directly correlates with pull time, while image uncompressed size \u2013 the size of the image once downloaded, impacts the time it takes to bootstrap the container \u2013 the larger the size the longer it takes to decompress and extract the image layers, mount them, and combine them into the container file system.","sidebar":"guidance"},"guidance/container-startup-time/reduce-container-image-size/optimize-image-size":{"id":"guidance/container-startup-time/reduce-container-image-size/optimize-image-size","title":"Optimizing container images size","description":"Selecting appropriate base images","sidebar":"guidance"},"guidance/dynamic-resource-allocation":{"id":"guidance/dynamic-resource-allocation","title":"Dynamic Resource Allocation for GPUs on Amazon EKS","description":"\ud83d\ude80 TL;DR \u2013 Dynamic GPU Scheduling with DRA on EKS","sidebar":"guidance"},"guidance/eks-best-practices":{"id":"guidance/eks-best-practices","title":"EKS Best Practices","description":"Amazon EKS Best Practices guide for AI/ML applications is located and manged at the AWS documentation site for Amazon EKS. Please use this resource for any best practice recommendations.","sidebar":"guidance"},"guidance/index":{"id":"guidance/index","title":"Guidance for AI workloads","description":"This section is a collection of guidance on different topics around building and deploying AI workloads on Amazon EKS.","sidebar":"guidance"},"guidance/networking":{"id":"guidance/networking","title":"Networking for AI","description":"VPC and IP Considerations","sidebar":"guidance"},"guidance/observability":{"id":"guidance/observability","title":"Observability","description":"Observability for AI/ML workloads requires a holistic view of multiple hardware/software components alongside multiple sources of data such as logs, metrics, and traces. Piecing together these components is challenging and time-consuming; therefore, we leverage the AI/ML Observability available in Github to bootstrap this environment.","sidebar":"guidance"},"infra/aibrix":{"id":"infra/aibrix","title":"AIBrix on EKS","description":"Deployment of ML models on EKS requires access to GPUs or Neuron instances. If your deployment isn\'t working, it\u2019s often due to missing access to these resources. Also, some deployment patterns rely on Karpenter autoscaling and static node groups; if nodes aren\'t initializing, check the logs for Karpenter or Node groups to resolve the issue.","sidebar":"infra"},"infra/emr-spark-rapids":{"id":"infra/emr-spark-rapids","title":"EMR on EKS NVIDIA RAPIDS Accelerator for Apache Spark","description":"The NVIDIA RAPIDS Accelerator for Apache Spark is a powerful tool that builds on the capabilities of NVIDIA CUDA\xae - a transformative parallel computing platform designed for enhancing computational processes on NVIDIA\'s GPU architecture. RAPIDS, a project developed by NVIDIA, comprises a suite of open-source libraries that are hinged upon CUDA, thereby enabling GPU-accelerated data science workflows.","sidebar":"infra"},"infra/index":{"id":"infra/index","title":"Introduction","description":"The AIoEKS foundational infrastructure lives in the infra/base directory. This directory contains the base","sidebar":"infra"},"infra/inference-ready-cluster":{"id":"infra/inference-ready-cluster","title":"Inference-Ready EKS Cluster","description":"The Inference-Ready EKS Cluster is a pre-configured infrastructure solution designed specifically for AI/ML inference","sidebar":"infra"},"infra/jark":{"id":"infra/jark","title":"JARK on EKS","description":"Deployment of ML models on EKS requires access to GPUs or Neuron instances. If your deployment isn\'t working, it\u2019s often due to missing access to these resources. Also, some deployment patterns rely on Karpenter autoscaling and static node groups; if nodes aren\'t initializing, check the logs for Karpenter or Node groups to resolve the issue.","sidebar":"infra"},"infra/jupyterhub":{"id":"infra/jupyterhub","title":"jupyterhub","description":"Deployment of ML models on EKS requires access to GPUs or Neuron instances. If your deployment isn\'t working, it\u2019s often due to missing access to these resources. Also, some deployment patterns rely on Karpenter autoscaling and static node groups; if nodes aren\'t initializing, check the logs for Karpenter or Node groups to resolve the issue.","sidebar":"infra"},"infra/trainium":{"id":"infra/trainium","title":"trainium","description":"Deployment of ML models on EKS requires access to GPUs or Neuron instances. If your deployment isn\'t working, it\u2019s often due to missing access to these resources. Also, some deployment patterns rely on Karpenter autoscaling and static node groups; if nodes aren\'t initializing, check the logs for Karpenter or Node groups to resolve the issue.","sidebar":"infra"},"infra/troubleshooting/troubleshooting":{"id":"infra/troubleshooting/troubleshooting","title":"Troubleshooting","description":"You will find troubleshooting info for AI on Amazon EKS(AIoEKS) installation issues","sidebar":"infra"},"intro":{"id":"intro","title":"intro","description":""}}}}')}}]);