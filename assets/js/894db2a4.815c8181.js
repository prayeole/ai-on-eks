"use strict";(self.webpackChunkdoeks_website=self.webpackChunkdoeks_website||[]).push([[4737],{28453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>o});var r=t(96540);const i={},s=r.createContext(i);function a(e){const n=r.useContext(s);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),r.createElement(s.Provider,{value:n},e.children)}},46022:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>u,frontMatter:()=>a,metadata:()=>r,toc:()=>l});const r=JSON.parse('{"id":"guidance/benchmarking/key-metrics-for-benchmarking-llms/index","title":"Key Metrics for Benchmarking LLMs","description":"When benchmarking a self-hosted LLM, it\'s important to look beyond simple throughput numbers. Different metrics capture different aspects of model performance, from how quickly a response starts to how efficiently tokens are generated over time. Below are the key metrics every customer should understand before running performance tests.","source":"@site/docs/guidance/benchmarking/2-key-metrics-for-benchmarking-llms/index.md","sourceDirName":"guidance/benchmarking/2-key-metrics-for-benchmarking-llms","slug":"/guidance/benchmarking/key-metrics-for-benchmarking-llms/","permalink":"/ai-on-eks/docs/guidance/benchmarking/key-metrics-for-benchmarking-llms/","draft":false,"unlisted":false,"editUrl":"https://github.com/awslabs/ai-on-eks/blob/main/website/docs/guidance/benchmarking/2-key-metrics-for-benchmarking-llms/index.md","tags":[],"version":"current","frontMatter":{"sidebar_label":"Key Metrics for Benchmarking LLMs"},"sidebar":"guidance","previous":{"title":"Understanding the Benchmark Challenge","permalink":"/ai-on-eks/docs/guidance/benchmarking/understanding-the-benchmark-challenge/"},"next":{"title":"Benchmarking with Inference Perf","permalink":"/ai-on-eks/docs/guidance/benchmarking/benchmarking-with-inference-perf/inference-perf"}}');var i=t(74848),s=t(28453);const a={sidebar_label:"Key Metrics for Benchmarking LLMs"},o="Key Metrics for Benchmarking LLMs",c={},l=[{value:"Time to First Token (TTFT)",id:"time-to-first-token-ttft",level:3},{value:"Intertoken Latency (ITL)",id:"intertoken-latency-itl",level:3},{value:"Tokens per Second (TPS)",id:"tokens-per-second-tps",level:3}];function d(e){const n={h1:"h1",h3:"h3",header:"header",li:"li",p:"p",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"key-metrics-for-benchmarking-llms",children:"Key Metrics for Benchmarking LLMs"})}),"\n",(0,i.jsx)(n.p,{children:"When benchmarking a self-hosted LLM, it's important to look beyond simple throughput numbers. Different metrics capture different aspects of model performance, from how quickly a response starts to how efficiently tokens are generated over time. Below are the key metrics every customer should understand before running performance tests."}),"\n",(0,i.jsx)(n.h3,{id:"time-to-first-token-ttft",children:"Time to First Token (TTFT)"}),"\n",(0,i.jsx)(n.p,{children:"Time to First Token measures how long it takes for the model to process a prompt and produce the very first token of output. It represents the user's initial waiting time, the delay between sending a request and seeing the first word appear.\nFormula: first_non_empty_token_received_time - request_send_time\nWhat it includes:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Request queuing time (wait time if other requests are being processed)"}),"\n",(0,i.jsx)(n.li,{children:"Prefill time (model processing the entire input prompt to create the KV cache)"}),"\n",(0,i.jsx)(n.li,{children:"Network latency"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Technical considerations:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Longer prompts result in larger TTFT; attention computation scales with input sequence length"}),"\n",(0,i.jsx)(n.li,{children:"Benchmarking tools like Inference Perf disregard initial empty responses to ensure meaningful measurements"}),"\n",(0,i.jsx)(n.li,{children:"TTFT represents your minimum latency baseline for the current deployment configuration; while users cannot see faster response times than TTFT within the same setup, this baseline can be reduced through optimisations such as model quantization, faster hardware, or improved parallelism strategies"}),"\n",(0,i.jsx)(n.li,{children:"Advanced optimisation note: Some production deployments use disaggregated prefill/decode architectures where prefill and token generation phases run on separate infrastructure to optimize resource utilisation; this is beyond the scope of this guide but can significantly impact TTFT in specialized setups."}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"intertoken-latency-itl",children:"Intertoken Latency (ITL)"}),"\n",(0,i.jsx)(n.p,{children:"Intertoken Latency (also called Time per Output Token, or TPOT) measures the average delay between consecutive tokens generated by the model. This reflects how smoothly and steadily the model streams output once generation starts.\nFormula (Inference Perf): generation_time / (output_tokens - 1)\nKey detail: The formula excludes TTFT by subtracting 1 in the denominator, making ITL a metric of only the decoding/generation phase.\nTechnical considerations:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"As output grows, the KV cache grows, impacting memory bandwidth and attention computation cost (linear with input + output length)"}),"\n",(0,i.jsx)(n.li,{children:"Consistent ITL indicates efficient memory management and bandwidth utilization"}),"\n",(0,i.jsx)(n.li,{children:"Increasing ITL during long generations suggests memory bandwidth constraints or KV cache pressure"}),"\n",(0,i.jsx)(n.li,{children:"Different tools calculate ITL differently; Inference Perf excludes TTFT while some tools (like LLMPerf) include it in the average"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"tokens-per-second-tps",children:"Tokens per Second (TPS)"}),"\n",(0,i.jsx)(n.p,{children:"Tokens per Second captures the overall throughput of the system, how many output tokens the model generates each second across all active requests. As more requests run in parallel, TPS typically increases until the hardware reaches its saturation point. Beyond that, performance may plateau or even drop as resources become overutilized. This metric helps teams understand system capacity and plan for scaling or batching strategies."}),"\n",(0,i.jsx)(n.p,{children:"Formula (Inference Perf): total_output_tokens / (last_response_time - first_request_time)"}),"\n",(0,i.jsx)(n.p,{children:"Two perspectives to understand:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"System TPS (total throughput): Aggregate capacity across all concurrent requests, increases with concurrency until saturation"}),"\n",(0,i.jsx)(n.li,{children:"Per-user TPS: Individual request throughput (output_tokens / e2e_latency), decreases as system load increases"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Critical relationship: Critical relationship: As concurrent load increases beyond available model replicas, System TPS \u2191 while per-user TPS \u2193 (assuming the number of concurrent requests exceeds the system's immediate serving capacity)"}),"\n",(0,i.jsx)(n.p,{children:"Technical considerations:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Inference Perf uses a sliding window technique, excluding warmup and cooldown requests for stable measurements"}),"\n",(0,i.jsx)(n.li,{children:"Other tools may include full benchmark duration, adding overhead (up to 33% in single-concurrency scenarios)"}),"\n",(0,i.jsx)(n.li,{children:"The saturation point is where TTFT spikes, TPS plateaus, and error rates begin appearing"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Why it matters: TPS is essential for capacity planning; understanding maximum sustainable throughput helps determine infrastructure sizing and production operating points (typically 50-70% of saturation)."})]})}function u(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}}}]);